{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (0.12.14)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (0.4.2)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.14 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (0.12.14)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (0.6.4)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (0.3.14)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (0.4.2)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (0.4.4)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.60.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.14->llama-index) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (1.2.17)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (2024.12.0)\n",
      "Requirement already satisfied: httpx in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (3.4.2)\n",
      "Requirement already satisfied: numpy in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (2.10.6)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.14->llama-index) (1.17.2)\n",
      "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.8 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.10)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.2.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.5.20)\n",
      "Requirement already satisfied: click in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.14->llama-index) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.14->llama-index) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.14->llama-index) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.14->llama-index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.14->llama-index) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.14->llama-index) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.14->llama-index) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
      "Requirement already satisfied: certifi<2025.0.0,>=2024.7.4 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-cloud<0.2.0,>=0.1.8->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2024.12.14)\n",
      "Requirement already satisfied: anyio in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.14->llama-index) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.14->llama-index) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.14->llama-index) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.14->llama-index) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from click->nltk>3.8.1->llama-index) (0.4.6)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.14->llama-index) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.14->llama-index) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.14->llama-index) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.14->llama-index) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.14->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.14->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.14->llama-index) (3.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.14->llama-index) (24.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
      "Collecting llama-index-readers-web\n",
      "  Downloading llama_index_readers_web-0.3.5-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-readers-web) (3.11.11)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-readers-web) (4.12.3)\n",
      "Collecting chromedriver-autoinstaller<0.7.0,>=0.6.3 (from llama-index-readers-web)\n",
      "  Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting html2text<2025.0.0,>=2024.2.26 (from llama-index-readers-web)\n",
      "  Using cached html2text-2024.2.26-py3-none-any.whl\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-readers-web) (0.12.14)\n",
      "Collecting newspaper3k<0.3.0,>=0.2.8 (from llama-index-readers-web)\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting playwright<2.0,>=1.30 (from llama-index-readers-web)\n",
      "  Downloading playwright-1.49.1-py3-none-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-readers-web) (2.32.3)\n",
      "Collecting selenium<5.0.0,>=4.17.2 (from llama-index-readers-web)\n",
      "  Downloading selenium-4.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting spider-client<0.0.28,>=0.0.27 (from llama-index-readers-web)\n",
      "  Using cached spider_client-0.0.27-py3-none-any.whl\n",
      "Requirement already satisfied: urllib3>=1.1.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-readers-web) (2.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-web) (2.6)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from chromedriver-autoinstaller<0.7.0,>=0.6.3->llama-index-readers-web) (24.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (2.0.37)\n",
      "Requirement already satisfied: dataclasses-json in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (1.2.17)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (2024.12.0)\n",
      "Requirement already satisfied: httpx in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (3.4.2)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (2.2.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (1.17.2)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting lxml>=3.6.0 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
      "  Using cached lxml-5.3.0-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tldextract>=2.0.1 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
      "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
      "  Using cached feedfinder2-0.0.4-py3-none-any.whl\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
      "  Using cached jieba3k-0.35.1-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (2.9.0.post0)\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
      "  Using cached tinysegmenter-0.3-py3-none-any.whl\n",
      "Requirement already satisfied: greenlet==3.1.1 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from playwright<2.0,>=1.30->llama-index-readers-web) (3.1.1)\n",
      "Collecting pyee==12.0.0 (from playwright<2.0,>=1.30->llama-index-readers-web)\n",
      "  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.31.0->llama-index-readers-web) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.31.0->llama-index-readers-web) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.31.0->llama-index-readers-web) (2024.12.14)\n",
      "Collecting trio~=0.17 (from selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
      "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
      "  Using cached trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client~=1.8 (from selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: six in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (1.17.0)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: click in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (2.27.2)\n",
      "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
      "  Using cached requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract>=2.0.1->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (0.4.6)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from trio~=0.17->selenium<5.0.0,>=4.17.2->llama-index-readers-web) (1.3.1)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
      "  Using cached cffi-1.17.1-cp311-cp311-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (1.0.0)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (3.26.0)\n",
      "Requirement already satisfied: anyio in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\saravana\\my github repo\\llamaindex-blog-creation\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-web) (0.14.0)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium<5.0.0,>=4.17.2->llama-index-readers-web)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading llama_index_readers_web-0.3.5-py3-none-any.whl (82 kB)\n",
      "Downloading chromedriver_autoinstaller-0.6.4-py3-none-any.whl (7.6 kB)\n",
      "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Downloading playwright-1.49.1-py3-none-win_amd64.whl (34.1 MB)\n",
      "   ---------------------------------------- 0.0/34.1 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 5.2/34.1 MB 29.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 11.5/34.1 MB 30.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 17.3/34.1 MB 29.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 23.3/34.1 MB 28.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 29.1/34.1 MB 28.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  33.8/34.1 MB 29.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 34.1/34.1 MB 28.1 MB/s eta 0:00:00\n",
      "Downloading pyee-12.0.0-py3-none-any.whl (14 kB)\n",
      "Downloading selenium-4.28.1-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 6.3/9.5 MB 32.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 28.3 MB/s eta 0:00:00\n",
      "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Using cached lxml-5.3.0-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
      "Downloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
      "Using cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: tinysegmenter, sortedcontainers, sgmllib3k, jieba3k, wsproto, websocket-client, pysocks, pyee, pycparser, outcome, lxml, html2text, filelock, feedparser, cssselect, chromedriver-autoinstaller, spider-client, requests-file, playwright, feedfinder2, cffi, trio, tldextract, trio-websocket, newspaper3k, selenium, llama-index-readers-web\n",
      "Successfully installed cffi-1.17.1 chromedriver-autoinstaller-0.6.4 cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 filelock-3.17.0 html2text-2024.2.26 jieba3k-0.35.1 llama-index-readers-web-0.3.5 lxml-5.3.0 newspaper3k-0.2.8 outcome-1.3.0.post0 playwright-1.49.1 pycparser-2.22 pyee-12.0.0 pysocks-1.7.1 requests-file-2.1.0 selenium-4.28.1 sgmllib3k-1.0.0 sortedcontainers-2.4.0 spider-client-0.0.27 tinysegmenter-0.3 tldextract-5.1.3 trio-0.28.0 trio-websocket-0.11.1 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index\n",
    "!pip install llama-index-readers-web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='c911f1c3eb11134cfe5eb5a0b59b0d2d821143b1', embedding=None, metadata={'title': 'Luma AI’s Ray2 video model is now available in Amazon Bedrock', 'link': 'https://aws.amazon.com/blogs/aws/luma-ai-ray-2-video-model-is-now-available-in-amazon-bedrock/', 'date': 'Thu, 23 Jan 2025 19:50:22 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>As we <a href=\"https://press.aboutamazon.com/aws/2024/12/luma-ai-announces-new-video-model-ray-2-will-soon-be-available-to-consumers-professionals-and-developers\">preannounced</a> at AWS re:Invent 2024, you can now use <a href=\"https://aws.amazon.com/bedrock/luma-ai/\">Luma AI Ray2 video model in Amazon Bedrock</a> to generate high-quality video clips from text, creating captivating motion graphics from static concepts. AWS is the first and only cloud provider to offer fully managed models from Luma AI.</p> \\n<p>On January 16, 2025, <a href=\"https://lumalabs.ai/\">Luma AI</a> introduced <a href=\"https://lumalabs.ai/ray\">Luma Ray2</a>, the large–scale video generative model capable of creating realistic visuals with natural, coherent motion with strong understanding of text instructions. Luma Ray2 exhibits advanced capabilities as a result of being trained on Luma’s new multi-modal architecture. It scales to ten times compute of Ray1, enabling it to produce 5 second or 9 second video clips that show fast coherent motion, ultra-realistic details, and logical event sequences with 540p and 720p resolution.</p> \\n<p>With Luma Ray2 in <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>, you can add high-quality, realistic, production-ready videos generated from text in your <a href=\"https://aws.amazon.com/generative-ai/\">generative AI</a> application through a single API. Luma Ray2 video model understands the interactions between people, animals, and objects, and you can create consistent and physically accurate characters through state-of-the-art natural language instruction understanding and reasoning.</p> \\n<p>You can use Ray2 video generations for content creation, entertainment, advertising, and media use cases, streamlining the creative process, from concept to execution. You can generate smooth, cinematic, and lifelike camera movements that match the intended emotion of the scene. You can rapidly experiment with different camera angles and styles and deliver creative outputs for architecture, fashion, film, graphic design, and music.</p> \\n<p>Let’s take a look at the <a href=\"https://youtu.be/lIKRRRhHN2U\">impressive video generations</a> by Luma Ray2 that Luma has published.</p> \\n<p></p> \\n<p><u><strong>Get started with Luma Ray2 model in Amazon Bedrock</strong><br /> </u>Before getting started, if you are new to using Luma models, go to the <a href=\"https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#modelaccess\">Amazon Bedrock console</a> and choose <strong>Model access</strong> on the bottom left pane. To access the latest Luma AI models, request access for Luma Ray2 in Luma AI.</p> \\n<p><img alt=\"\" class=\"aligncenter wp-image-93395 size-full\" height=\"994\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/01/24/2025-luma-ray2-bedrock-1-enable-model-1.png\" style=\"width: 90%; border: solid 1px #ccc;\" width=\"1874\" /></p> \\n<p>To test the Luma AI model in Amazon Bedrock, choose <strong>Image/Video </strong>under <strong>Playgrounds </strong>in the left menu pane. Choose <strong>Select model</strong>, then select <strong>Luma AI</strong> as the category and<strong> Ray </strong>as the model.</p> \\n<p><img alt=\"\" class=\"aligncenter size-full wp-image-93315\" height=\"782\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/01/17/2025-luma-ray2-bedrock-1-choose-model.png\" style=\"width: 90%; border: solid 1px #ccc;\" width=\"1744\" /></p> \\n<p>For video generation models, you should have an <a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service (Amazon S3)</a> bucket to store all generated videos. This bucket will be created in your AWS account, and Amazon Bedrock will have read and write permissions for it. Choose <strong>Confirm </strong>to create a bucket and generate a video.</p> \\n<p><img alt=\"\" class=\"aligncenter size-full wp-image-93316\" height=\"632\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/01/17/2025-luma-ray2-bedrock-2-creat-s3-bucekt.png\" style=\"width: 70%; border: solid 1px #ccc;\" width=\"1244\" /></p> \\n<p>I will generate a 5-second video with 720P and 24 frames per second with 16:9 aspect ratio for my prompt.</p> \\n<p><img alt=\"\" class=\"aligncenter wp-image-93325 size-full\" height=\"1384\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/01/17/2025-luma-ray2-bedrock-3-create-video.png\" style=\"width: 100%; border: solid 1px #ccc;\" width=\"2746\" /></p> \\n<p>Here is an example prompt and generated video. You can download it stored in the S3 bucket.<br /> <code>a humpback whale swimming through space particles</code></p> \\n<div class=\"wp-video\" style=\"width: 640px;\">\\n <video class=\"wp-video-shortcode\" controls=\"controls\" height=\"360\" id=\"video-93300-1\" preload=\"metadata\" width=\"640\">\\n  <source src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/AWSNews/2025/Ray2-Cosmic+Whale.mp4?_=1\" type=\"video/mp4\" />\\n </video>\\n</div> \\n<p>Here are another featured examples to demonstrate Ray2 model.</p> \\n<p>Prompt 1: <code>A miniature baby cat is walking and exploring on the surface of a fingertip</code></p> \\n<div class=\"wp-video\" style=\"width: 640px;\">\\n <video class=\"wp-video-shortcode\" controls=\"controls\" height=\"360\" id=\"video-93300-2\" preload=\"metadata\" width=\"640\">\\n  <source src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/AWSNews/2025/Ray2-A+Miniature+Baby+Cat.mp4?_=2\" type=\"video/mp4\" />\\n </video>\\n</div> \\n<p>Prompt 2: <code>A massive orb of water floating in a backlit forest</code></p> \\n<div class=\"wp-video\" style=\"width: 640px;\">\\n <video class=\"wp-video-shortcode\" controls=\"controls\" height=\"360\" id=\"video-93300-3\" preload=\"metadata\" width=\"640\">\\n  <source src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/AWSNews/2025/Ray2-Floating+Water+Orb.mp4?_=3\" type=\"video/mp4\" />\\n </video>\\n</div> \\n<p>Prompt 3: <code>A man plays saxophone</code> by <a href=\"https://x.com/ziguratt\">@ziguratt</a></p> \\n<div class=\"wp-video\" style=\"width: 640px;\">\\n <video class=\"wp-video-shortcode\" controls=\"controls\" height=\"360\" id=\"video-93300-4\" preload=\"metadata\" width=\"640\">\\n  <source src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/AWSNews/2025/Ray2-Saxophone+Player.mp4?_=4\" type=\"video/mp4\" />\\n </video>\\n</div> \\n<p>Prompt 4: <code>Macro closeup of a bee pollinating</code></p> \\n<div class=\"wp-video\" style=\"width: 640px;\">\\n <video class=\"wp-video-shortcode\" controls=\"controls\" height=\"360\" id=\"video-93300-5\" preload=\"metadata\" width=\"640\">\\n  <source src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/AWSNews/2025/Ray2-Pollen+Collector.mp4?_=5\" type=\"video/mp4\" />\\n </video>\\n</div> \\n<p>To check out more examples and generated videos, visit the <a href=\"https://lumalabs.ai/ray\">Luma Ray2</a> page.</p> \\n<p>By choosing <strong>View API request</strong> in the Bedrock console, you can also access the model using code examples in the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a> and AWS SDKs. You can use <code>luma.ray-v2:0</code> as the model ID.</p> \\n<p>Here is a sample of the AWS CLI command:</p> \\n<pre><code class=\"lang-bash\">aws bedrock-runtime start-async-invoke \\\\\\n    --region us-west-2 \\\\\\n    --model-id luma.ray-v2:0 \\\\\\n    --model-input  \"{ \\\\\"prompt\\\\\": \\\\\"a humpback whale swimming through space particles\\\\\", \\\\\"duration\\\\\":\\\\\"5s\\\\\", \\\\\"resolution\\\\\": \\\\\"540p\\\\\", \\\\\"aspect_ratio\\\\\":\\\\\"16:9\\\\\"}\" \\\\\\n    --output-data-config \"{\\\\\"s3OutputDataConfig\\\\\": {\\\\\"s3Uri\\\\\": \\\\\"s3:\\\\/\\\\/testing-bucket-ais-region-us-west-2\\\\/\\\\\"}}\"</code></pre> \\n<p>You can use a <a href=\"https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_StartAsyncInvoke.html\"><code>StartAsyncInvoke</code> API action</a> to generate videos using <a href=\"https://aws.amazon.com/developer/tools/\">AWS SDKs</a> to build your applications using various programming languages.</p> \\n<p><u><strong>Now available</strong><br /> </u>Luma Ray2 video model is generally available today in Amazon Bedrock in the US West (Oregon) <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\">AWS Region</a>. Check the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html\">full Region list</a> for future updates. To learn more, check out the <a href=\"https://aws.amazon.com/bedrock/luma-ai/\">Luma AI in Amazon Bedrock</a> product page and the <a href=\"https://aws.amazon.com/bedrock/pricing/\">Amazon Bedrock Pricing</a> page.</p> \\n<p>Give Luma Ray2 a try in the <a href=\"https://console.aws.amazon.com/bedrock\">Amazon Bedrock console</a> today, and send feedback to <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag/amazon-bedrock\">AWS re:Post for Amazon Bedrock</a> or through your usual AWS Support contacts.</p> \\n<p>— <a href=\"https://twitter.com/channyun\">Channy</a></p> \\n<p><i><strong>Update January 24, 2025</strong> – The AWS CLI command was fixed to use the <code>start-async-invoke</code> parameter instead of <code>invoke-model</code>.</i></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='156b5d31d6e320533cdb9b110de5d093b614cb42', embedding=None, metadata={'title': 'AWS Weekly Roundup: New AWS Mexico (Central) Region, simultaneous sign-in for multiple AWS accounts, and more (January 20, 2025)', 'link': 'https://aws.amazon.com/blogs/aws/aws-weekly-roundup-new-aws-mexico-central-region-simultaneous-sign-in-for-multiple-aws-accounts-and-more-january-20-2025/', 'date': 'Mon, 20 Jan 2025 16:52:30 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p><img alt=\"\" class=\"alignright\" height=\"270\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/01/18/AWSNEWS-2120.jpeg\" style=\"width: 202px !important; height: 270px !important;\" width=\"202\" />As winter maintains its hold over where I live in the Netherlands, rare moments of sunlight become precious gifts. This weekend offered one such treasure—while cycling along a quiet canal, golden rays broke through the typically gray Dutch sky, creating a perfect moment of serenity. These glimpses of brightness feel particularly special during January, when daylight can be scarce in our corner of Europe. As we move deeper into 2025, the third week of the new year brings both reflection and forward momentum. While global conversations swirl around technological advancements, it’s these small, personal moments that remind us to pause and appreciate the simple pleasures among our rapidly evolving world.</p> \\n<p>Let’s look at the last week’s new announcements.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Last week’s launches</strong></span><br /> Here are the launches that got my attention.</p> \\n<p><a href=\"https://aws.amazon.com/blogs/aws/now-open-aws-mexico-central-region/\">AWS Mexico (Central) Region</a> – In February 2024, we announced plans to expand infrastructure in Mexico, and we’ve now launched the AWS Mexico (Central) Region with three Availability Zones and API code <strong>mx-central-1</strong>. This marks the first AWS infrastructure Region in Mexico and adds to our growing presence in Latin America. The new Region provides you with local workload management, data storage capabilities, enhanced performance with lower latency, and robust security standards. It features advanced cloud technologies, including cutting-edge artificial intelligence and machine learning (AI/ML) capabilities with purpose-built processors, comprehensive security capabilities with support for 143 security standards and compliance certifications. With this launch, AWS now spans 114 Availability Zones within 36 geographic Regions.</p> \\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/aws-management-console-simultaneous-sign-in-multiple-accounts/\">AWS Management Console now supports simultaneous sign-in for multiple AWS accounts</a> – Using multi-session capability in the <a href=\"https://aws.amazon.com/console/\">AWS Management Console</a>, you can now sign-in to multiple AWS accounts and manage your resources in a single browser. You can sign in to up to 5 sessions and this can be any combination of root, <a href=\"https://aws.amazon.com/iam/\">AWS Identity and Access Management (IAM)</a>, or federated roles in different accounts or in the same account. You can scale your applications using multiple accounts following AWS best-practice guidelines. You can use accounts for different environments, such as development, testing, and production, and compare resource configurations and status across multiple accounts for troubleshooting application issues and other application related jobs.</p> \\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-ec2-flex-instances-new-larger-sizes/\">Introducing new larger sizes on Amazon EC2 Flex instances</a> – We’re announcing the general availability of two new larger sizes (12xlarge and 16xlarge) on <a href=\"https://aws.amazon.com/ec2/\">Amazon Elastic Compute Cloud (Amazon EC2)</a> Flex (C7i-flex and M7i-flex) instances. The new sizes expand the EC2 Flex portfolio, providing additional compute options to scale up existing workloads or run larger-sized applications that need additional memory. These instances are powered by custom 4th Gen Intel Xeon Scalable processors, which are available only on AWS, and offer up to 15% better performance over comparable x86-based Intel processors used by other cloud providers. Flex instances are the easiest way to get price performance benefits and lower prices for a majority of compute-intensive and general-purpose workloads. They deliver up to 19% better price performance than comparable previous generation instances and are a great first choice for applications that don’t fully utilize the compute resources. Flex instances are ideal for web and application servers, batch processing, enterprise applications, databases, and more. For compute-intensive and general-purpose workloads that need even larger instance sizes (up to 192 vCPUs and 768 GiB memory) or continuous high CPU usage, you can use Amazon EC2 C7i and M7i instances.</p> \\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/aws-user-notifications-ga-cloudformation/\">Announcing AWS User Notifications general availability on AWS CloudFormation</a> – You can use <a href=\"https://aws.amazon.com/notifications/\">AWS User Notifications</a> to configure notifications to be sent using the AWS Management Console Notifications Center, email, <a href=\"https://aws.amazon.com/chatbot/\">AWS Chatbot</a>, or mobile push notifications to the <a href=\"https://docs.aws.amazon.com/consolemobileapp/latest/userguide/what-is-consolemobileapp.html\">AWS Console Mobile App</a> to keep you informed about important events such as <a href=\"https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\">Amazon CloudWatch alarms</a>. With this capability, you can define notification configurations as part of your infrastructure-as-code (IaC) practices and specify notification configurations for specific resource types within your <a href=\"https://aws.amazon.com/cloudformation/\">AWS CloudFormation</a> templates. For example, you can set up notifications to trigger when an <a href=\"https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.html\">Amazon EC2 Auto Scaling</a> group scales out, an <a href=\"https://aws.amazon.com/elasticloadbalancing/\">Elastic Load Balancing (ELB)</a> load balancer is provisioned, or an <a href=\"https://aws.amazon.com/rds/\">Amazon Relational Database Service (Amazon RDS)</a> database is modified. You have granular control over which events will trigger notifications and who should receive them.</p> \\n<a href=\"https://aws.amazon.com/new/\">For a full list of AWS announcements, be sure to keep an eye on the What\\'s New at AWS page.</a> \\n<p>We launched existing services and instance types in additional Regions:</p> \\n<ul> \\n <li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-ec2-m8g-instances-europe-stockholm/\">Amazon EC2 M8g instances are now available in AWS Europe (Stockholm)</a> – These instances offer larger instance sizes with up to three times more vCPUs and memory compared to Graviton3 based Amazon M7g instances. AWS Graviton4 processors are up to 40% faster for databases, 30% faster for web applications, and 45% faster for large Java applications than AWS Graviton3 processors.</li> \\n <li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/aws-direct-connect-expansion-queretaro-mexico/\">AWS announces new AWS Direct Connect location and expansion in Querétaro, Mexico</a> – The Direct Connect service helps you establish a private, physical network connection between AWS and your data center, office, or colocation environment. These private connections can provide a more consistent network experience than those made over the public internet.</li> \\n <li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-ec2-c7i-flex-instances-govcloud-us-west-region/\">Amazon EC2 C7i-flex instances are now available in AWS GovCloud (US-West) Region</a> – These instances expand the EC2 Flex instances portfolio to provide the easiest way for you to get price performance benefits for a majority of compute intensive workloads.</li> \\n <li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/aws-backup-organization-wide-reports-govcloud/\">AWS Backup now supports organization-wide reports in AWS GovCloud (US) Regions</a> – You can use <a href=\"https://aws.amazon.com/backup/\">AWS Backup</a> <a href=\"https://docs.aws.amazon.com/aws-backup/latest/devguide/aws-backup-audit-manager.html\">Audit Manager</a> to generate aggregated cross-account and cross-Region reports on your data protection policies and retrieve operational data about your backup and recovery activities.</li> \\n <li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-opensearch-serverless-asia-pacific-hong-kong-region/\">Amazon OpenSearch Serverless is now available in Asia Pacific (Hong Kong) Region</a> – OpenSearch Serverless is a serverless deployment option for <a href=\"https://aws.amazon.com/opensearch-service/\">Amazon OpenSearch Service</a> that makes it simple to run search and analytics workloads without the complexities of infrastructure management.</li> \\n <li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/aws-backup-asia-pacific-thailand/\">AWS Backup is now available in Asia Pacific (Thailand) Region</a> – Using AWS Backup, you can centrally create and manage backups of your application data, protect your data from inadvertent or malicious actions with immutable recovery points and vaults, and restore your data in the event of a data loss incident.</li> \\n <li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-guardduty-asia-pacific-malaysia-region/\">Amazon GuardDuty is now available in Asia Pacific (Malaysia) Region</a> – You can now use this additional Region to continually monitor and detect anomalous behavior, security threats, and sophisticated multistage attack sequences targeting your AWS accounts to help protect your AWS accounts, workloads, and data.</li> \\n <li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-ec2-r8g-instances-additional-regions/\">Amazon EC2 R8g instances are now available in additional Regions</a> – Amazon EC2 R8g instances are ideal for memory-intensive workloads such as databases, in-memory caches, and real-time big data analytics.</li> \\n <li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-ec2-i8g-instances-europe-frankfurt-region/\">Amazon EC2 I8g instances are now available in Europe (Frankfurt) Region</a> – I8g instances offer the best performance in Amazon EC2 for storage-intensive workloads.</li> \\n <li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-s3-tables-additional-aws-regions/\">Amazon S3 Tables are now available in five additional AWS Regions</a> – <a href=\"https://aws.amazon.com/s3/features/tables/\">Amazon S3 Tables</a> deliver the first cloud object store with built-in Apache Iceberg support and the easiest way to store tabular data at scale. S3 Tables are specifically optimized for analytics workloads, resulting in up to three times faster query performance through continual table optimization compared to unmanaged Iceberg tables, and up to ten times higher transactions per second compared to Iceberg tables stored in general purpose S3 buckets.</li> \\n</ul> \\n<p><span style=\"text-decoration: underline;\"><strong>Other AWS events<br /> </strong></span>Check your calendar and sign up for upcoming AWS events.</p> \\n<p>AWS Summits are free online and in-person events that bring the cloud computing community together to connect, collaborate, and learn about AWS. Stay updated by visiting the official <a href=\"https://aws.amazon.com/events/summits/\">AWS Summit website</a> and sign up for notifications to learn when registration opens for events in your area.</p> \\n<p><a href=\"https://aws.amazon.com/startups/lp/aws-gen-ai-lofts\">AWS GenAI Lofts</a>&nbsp;are collaborative spaces and immersive experiences that showcase AWS expertise in cloud computing and AI. They provide startups and developers with hands-on access to AI products and services, exclusive sessions with industry leaders, and valuable networking opportunities with investors and peers.&nbsp;<a href=\"https://aws.amazon.com/startups/lp/aws-gen-ai-lofts#locations\">Find a GenAI Loft location near you</a>&nbsp;and don’t forget to register.</p> \\n<p>Browse all upcoming&nbsp;<a href=\"https://aws.amazon.com/events/explore-aws-events/\">AWS led in-person and virtual events here</a>.</p> \\n<p>That’s all for this week. Check back next Monday for another Weekly Roundup!</p> \\n<a href=\"https://www.linkedin.com/in/esrakayabali/\">— Esra</a> \\n<p><em>This post is part of our&nbsp;<a href=\"https://aws.amazon.com/blogs/aws/tag/week-in-review/\">Weekly Roundup</a>&nbsp;series. Check back each week for a quick roundup of interesting news and announcements from AWS!</em></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4697b81809f2a75e2d8932d0175b678882ca87b4', embedding=None, metadata={'title': 'Now open — AWS Mexico (Central) Region', 'link': 'https://aws.amazon.com/blogs/aws/now-open-aws-mexico-central-region/', 'date': 'Tue, 14 Jan 2025 14:26:31 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>In February 2024, we <a href=\"https://aws.amazon.com/es/blogs/aws/new-aws-region-in-mexico-is-in-the-works/\">announced</a> plans to expand <a href=\"https://aws.amazon.com\">Amazon Web Services (AWS)</a> infrastructure in Mexico. Today, I’m excited to announce the general availability of the AWS Mexico (Central) Region with three Availability Zones and <a href=\"https://docs.aws.amazon.com/general/latest/gr/rande.html#regional-endpoints\">API code</a> <strong>mx-central-1</strong>. This new <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Region</a> is the first AWS infrastructure Region in Mexico and adds to our growing presence in Latin America.</p> \\n<p>The AWS Region in Mexico represents a significant commitment to the country’s digital future. AWS is planning to invest more than $5 billion in Mexico over 15 years. This AWS Region will provide customers with advanced and secure cloud technologies, including cutting-edge artificial intelligence (AI) and machine learning (ML) capabilities with purpose-built processors, while supporting Mexico’s growing digital economy. With this effort, AWS will support an average of more than 7,000 full-time equivalent jobs annually in Mexico, adding more than $10 billion to Mexico’s gross domestic product (GDP). AWS has also launched a $300,000 <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/aws-incommunities/\">AWS InCommunities</a> Fund in Queretaro to help local groups, schools, and organizations initiate new community projects.</p> \\n<div class=\"wp-caption aligncenter\" id=\"attachment_93007\" style=\"width: 810px;\">\\n <a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/09/mexico-fine-arts-feat-img.png\"><img alt=\"mexico city\" class=\"wp-image-93007 size-full\" height=\"400\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/09/mexico-fine-arts-feat-img.png\" width=\"800\" /></a>\\n <p class=\"wp-caption-text\" id=\"caption-attachment-93007\">Palacio de Bellas Artes, Mexico City</p>\\n</div> \\n<p>The AWS Mexico (Central) Region provides organizations in Mexico with a new option to run their workloads and store data locally. Organizations that need data residency capabilities, enhanced performance with lower latency, or robust security standards can now use infrastructure located in Mexico.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>AWS in Mexico</strong></span><br /> AWS has operated infrastructure in Mexico since 2020. The infrastructure includes seven <a href=\"https://aws.amazon.com/cloudfront/\">Amazon CloudFront</a> edge locations, <a href=\"https://aws.amazon.com/outposts/\">AWS Outposts</a>, and strategic offerings such as <a href=\"https://aws.amazon.com/about-aws/whats-new/2023/01/aws-local-zones-lagos-lima-queretaro/\">AWS Local Zones in Queretaro</a> and <a href=\"https://aws.amazon.com/directconnect/locations/\">AWS Direct Connect</a>. These infrastructure offerings help customers run low-latency applications while maintaining secure connectivity.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Performance and Innovation</strong></span><br /> The AWS Mexico (Central) Region brings AWS infrastructure and services closer to local customers. With this new Region, AWS provides lower latency for customers in Mexico compared to using other AWS Regions. Customers will also be able to use our innovation in purpose-built processors, notably <a href=\"https://aws.amazon.com/ec2/graviton/\">AWS Graviton,</a> that delivers up to 40% better price performance compared to x86-based <a href=\"https://aws.amazon.com/ec2/\">Amazon EC2</a> instances across diverse workloads.</p> \\n<p>This technological advantage extends to our cutting-edge AI and ML capabilities, including:</p> \\n<ul> \\n <li>Advanced ML infrastructure with <a href=\"https://aws.amazon.com/ai/machine-learning/trainium/\">AWS Trainium</a> and <a href=\"https://aws.amazon.com/ai/machine-learning/inferentia/\">AWS Inferentia</a> for scalable generative AI deployment.</li> \\n <li>Purpose-built processors optimized for cloud workloads to deliver best price-performance.</li> \\n</ul> \\n<p><span style=\"text-decoration: underline;\"><strong>Security and Compliance</strong></span><br /> AWS provides <a href=\"https://aws.amazon.com/security/\">comprehensive security capabilities</a> with support for 143 security standards and compliance certifications, including <a href=\"https://aws.amazon.com/compliance/pci-dss-level-1-faqs/\">PCI-DSS</a>, <a href=\"https://aws.amazon.com/compliance/hipaa-compliance/\">HIPAA</a>/<a href=\"https://www.hhs.gov/hipaa/for-professionals/special-topics/hitech-act-enforcement-interim-final-rule/index.html\">HITECH</a>, <a href=\"https://aws.amazon.com/compliance/fedramp/\">FedRAMP</a>, <a href=\"https://aws.amazon.com/compliance/gdpr-center/\">GDPR</a>, <a href=\"https://aws.amazon.com/compliance/fips/\">FIPS 140-2</a>, and <a href=\"https://aws.amazon.com/compliance/nist/\">NIST</a> 800-171. All AWS customers own their data, choose where to store it, and decide if/when to move it. This means customers storing content in the AWS Mexico (Central) Region have the assurance that their content will not leave Mexico, unless they chose to move it.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>AWS Customers in Mexico</strong></span><br /> Leading Mexican organizations are already achieving significant results with AWS. Companies such as <a href=\"https://aeromexico.com/\">Aeroméxico</a>, <a href=\"https://www.santander.com.mx/\">Banco Santander Mexico</a>, <a href=\"https://cinepolis.com/\">Cinépolis</a>, <a href=\"https://www.gruposalinas.com/\">Grupo Salinas</a>, <a href=\"http://www.kavak.com\">Kavak</a>, <a href=\"https://www.palaceresorts.com/\">Palace Resorts</a>, and <a href=\"https://www.vector.com.mx/\">Vector Casa de Bolsa</a> are running mission-critical workloads on AWS. Here are key examples:</p> \\n<p><a href=\"https://www.bbva.com/en/innovation/bbva-selects-aws-to-accelerate-its-data-driven-transformation/\"><strong>BBVA</strong></a>, a leading multinational financial services company, is using AWS to accelerate its data-driven transformation. Using <a href=\"https://aws.amazon.com/sagemaker/\">Amazon SageMaker</a> and <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>, BBVA is empowering over 1,000 data scientists to build, train, and deploy machine learning models efficiently. This technology enables BBVA to explore advanced technologies and create innovative financial solutions, supporting their goal of becoming a true data and AI-driven digital organization.</p> \\n<p class=\"jss225\"><strong><a href=\"https://grupomultimedios.com/\">Grupo Multimedios</a>, </strong>a leading Mexican media group, is pioneering the use of generative AI, by implementing Amazon Bedrock for their <a href=\"https://aws.amazon.com/what-is/media-asset-management/\">media asset manager (MAM)</a>, reducing content research time by 88%, decreasing news generation time by 40%, and increasing content production by 70% (250 additional news items daily). As the fastest-growing media group embracing technological leadership, their AI implementation demonstrates a commitment to innovation while streamlining operations.</p> \\n<p><strong><a href=\"https://www.bowheadhealth.com/\">Bowhead Health</a></strong>, a digital healthcare company, is revolutionizing cancer research by using Amazon Bedrock to accelerate the research pipeline. The company has built a vast, de-identified dataset that’s ready for analysis without traditional recruitment barriers. Bowhead Health also delivers robust, real-world insights to drive faster breakthroughs in oncology drug development.</p> \\n<p><strong><a href=\"https://aws.amazon.com/solutions/case-studies/skyalert/?did=cr_card&amp;trk=cr_card\">SkyAlert</a></strong>, an innovative technology company protecting millions in earthquake-prone areas, transformed its alert system by migrating to AWS in 2018. Before AWS, their system required 20 virtual machines and experienced significant delays during critical moments. Using <a href=\"https://aws.amazon.com/lambda/\">AWS Lambda</a>, <a href=\"https://aws.amazon.com/fargate/\">AWS Fargate</a>, and <a href=\"https://aws.amazon.com/pinpoint/\">Amazon Pinpoint</a>, they can now scale automatically and deliver messages to users quickly. With the opening of the AWS Mexico (Central) Region, SkyAlert anticipates further improvements to their services with local AWS infrastructure. As <strong>Santiago Cantú, Co-Founder of SkyAlert,</strong> explains, “The opening of the AWS Region in Mexico is an extremely important event for SkyAlert and for the security of those who trust us. Having local AWS infrastructure will improve our ability to deliver critical alerts, which potentially save lives, even faster and more reliably. This perfectly aligns with our mission to provide the most robust and advanced earthquake early warning system available. The new Region will allow us to take even greater advantage of AWS services, ensuring that we continue to be at the forefront of innovation in disaster preparedness.”</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Building Skills Together</strong></span><br /> AWS has made significant investments in upskilling initiatives in Mexico including:</p> \\n<ul> \\n <li>Training over 500,000 individuals in cloud technology since 2017.</li> \\n <li>Collaborating with the the <a href=\"https://e.economia.gob.mx/\">Ministry of Economy</a> to train 138,000 people in digital technology as of 2024.</li> \\n <li>Partnering with universities like <a href=\"https://www.up.edu.mx/\">Universidad Panamericana</a> and <a href=\"https://tec.mx\">Tec de Monterrey</a> to teach digital skills.</li> \\n <li>Training programs with <a href=\"https://canacintra.org.mx/camara/\">Canacintra</a> for 20,000 Small and Medium Businesses (SMB)&nbsp;leaders.</li> \\n</ul> \\n<p><span style=\"text-decoration: underline;\"><strong>AWS Commitment to Sustainability</strong></span><br /> Amazon is committed to reaching net-zero carbon across its business by 2040. A <a href=\"https://sustainability.aboutamazon.com/carbon-reduction-aws.pdf\">recent Accenture study</a> shows that running workloads on AWS is up to 4.1 times more energy-efficient than on-premises environments. When workloads are optimized on AWS, the associated carbon footprint can be lowered by up to 99%. The AWS Mexico (Central) Region incorporates sustainable design practices, using air-cooling technology that eliminates the need for cooling water in operations. With this new Region, customers will also benefit from AWS sustainability efforts across its infrastructure. To learn more about sustainability at AWS, visit the <a href=\"http://aws.amazon.com/about-aws/sustainability\">AWS Cloud sustainability page</a>.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Things to know</strong></span><br /> <strong>AWS Community in Mexico</strong> – The AWS Community in Mexico is one of the most vibrant in Latin America, with <a href=\"https://aws.amazon.com/developer/community/community-builders/community-builders-directory/?cb-cards.sort-by=item.additionalFields.cbName&amp;cb-cards.sort-order=asc&amp;awsf.builder-category=*all&amp;awsf.location=*all&amp;awsf.year=*all&amp;cb-cards.q=mexico&amp;cb-cards.q_operator=AND\">26+ AWS Community Builders</a> and 15 <a href=\"https://aws.amazon.com/es/developer/community/usergroups/?community-user-groups-cards.sort-by=item.additionalFields.ugName&amp;community-user-groups-cards.sort-order=asc&amp;awsf.location=*all&amp;awsf.category=*all&amp;community-user-groups-cards.q=mexico&amp;community-user-groups-cards.q_operator=AND\">AWS User Groups</a>. These groups are located in <a href=\"https://www.meetup.com/Amazon-Web-Services-Jalisco/\">Jalisco</a>, Puebla, <a href=\"https://www.meetup.com/AWS-User-Group-Monterrey/\">Monterrey</a>, <a href=\"https://www.meetup.com/comunidad-aws-merida-yucatan/\">Mérida</a>, <a href=\"https://www.meetup.com/es-ES/ajolotesenlanube/\">Mexico City</a>, Mexicali, Cancún, León, <a href=\"https://www.meetup.com/Amazon-Web-Services-Queretaro/\">Querétaro</a>, San Luis Potosí, <a href=\"https://www.meetup.com/aws-user-group-ensenada/\">Ensenada</a>, <a href=\"https://www.meetup.com/aws-user-group-saltillo/\">Saltillo</a>, Tijuana, and <a href=\"https://www.meetup.com/aws-ug-villahermosa/\">Villahermosa</a>, plus a specialized User Group called <a href=\"https://www.meetup.com/Embajadoras-Cloud/\">Embajadoras cloud</a> (Cloud ambassadors) focused on women’s professional development. Together, these groups comprise 9,000+ total members.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/mexico_CD.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-91388\" height=\"329\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/mexico_CD-1024x329.png\" width=\"1024\" /></a></p> \\n<p><strong><a href=\"https://aws.amazon.com/about-aws/global-infrastructure/\">AWS Global footprint</a> </strong>– With this launch, AWS now spans 114 Availability Zones within 36 geographic Regions.</p> \\n<p><strong>Available now</strong> – The new AWS Mexico (Central) Region is ready to support your business, and you can find a detailed list of the services available in this Region on the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/\">AWS Services by Region</a>&nbsp;page.</p> \\n<p>To start building in <strong>mx-central-1</strong>, visit the&nbsp;<a href=\"https://aws.amazon.com/about-aws/global-infrastructure/\">AWS Global Infrastructure</a> page.</p> \\n<p>Thanks to <a href=\"https://www.linkedin.com/in/vikomex/\">David Victoria</a> for the <a href=\"https://www.linkedin.com/company/awscommunitymx/\">AWS Community México</a> 2024 photo.</p> \\n<p>— <a href=\"https://www.linkedin.com/in/lizfue/\">Eli</a></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='1acc45900a7fda30d9bf8fb9760ddc9d5552bb16', embedding=None, metadata={'title': 'AWS Weekly Roundup: New Asia Pacific Region, DynamoDB updates, Amazon Q developer, and more (January 13, 2025)', 'link': 'https://aws.amazon.com/blogs/aws/aws-weekly-roundup-new-asia-pacific-region-dynamodb-updates-amazon-q-developer-and-more-january-13-2025/', 'date': 'Mon, 13 Jan 2025 17:29:48 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p><img alt=\"\" class=\"alignright wp-image-93258\" height=\"336\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/01/10/eraser_20250110103729866-300x240.jpg\" width=\"420\" />As we move into the second week of 2025, China is celebrating Laba Festival (腊八节), a traditional holiday, which marks the beginning of Chinese New Year preparations. On this day, Chinese people prepare <a href=\"https://en.wikipedia.org/wiki/Laba_congee\">Laba congee</a>, a special porridge combining various grains, dried fruits, and nuts. This</p> \\n<p>nutritious mixture symbolizes harmony, prosperity, and good fortune — with each ingredient representing the diversity and abundance of life. This traditional practice dates back to when Buddha achieved enlightenment after consuming rice porridge, making it a symbol of both material and spiritual nourishment. The festival, occurring on the eighth day of the twelfth lunar month, marks the countdown to Spring Festival, China’s most significant traditional holiday celebrating family reunion and renewal.</p> \\n<p>As our global tech community grows, such cultural celebrations remind us of the importance of inclusive innovation and shared progress.</p> \\n<p><strong>Last week’s launches</strong></p> \\n<p>Let’s take a look at what <a href=\"https://aws.amazon.com/\">Amazon Web Services (AWS)</a> launched in this week.</p> \\n<p><a href=\"https://aws.amazon.com/blogs/aws/announcing-the-new-aws-asia-pacific-thailand-region/\">New AWS Asia Pacific (Thailand) Region</a>– AWS has expanded its global infrastructure with the launch of the new Asia Pacific (Thailand) <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\">AWS Region</a>, featuring three Availability Zones. With this addition, customers in Thailand and throughout Southeast Asia can serve customers with reduced latency while maintaining data residency within Thailand. The newly launched Region supports the complete range of AWS services and strengthens our presence in the rapidly growing ASEAN market.</p> \\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/aws-direct-connect-location-expansion-bangkok-thailand/\">New AWS Direct Connect location in Bangkok</a> – Following the launch of our Thailand Region, we’ve established a new <a href=\"https://aws.amazon.com/directconnect/\">AWS Direct Connect</a> location in Bangkok and expanded our existing infrastructure. This addition provides customers in Thailand with improved connectivity options and reduced network latency when accessing AWS services.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/01/09/news-2024-thailand-region.jpg\"><img alt=\"\" class=\"aligncenter size-full wp-image-93249\" height=\"927\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/01/09/news-2024-thailand-region.jpg\" width=\"1819\" /></a></p> \\n<p><em>Database and analytics</em></p> \\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-dynamodb-configurable-point-in-time-recovery-periods/\">Configurable point-in-time recovery periods for Amazon DynamoDB</a> – <a href=\"https://aws.amazon.com/dynamodb/?nc2=type_a\">Amazon DynamoDB</a> now enables customizable point-in-time recovery (PITR) periods, which means customers can specify recovery durations ranging from 1 to 35 days on a per-table basis. This enhancement enables organizations to meet precise compliance requirements while maximizing cost-efficiency. The feature is now available across all AWS Regions, including AWS GovCloud (US West) and China Regions. This flexibility in data recovery periods empowers customers to align their backup policies precisely with their business requirements and regulatory obligations.</p> \\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-msk-connect-apis-aws-privatelink/\">Amazon MSK Connect APIs with AWS PrivateLink</a> – <a href=\"https://aws.amazon.com/msk/features/msk-connect/\">Amazon Managed Streaming for Apache Kafka Connect (Amazon MSK Connect)</a> APIs now support<a href=\"https://aws.amazon.com/privatelink/\"> AWS PrivateLink</a>, giving customers access to MSK Connect APIs through private endpoints within their virtual private cloud (VPC). This enhancement provides increased security and reduced data exposure by keeping traffic within the AWS network.</p> \\n<p><em>Generative AI and machine learning</em></p> \\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-q-developer-sagemaker-code-editor-ide/\">Amazon Q Developer in SageMaker Code Editor</a>– <a href=\"https://aws.amazon.com/q/developer/?nc2=type_a\">Amazon Q Developer</a> is now integrated into the <a href=\"https://aws.amazon.com/sagemaker/?nc2=type_a\">Amazon SageMaker</a> Code Editor integrated development environment (IDE), enhancing the developer’s experience with AI-powered code assistance. Intelligent code suggestions, documentation assistance, and contextual recommendations are now directly available within the SageMaker development environment.</p> \\n<p><em>Management and governance</em></p> \\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/20-additional-aws-systems-manager-automation-runbook-recommendations-chatbot/\">AWS Systems Manager Automation in AWS Chatbot</a> – <a href=\"https://aws.amazon.com/chatbot/?nc2=type_a\">AWS Chatbot</a> now offers 20 additional <a href=\"https://aws.amazon.com/systems-manager/?nc2=type_a\">AWS Systems Manager</a> Automation runbook recommendations, expanding its capabilities for automated operations management. These new recommendations help customers streamline their operational tasks and implement best practices more efficiently through chat-based interactions.</p> \\n<p><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/analyzing-aws-transit-gateway-data-processing-charges-with-cost-allocation-tags/\">AWS Transit Gateway cost analysis enhancement</a> – We’ve introduced new capabilities for analyzing Transit Gateway data processing charges using cost allocation tags. This feature provides improved visibility and control over networking costs, enabling organizations to track and optimize <a href=\"https://aws.amazon.com/transit-gateway/\">AWS Transit Gateway</a> usage efficiently. The enhanced cost analysis tools deliver detailed insights into network traffic patterns and associated costs.</p> \\n<p><strong>Other AWS news and highlights</strong></p> \\n<p><a href=\"https://aws.amazon.com/blogs/devops/the-most-visited-devops-and-developer-productivity-blog-posts-in-2024-copy/\">2024’s most popular DevOps blog posts</a> – The retrospective blog post “The most visited DevOps and Developer Productivity blog posts in 2024” has reached the top one position on this week’s AWS most popular articles chart. This compilation presents the most influential DevOps content from 2024, offering insights into trending topics and best practices. The collection examines key developments in continuous integration and continuous development (CI/CD), infrastructure as code (IaC), and automation practices.</p> \\n<p><a href=\"https://aws.amazon.com/blogs/security/new-aws-skill-builder-course-available-securing-generative-ai-on-aws/\">New security course for generative AI</a> – <a href=\"https://explore.skillbuilder.aws/learn\">AWS Skill Builder</a> has released a new course focusing on securing <a href=\"https://aws.amazon.com/generative-ai/\">generative AI</a> applications on AWS. This comprehensive training teaches professionals to implement security best practices for <a href=\"https://aws.amazon.com/training/learn-about/machine-learning/\">artificial intelligence and machine learning (AI/ML)</a> workloads, addressing data protection, model security, and compliance requirements. The course meets the growing demand for specialized security knowledge in the rapidly evolving field of generative AI.</p> \\n<p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/01/amazon-connect-contact-lens-free-trials-conversational-analytics-performance-evaluations/\">Amazon Connect Contact Lens free trials</a> – We’re introducing free trials for first-time users of <a href=\"https://aws.amazon.com/connect/contact-lens/\">Amazon Connect Contact Lens</a> conversational analytics and performance evaluations. New customers can process up to 100,000 voice minutes monthly at no cost for 2 months, and first-time performance evaluation users receive a 30-day free trial starting with their first evaluation. With this initiative, customers can experience Contact Lens capabilities in their environment without additional costs. The free trials are available across all <a href=\"https://docs.aws.amazon.com/connect/latest/adminguide/regions.html#contactlens_region\">AWS Regions</a> where Contact Lens is supported.</p> \\n<p>For a full list of AWS announcements, be sure to keep an eye on the <a href=\"https://aws.amazon.com/new/\">What’s New with AWS page.</a></p> \\n<p>Whether you’re a developer, architect, business leader, or you’re starting your cloud journey – and regardless of what 2024 brought your way – 2025 presents new opportunities for everyone.</p> \\n<p>This post is part of our&nbsp;<a href=\"https://aws.amazon.com/blogs/aws/tag/week-in-review/\">Weekly Roundup series</a>. Check back each week for a quick roundup of interesting news and announcements from AWS!</p> \\n<p>– <a href=\"https://www.linkedin.com/in/zhengyubin714/\">Betty</a></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3acab191e810ab04e856c629a911bcd01c319944', embedding=None, metadata={'title': 'Announcing the new AWS Asia Pacific (Thailand) Region', 'link': 'https://aws.amazon.com/blogs/aws/announcing-the-new-aws-asia-pacific-thailand-region/', 'date': 'Wed, 08 Jan 2025 02:30:36 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>Today, we’re pleased to announce that the AWS Asia Pacific (Thailand) Region is now generally available with three Availability Zones and API name <code>ap-southeast-7</code>.</p> \\n<p>The AWS Asia Pacific (Thailand) Region is the first infrastructure Region in Thailand and the fourteenth Region in Asia Pacific, joining existing Regions in Hong Kong, Hyderabad, Jakarta, Malaysia, Melbourne, Mumbai, Osaka, Seoul, Singapore, Sydney, and Tokyo, as well as the Beijing and Ningxia China Regions.</p> \\n<div class=\"wp-caption aligncenter\" id=\"attachment_92882\" style=\"width: 1829px;\">\\n <img alt=\"\" class=\"wp-image-92882 size-full\" height=\"927\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/03/news-2024-thailand-region.jpg\" style=\"border: 1px solid black; padding: 3px;\" width=\"1819\" />\\n <p class=\"wp-caption-text\" id=\"caption-attachment-92882\"><em>Lumphini Park, one of the largest green spaces in central Bangkok spanning 142 acres.</em></p>\\n</div> \\n<p>The adoption of cloud computing has gained significant momentum in Thailand, driven by evolving business needs and government initiatives such as <a href=\"https://www.boi.go.th/upload/content/Thailand,%20Taking%20off%20to%20new%20heights%20@%20belgium_5ab4e8042850e.pdf\">Thailand 4.0</a>. These initiatives aim to transform Thailand into an innovation-driven economy by using emerging technologies to enhance productivity, competitiveness, and sustainable growth.</p> \\n<p>The new AWS Region will help startups, enterprises, government agencies, educational institutions, and nonprofit organizations run their applications and serve end users while maintaining data residency in Thailand. This aligns with Thailand’s digital transformation goals and the growing demand for cloud services. Over the next 15 years, Amazon Web Services (AWS) planned investments in Thailand are estimated to contribute $10B to Thailand’s Gross Domestic Product (GDP) and support an estimated average of 11,000 full-time equivalent (FTE) jobs in local Thai businesses annually.</p> \\n<p><strong><span style=\"text-decoration: underline;\">Growing presence of AWS in Thailand</span><br /></strong>Our journey in Thailand began in 2013 with the first AWS office in Bangkok. Since then, AWS has continuously expanded its infrastructure and services in the country:</p> \\n<p><strong><a href=\"https://aws.amazon.com/cloudfront/\">Amazon CloudFront</a></strong> – Since 2020, AWS has established six Amazon CloudFront edge locations throughout Thailand. These edge locations are part of the highly secure and programmable AWS content delivery network (CDN), designed to accelerate the delivery of data, videos, applications, and APIs to users worldwide with low latency and high transfer speeds.</p> \\n<p><strong><a href=\"https://aws.amazon.com/outposts/\">AWS Outposts</a></strong> – In the same year, 2020, AWS introduced AWS Outposts to the Thai market. As a fully managed solution, AWS Outposts brings AWS infrastructure and services to virtually any on-premises or edge location, enabling a truly consistent hybrid experience. This service is particularly valuable for workloads that require low latency, local data processing, or local data storage.</p> \\n<p><strong><a href=\"https://aws.amazon.com/about-aws/global-infrastructure/localzones/\">AWS Local Zones</a></strong> – In 2022, AWS strengthened its commitment to Thailand by launching AWS Local Zones in Bangkok. This infrastructure deployment places compute, storage, database, and other select services closer to large population, industry, and IT centers. As a result, customers can deliver applications requiring single-digit millisecond latency to end users.</p> \\n<p><strong><a href=\"https://aws.amazon.com/directconnect/\">AWS Direct Connect</a></strong> – AWS established a AWS Direct Connect location in Bangkok in 2023 to enhance connectivity options and added a new AWS Direct Connect location with the launch of the AWS Asia Pacific (Thailand) Region. Customers can use AWS Direct Connect to establish secure and dedicated network connections to their AWS resources, providing improved network performance and reduced bandwidth costs.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>AWS customer success stories in Thailand<br /></strong></span>Organizations in Thailand are using our services to drive innovation and transformation. Here are a few examples:</p> \\n<p><a href=\"https://aws.amazon.com/solutions/case-studies/2c2p-case-study/\"><strong>2C2P</strong></a><br />2C2P, a leading Thailand-based FinTech startup, chose AWS for its robust security capabilities. As an omnichannel payment service provider in Southeast Asia, the company processes millions of customer payments globally using AWS CloudHSM for cryptographic key management, AWS Shield for distributed denial of service (DDoS) protection, and AWS Secrets Manager to safeguard sensitive credentials.</p> \\n<p>“Through AWS, we’ve unlocked the power to securely, dynamically, and compliantly scale to meet the surge in payment transaction volumes. AWS CloudHSM plays a pivotal role in fulfilling compliance requirements and propelling us toward accelerated business expansion,” says Myo Zaw, Chief Technology Officer at 2C2P.</p> \\n<p><strong><a href=\"https://press.aboutamazon.com/aws/2024/6/southeast-asian-brands-thrive-with-acommerces-new-generative-ai-powered-data-platform-powered-by-aws\">aCommerce</a><br /></strong>aCommerce, the largest ecommerce enabler in Southeast Asia, has revolutionized market intelligence by launching AskIQ, a generative AI–powered feature on AWS. This software as a service (SaaS) platform provides the world’s leading brands with comprehensive competitor and category performance tracking capabilities across Southeast Asia’s largest ecommerce sites.</p> \\n<p>Leena Chanvirach, VP of Data Products at aCommerce Group, emphasizes the strategic value of their AWS collaboration: “Our collaboration with AWS allows clients to double down on their core competencies and business priorities. This best-of-both-worlds approach gives brands a competitive edge without the burden of building and maintaining sophisticated data infrastructure in-house.”</p> \\n<p><a href=\"https://aws.amazon.com/solutions/case-studies/ascend-money-case-study/\"><strong>Ascend Money</strong></a><br />Ascend Money, a leading Southeast Asian FinTech company, achieved a 70 percent reduction in compute costs while simultaneously improving application performance by up to 40 percent in certain workloads. Ascend Money implemented a sophisticated compute strategy using Amazon EC2 instances, resulting in significant operational improvements.</p> \\n<p>“AWS has significantly improved our performance, enabling us to deliver more innovative services to our customers,” says Peerawit Phuangkaeo, Head of Technical Operations at Ascend Money.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Building cloud skills together<br /></strong></span>AWS has built comprehensive programs for cloud education and skills development in Thailand, training more than 50,000 individuals in cloud skills since 2017. Here are some of the programs:</p> \\n<p><a href=\"https://skillbuilder.aws/\"><strong>AWS Skill Builder</strong></a><br />AWS Skill Builder&nbsp;is an online learning center where you can learn from AWS experts and build cloud skills online. AWS has made cloud education more accessible to Thai learners by offering more than 600 courses, with 106 courses specifically available in Thai language. The recent launch of the Amazon AI Ready initiative has further expanded the learning opportunities, particularly in the growing field of AI.</p> \\n<p><strong><a href=\"https://aws.amazon.com/education/awseducate/\">AWS Educate</a><br /></strong>Since its introduction in 2016, AWS Educate has played a transformative role in Thai education. The program has successfully integrated cloud computing into educational curricula across Thailand, providing students with direct access to AWS resources and hands-on experience. The impact has been substantial, with over 20,000 Thai students enrolled in the program. Beyond student education, AWS Educate has invested in training Thai educators, preparing them to deliver engaging and practical cloud computing courses that prepare students for the demands of the digital economy.</p> \\n<p><strong><a href=\"https://aws.amazon.com/training/awsacademy/\">AWS Academy</a><br /></strong>AWS Academy has been instrumental in connecting academic learning with industry needs since its launch in Thailand in 2017. Through strategic partnerships with more than 30 leading universities and colleges across the country, AWS Academy has created a robust pipeline of cloud-skilled professionals. The program provides educational institutions with comprehensive cloud computing curricula that align with industry needs, making sure that students graduate with practical, job-ready skills.</p> \\n<p>Through these various initiatives and programs, AWS is not just providing educational resources, it’s building a foundation for Thailand’s digital future by helping equip the workforce with the necessary skills to use cloud technologies effectively.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Supporting sustainable innovation in Thailand<br /></strong></span>The AWS commitment to sustainability extends to supporting innovative companies in Thailand that are driving environmental initiatives.</p> \\n<p><a href=\"https://bo-da.co/en-us/about-us\"><strong>BODA Technology &amp; Consultancy</strong></a><br />BODA, an AWS powered sustainability startup, uses AWS IoT Core to develop AI-powered solutions for energy efficiency optimization. The company has successfully improved operations in over 100,000 buildings and factories across Thailand, enabling these facilities to maximize efficiency while reducing costs and environmental impact.</p> \\n<p><a href=\"https://www.gpscgroup.com/en/home\"><strong>GPSC Group</strong></a><br />GPSC Group, a leading sustainable power company in Thailand, demonstrates how AWS supports the energy sector’s digital transformation. Following the merger between Global Power Synergy Public Company and Glow Energy, the group chose AWS Cloud for migrating its photovoltaic solar plant operations. Working with AWS and AWS Partner <a href=\"https://www.dailitech.com/\">Dailitech</a>, GPSC Group has achieved a 20–25 percent reduction in hardware, software, and licensing costs since moving to the cloud.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Things to know<br /></strong></span><strong>AWS Community in Thailand</strong> — Thailand is home to two AWS Heroes, seven AWS Community Builders, and more than 17,000 members of the AWS User Group. If you’re interested in joining AWS User Group Thailand, visit their <a href=\"https://www.facebook.com/groups/696722700418928\">Facebook</a> page.</p> \\n<p><img alt=\"\" class=\"aligncenter size-full wp-image-93165\" height=\"1045\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/18/2024-news-thailand-community.jpg\" style=\"border: 1px solid black; padding: 3px;\" width=\"1567\" /></p> \\n<p><strong>AWS Global footprint</strong> — AWS now spans 111 Availability Zones within 35 geographic regions worldwide. We have announced plans for 15 more Availability Zones and five more AWS Regions in Germany, Taiwan, Mexico, the Kingdom of Saudi Arabia, and New Zealand.&nbsp;</p> \\n<p>The new Asia Pacific (Thailand) Region is ready to support your business. To learn more, visit the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/\">AWS Global Infrastructure</a> page and start building on <code>ap-southeast-7</code>!</p> \\n<p>Happy building!<br />— <a href=\"https://linkedin.com/in/donnieprakoso\">Donnie</a></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4fa89b3af496404ca2ee8f7c6b9813035cee90e6', embedding=None, metadata={'title': 'Happy New Year! AWS Weekly Roundup: 2025 Tech Predictions, Llama 3.3 70B, Stable Diffusion 3.5 Large, custom billing view, and more (January 6, 2025)', 'link': 'https://aws.amazon.com/blogs/aws/happy-new-year-aws-weekly-roundup-2025-tech-predictions-llama-3-3-70b-stable-diffusion-3-5-large-custom-billing-view-and-more-january-6-2025/', 'date': 'Mon, 06 Jan 2025 18:12:56 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>Happy New Year! We are witnessing technology augment human ingenuity in inspiring ways. In the coming years, using technology for positive impact will redefine the way we think about success. Amazon CTO, Dr. Werner Vogels, offers five forward-looking tech predictions for 2025, and beyond:<a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/01/03/WV.png\"><img alt=\"\" class=\"wp-image-93206 size-medium alignright\" height=\"300\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/01/03/WV-232x300.png\" width=\"232\" /></a></p> \\n<ul> \\n <li>The workforce of tomorrow is mission-driven</li> \\n <li>A new era of energy efficiency drives innovation</li> \\n <li>Technology tips the scales in the discovery of truth</li> \\n <li>Open data drives decentralized disaster preparedness</li> \\n <li>Intention-driven consumer technology takes hold</li> \\n</ul> \\n<p>Download the Werner Vogels’ <a href=\"https://d1.awsstatic.com/executive-insights/en_US/werner_vogels_tech_predictions_for_2025_and_beyond_ebook.pdf\">Tech Predictions for 2025 and Beyond ebook,</a> or read Werner’s <a href=\"https://www.allthingsdistributed.com/2024/12/tech-predictions-for-2025-and-beyond.html\">All Things Distributed blog</a> to learn more about how these technological trends are shaping our world and paving the way for a more innovative, efficient, and purposeful future.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>AWS re:Invent 2025 videos and re:Caps</strong></span><br /> If you’re looking to catch up on re:Invent announcements or delve deeper into the latest AWS innovations, you have several options available:</p> \\n<ul> \\n <li><a href=\"https://reinvent.awsevents.com/on-demand/\">Watch the keynotes, innovation talks, and breakout sessions</a> on-demand.</li> \\n <li><a href=\"https://community.aws/recaps?postLogin=downloadPDF\">Download the summary of key AWS re:Invent announcements.</a></li> \\n <li>Attend <a href=\"https://community.aws/recaps\">free in-person community re:Cap sessions</a> organized by volunteers from <a href=\"https://aws.amazon.com/developer/community/usergroups/\">AWS User Groups</a> around the world.</li> \\n</ul> \\n<p><span style=\"text-decoration: underline;\"><strong>Launches from the last few weeks</strong></span><br /> Since our <a href=\"https://aws.amazon.com/blogs/aws/aws-weekly-roundup-amazon-ec2-f2-instances-amazon-bedrock-guardrails-price-reduction-amazon-ses-update-and-more-december-16-2024/\">last week in review</a> on December 16, 2024, I’d like to highlight some launches from year end, as well as from last week:</p> \\n<p><a href=\"https://aws.amazon.com/blogs/machine-learning/llama-3-3-70b-now-available-in-amazon-sagemaker-jumpstart/\">Availability of Llama 3.3 70B in Amazon SageMaker JumpStart</a> and in <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/metas-llama-3-3-70b-model-amazon-bedrock/\">Amazon Bedrock</a>&nbsp;–&nbsp;Meta’s&nbsp;<a href=\"https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/\">Llama 3.3 70B</a> represents a significant advancement in model efficiency and performance optimization. Llama 3.3 70B is a text-only instruction-tuned model that provides enhanced performance relative to <a href=\"https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1\">Llama 3.1 70B</a> and <a href=\"https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2\">Llama 3.2 90B</a>, when used for text-only applications. You can now use the model in both <a href=\"https://aws.amazon.com/sagemaker-ai/jumpstart/\">Amazon SageMaker JumpStart,</a> as well as <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>.</p> \\n<p><a href=\"https://aws.amazon.com/blogs/aws/stable-diffusion-3-5-large-is-now-available-in-amazon-bedrock/\">Availability of Stable Diffusion 3.5 Large in Amazon Bedrock</a> –&nbsp;<a href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\">Stable Diffusion 3.5 Large</a> by <a href=\"https://stability.ai/\">Stability AI</a> is the most powerful model in the Stable Diffusion family at 8.1 billion parameters trained on <a href=\"https://aws.amazon.com/sagemaker-ai/hyperpod/\">Amazon SageMaker HyperPod</a>. You can now use <a href=\"https://aws.amazon.com/bedrock/stability-ai/\">Stable Diffusion 3.5 Large in Amazon Bedrock</a> to generate high-quality images from text descriptions.</p> \\n<p><a href=\"https://aws.amazon.com/blogs/big-data/introducing-the-new-amazon-kinesis-source-connector-for-apache-flink/\">New Amazon Kinesis source connector for Apache Flink</a>&nbsp;–&nbsp;The <a href=\"https://flink.apache.org/\">Apache Flink</a> community has released version 5.0.0 of AWS services connectors, an AWS open source contribution. This release introduces the <a href=\"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/kinesis/#kinesis-streams-source\">Kinesis Streams Source</a>, a new connector for reading data from <a href=\"https://aws.amazon.com/kinesis/data-streams/\">Amazon Kinesis Data Streams</a>, replacing the previous <a href=\"https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/connectors/datastream/kinesis/#kinesis-consumer\">Kinesis Consumer</a>.</p> \\n<p><a href=\"https://aws.amazon.com/blogs/desktop-and-application-streaming/announcing-support-for-amazon-workspaces-personal-with-aws-global-accelerator/\">Support of AWS Global Accelerator in Amazon WorkSpaces Personal</a> – <a href=\"https://aws.amazon.com/workspaces-family/workspaces/features/personal/\">Amazon WorkSpaces Personal</a> now integrates with&nbsp;<a href=\"https://aws.amazon.com/global-accelerator/\">AWS Global Accelerator (AGA)</a>&nbsp;to enhance WorkSpaces connection performance by optimizing streaming traffic through the <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/\">AWS Global Network and edge locations</a>. This feature particularly benefits customers whose end users connect to WorkSpaces across long distances. You can enable the AGA feature at either the WorkSpaces directory level or for individual WorkSpaces running&nbsp;<a href=\"https://docs.aws.amazon.com/dcv/latest/adminguide/what-is-dcv.html\">Amazon DCV protocol</a>.</p> \\n<p><a href=\"https://aws.amazon.com/blogs/mt/aws-launches-enhanced-aws-resource-explorer-features-for-new-resource-insights/\">New resource insights in AWS Resource Explorer</a>&nbsp;–&nbsp;With the enhanced Resource Explorer experience, relevant data and insights from multiple AWS services is centralized for <a href=\"https://docs.aws.amazon.com/resource-explorer/latest/userguide/supported-resource-types.html\">supported resource types</a>. You can use the new features to take actions on resources directly from the Resource Explorer console, such as manage tags, add resources to applications, and get additional information about a resource with <a href=\"https://aws.amazon.com/q/\">Amazon Q</a>.</p> \\n<p><a href=\"https://aws.amazon.com/blogs/aws-cloud-financial-management/introducing-custom-billing-views-tailored-cost-and-usage-view-relevant-for-your-stakeholders/\">General availability of custom billing view in AWS Billing and Cost Management</a>&nbsp;–&nbsp;<a href=\"https://docs.aws.amazon.com/cost-management/latest/userguide/billing-view.html\">Custom billing view</a> allows you to provide application and business unit owners access to relevant cost management data across multiple AWS accounts using a single view in <a href=\"https://docs.aws.amazon.com/cost-management/latest/userguide/ce-getting-started.html\">AWS Cost Explorer</a> without granting access to the <a href=\"https://docs.aws.amazon.com/managedservices/latest/userguide/management-account.html\">AWS management account</a>. You can create filtered views of cost management data based on cost allocation tags or specific AWS accounts.</p> \\n<p><a href=\"https://aws.amazon.com/new/\">For a full list of AWS announcements, be sure to keep an eye on the What’s New at AWS page.</a></p> \\n<p><span style=\"text-decoration: underline;\"><strong>Upcoming AWS events</strong></span><br /> Check your calendars and sign up for these AWS events:</p> \\n<p><a href=\"https://aws.amazon.com/automotive/ces-2025/\">AWS at CES 2025</a> (January 7-10) – AWS will be representing some of the latest cloud services and solutions that are purpose built for the automotive, mobility, transportation, and manufacturing industries. Join us to learn about the latest cloud capabilities across generative AI, software define vehicles, product engineering, sustainability, new digital customer experiences, connected mobility, autonomous driving, and so much more, in&nbsp;<a href=\"https://developer.amazon.com/ces\">Amazon Experience Area</a>.</p> \\n<p><a href=\"https://aws.amazon.com/retail/nrf-2025/\">AWS at NRF 2025</a> (January 12-14) – Join AWS at the retail’s big show to see generative AI and cutting-edge technologies in action. Listen to innovative big ideas sessions, curated TechTalks, and experience the latest in retail trends, technologies, and more.</p> \\n<p><a href=\"https://aws.amazon.com/blogs/business-intelligence/january-2025-amazon-quicksight-events/\">Amazon QuickSight Learning Series</a> – Kick off your 2025 by supercharging your data skills. Join the online Learning Series in January to learn about the cutting-edge&nbsp;<a href=\"https://community.amazonquicksight.com/c/reinvent/quicksight-reinvent/32\">features of Amazon QuickSight unveiled at re:Invent 2024</a>.</p> \\n<p><a href=\"https://aws.amazon.com/events/explore-aws-events/\">You can browse all upcoming in-person and virtual events.</a></p> \\n<p>That’s all for this week. Check back next Monday for another Weekly Roundup!</p> \\n<p>–<a href=\"https://www.linkedin.com/in/kprasadrao/\">Prasad</a></p> \\n<p><em>This post is part of our&nbsp;<a href=\"https://aws.amazon.com/blogs/aws/tag/week-in-review/\">Weekly Roundup&nbsp;series</a>. Check back each week for a quick roundup of interesting news and announcements from AWS!</em></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d9bc4ab0950931ef589e91784abcf2b095b29367', embedding=None, metadata={'title': 'Stable Diffusion 3.5 Large is now available in Amazon Bedrock', 'link': 'https://aws.amazon.com/blogs/aws/stable-diffusion-3-5-large-is-now-available-in-amazon-bedrock/', 'date': 'Thu, 19 Dec 2024 19:22:10 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>As we preannounced at AWS re:Invent 2024, you can now use <a href=\"https://aws.amazon.com/bedrock/stability-ai/\">Stable Diffusion 3.5 Large in Amazon Bedrock</a> to generate high-quality images from text descriptions in a wide range of styles to accelerate the creation of concept art, visual effects, and detailed product imagery for customers in media, gaming, advertising, and retail.</p> \\n<p>In October 2024, <a href=\"https://stability.ai/\">Stability AI</a> introduced <a href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\">Stable Diffusion 3.5 Large</a>, the most powerful model in the Stable Diffusion family at 8.1 billion parameters trained on <a href=\"https://aws.amazon.com/sagemaker-ai/hyperpod/\">Amazon SageMaker HyperPod</a>, with superior quality and prompt adherence. Stable Diffusion 3.5 Large can accelerate storyboarding, concept art creation, and rapid prototyping of visual effects. You can quickly generate high-quality 1-megapixel images for campaigns, social media posts, and advertisements, saving time and resources while maintaining creative control.</p> \\n<p>Stable Diffusion 3.5 Large offers users nearly endless creative possibilities, including:</p> \\n<ul> \\n <li><strong>Versatile Styles</strong> – You can generate images in a wide range of styles and aesthetics, including 3-dimentional, photography, painting, line art, and virtually any visual style you can imagine.</li> \\n <li><strong>Prompt Adherence</strong> – You can use Stable Diffusion 3.5 Large’s advanced prompt adherence to closely follow your text prompts, making it a top choice for efficient, high-quality performance.</li> \\n <li><strong>Diverse Outputs</strong> – You can create images representative of the diverse world around you, featuring people with different skin tones and features, without the need for extensive prompting.</li> \\n</ul> \\n<p>Today, Stable Image Ultra in Amazon Bedrock has been updated to include Stable Diffusion 3.5 Large in the model’s underlying architecture. Stable Image Ultra, powered by Stability AI’s most advanced models, including Stable Diffusion 3.5, sets a new standard in image generation. It excels in typography, intricate compositions, dynamic lighting, vibrant colors, and artistic cohesion.</p> \\n<p>With the latest update of Stable Diffusion models in <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>, you have a broader set of solutions to boost your creativity and accelerate image generation workflows.</p> \\n<p><u><strong>Get started with Stable Diffusion 3.5 Large in Amazon Bedrock</strong></u><br /> Before getting started, if you are new to using Stability AI models, go to the <a href=\"https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#modelaccess\">Amazon Bedrock console</a> and choose <strong>Model access</strong> on the bottom left pane. To access the latest Stability AI models, request access for <strong>Stable Diffusion 3.5 Large</strong> in Stability AI.</p> \\n<p><img alt=\"\" class=\"aligncenter size-full wp-image-93105\" height=\"1247\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/14/2024-sd35-bedrock-1-model-access.png\" style=\"width: 95%; border: solid 1px #ccc;\" width=\"1840\" /></p> \\n<p>To test the Stability AI models in Amazon Bedrock, choose <strong>Image </strong>under <strong>Playgrounds </strong>in the left menu pane. Then choose <strong>Select model</strong> and select <strong>Stability AI</strong> as the category and <strong>Stable Diffusion 3.5 Large </strong>as the model.</p> \\n<p><img alt=\"\" class=\"aligncenter wp-image-93118 size-full\" height=\"1127\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/16/2024-sd35-bedrock-2-select-model-1.png\" style=\"width: 95%; border: solid 1px #ccc;\" width=\"1752\" /></p> \\n<p>You can generate an image with your prompt. Here is a sample prompt to generate the image:</p> \\n<p><code>High-energy street scene in a neon-lit Tokyo alley at night, where steam rises from food carts, and colorful neon signs illuminate the rain-slicked pavement.</code></p> \\n<p><img alt=\"\" class=\"aligncenter size-full wp-image-93107\" height=\"1344\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/14/2024-sd35-bedrock-3-play-ground.jpg\" style=\"width: 95%; border: solid 1px #ccc;\" width=\"1856\" /></p> \\n<p>By choosing <strong>View API request</strong>, you can also access the model using code examples in the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a> and <a href=\"https://aws.amazon.com/developer/tools/\">AWS SDKs</a>. You can use <code>stability.sd3-5-large-v1:0</code> as the model ID.</p> \\n<p>To get the image with a single command, I write the output JSON file to standard output and use the <a href=\"https://jqlang.github.io/jq/\">jq</a> tool to extract the encoded image so that it can be decoded on the fly. The output is written in the img.png file.</p> \\n<p>Here is a sample of the AWS CLI command:</p> \\n<pre><code class=\"lang-bash\">$ aws bedrock-runtime invoke-model \\\\\\n   --model-id stability.sd3-5-large-v1:0 \\\\\\n   --body \"{\\\\\"prompt\\\\\":\\\\\"High-energy street scene in a neon-lit Tokyo alley at night, where steam rises from food carts, and colorful neon signs illuminate the rain-slicked pavement.\\\\\",\\\\\"mode\\\\\":\\\\\"text-to-image\\\\\",\\\\\"aspect_ratio\\\\\":\\\\\"1:1\\\\\",\\\\\"output_format\\\\\":\\\\\"jpeg\\\\\",\\\\\"seed\\\\\":0}\" \\\\\\n   --cli-binary-format raw-in-base64-out \\\\\\n   --region us-west-2 \\\\\\n/dev/stdout | jq -r \\'.images[0]\\' | base64 --decode &gt; img.jpg</code></pre> \\n<p>Here’s how you can use Stable Image Ultra 1.1 to include Stable Diffusion 3.5 Large in the model’s underlying architecture with the <a href=\"https://aws.amazon.com/sdk-for-python/\">AWS SDK for Python (Boto3)</a>. This simple application interactively asks for a text-to-image prompt and then calls Amazon Bedrock to generate the image with <code>stability.stable-image-ultra-v1:1</code> as the model ID.</p> \\n<pre><code class=\"lang-python\">import base64\\nimport boto3\\nimport json\\nimport os\\n\\nMODEL_ID = \"stability.stable-image-ultra-v1:1\"\\n\\nbedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\\n\\nprint(\"Enter a prompt for the text-to-image model:\")\\nprompt = input()\\n\\nbody = {\\n    \"prompt\": prompt,\\n    \"mode\": \"text-to-image\"\\n}\\nresponse = bedrock_runtime.invoke_model(modelId=MODEL_ID, body=json.dumps(body))\\n\\nmodel_response = json.loads(response[\"body\"].read())\\n\\nbase64_image_data = model_response[\"images\"][0]\\n\\ni, output_dir = 1, \"output\"\\nif not os.path.exists(output_dir):\\n    os.makedirs(output_dir)\\nwhile os.path.exists(os.path.join(output_dir, f\"img_{i}.png\")):\\n    i += 1\\n\\nimage_data = base64.b64decode(base64_image_data)\\n\\nimage_path = os.path.join(output_dir, f\"img_{i}.png\")\\nwith open(image_path, \"wb\") as file:\\n    file.write(image_data)\\n\\nprint(f\"The generated image has been saved to {image_path}\")</code></pre> \\n<p>The application writes the resulting image in an <code>output</code> directory that is created if not present. To not overwrite existing files, the code checks for existing files to find the first file name available with the <code>img_&lt;number&gt;.png</code> format.</p> \\n<p>To learn more, visit the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-runtime_example_bedrock-runtime_InvokeModel_StableDiffusion_section.html\">Invoke API examples</a> using AWS SDKs to build your applications to generate an image using various programming languages.</p> \\n<p><strong><u>Interesting examples</u></strong><br /> Here are a few images created with Stable Diffusion 3.5 Large.</p> \\n<table> \\n <tbody> \\n  <tr> \\n   <td width=\"347\"><img alt=\"\" class=\"aligncenter wp-image-93173 size-full\" height=\"1024\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/19/Full-body-university-students-working-on-a-tech-project-with-the-words-_Stable-Diffusion-3.5-in-Amazon-Bedrock_-cheerful-cursive-typography-font-in-the-foreground.jpeg\" width=\"1024\" /></td> \\n   <td width=\"347\"><img alt=\"\" class=\"aligncenter size-full wp-image-93147\" height=\"1024\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/17/photo-of-three-potions.jpeg\" width=\"1024\" /></td> \\n  </tr> \\n  <tr> \\n   <td width=\"347\"><code><strong>Prompt:</strong> Full-body university students working on a tech project with the words Stable Diffusion 3.5 in Amazon Bedrock, cheerful cursive typography font in the foreground.<br /> </code></td> \\n   <td width=\"347\"><code><strong>Prompt:</strong> Photo of three potions: the first potion is blue with the label \"MANA\", the second potion is red with the label \"HEALTH\", the third potion is green with the label \"POISON\". Old apothecary.</code></td> \\n  </tr> \\n  <tr> \\n   <td width=\"347\"><img alt=\"\" class=\"aligncenter size-full wp-image-93148\" height=\"1024\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/17/Photography-pink-rose-flowers.jpg\" width=\"1024\" /></td> \\n   <td width=\"347\"><strong><img alt=\"\" class=\"aligncenter size-full wp-image-93149\" height=\"1024\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/17/3D-animation-scene.jpeg\" width=\"1024\" /></strong></td> \\n  </tr> \\n  <tr> \\n   <td width=\"347\"><code></code><code><strong>Prompt:</strong> Photography, pink rose flowers in the twilight, glowing, tile houses in the background.</code></td> \\n   <td width=\"347\"><code><strong>Prompt:</strong> 3D animation scene of an adventurer traveling the world with his pet dog.</code></td> \\n  </tr> \\n </tbody> \\n</table> \\n<p><u><strong>Now available</strong><br /> </u>Stable Diffusion 3.5 Large model is generally available today in Amazon Bedrock in the US West (Oregon) <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\">AWS Region</a>. Check the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html\">full Region list</a> for future updates. To learn more, check out the <a href=\"https://aws.amazon.com/bedrock/stability-ai/\">Stability AI in Amazon Bedrock</a> product page and the <a href=\"https://aws.amazon.com/bedrock/pricing/\">Amazon Bedrock Pricing</a> page.</p> \\n<p>Give Stable Diffusion 3.5 Large a try in the <a href=\"https://console.aws.amazon.com/bedrock\">Amazon Bedrock console</a> today and send feedback to <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag/amazon-bedrock\">AWS re:Post for Amazon Bedrock</a> or through your usual AWS Support contacts.</p> \\n<p>— <a href=\"https://twitter.com/channyun\">Channy</a></p> \\n<p><i><strong>Updated on Dec 19, 2024</strong> — Fixed AWS CLI command to invoke the model.</i></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='14aea61aa94665071e4b352fb1ccf17580bf8876', embedding=None, metadata={'title': 'New Amazon EC2 High Memory U7inh instance on HPE Server for large in-memory databases', 'link': 'https://aws.amazon.com/blogs/aws/new-amazon-ec2-high-memory-u7inh-instance-on-hpe-server-for-large-in-memory-databases/', 'date': 'Mon, 16 Dec 2024 20:30:18 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>Today we’re announcing the general availability of <a href=\"https://aws.amazon.com/ec2\">Amazon Elastic Compute Cloud (Amazon EC2)</a> <a href=\"https://aws.amazon.com/ec2/instance-types/u7i/\">U7inh instance</a>, a new addition to EC2 High Memory family, built in collaboration with Hewlett Packard Enterprise (HPE). Amazon EC2 U7inh instance runs on the 16-socket HPE Compute Scale-up Server 3200, and are built on the <a href=\"https://aws.amazon.com/ec2/nitro/\">AWS Nitro System</a> to deliver a fully integrated and managed experience consistent with other EC2 instances.</p> \\n<p>Powered by the fourth generation Intel<sup>®</sup> Xeon<sup>®</sup> Scalable processors (Sapphire Rapids), U7inh instance supports 32 TB of memory and 1920 vCPUs. This instance offers the highest compute performance, largest compute and memory size in the <a href=\"https://aws.amazon.com/\">Amazon Web Services (AWS)</a> Cloud for running large, mission-critical database workloads, like SAP HANA.</p> \\n<p>In May 2024, we launched <a href=\"https://aws.amazon.com/blogs/aws/amazon-ec2-high-memory-u7i-instances-for-large-in-memory-databases/\">U7i instances</a> to support up to 896 vCPUs and up to 32 TB of memory, which our enterprise customers could use to successfully migrate their large mission-critical in-memory databases to AWS and benefit from the flexibility, scalability, reliability, and cost advantages that AWS offers.</p> \\n<p>As customers continue to scale their business applications, they wanted the performance combined with the additional CPUs and memory along with SAP certification to generate real-time business insights. Other customers that currently run on-premises with HPE servers have also asked how we can help them migrate to AWS to take advantage of cloud benefits while continuing to use HPE hardware.</p> \\n<p>Here are the detailed specs of new U7inh instance:</p> \\n<table style=\"border: 2px solid black; border-collapse: collapse; margin-left: auto; margin-right: auto; height: 67px;\"> \\n <tbody> \\n  <tr style=\"border-bottom: 1px solid black; background-color: #e0e0e0;\"> \\n   <td style=\"border-right: 1px solid black; padding: 4px; text-align: center;\"><strong>Instance name</strong></td> \\n   <td style=\"border-right: 1px solid black; padding: 4px; text-align: center;\"><strong>vCPUs</strong></td> \\n   <td style=\"border-right: 1px solid black; padding: 4px; text-align: center;\"><strong>Memory (DDR5)</strong></td> \\n   <td style=\"border-right: 1px solid black; padding: 4px; text-align: center;\"><strong>EBS bandwidth</strong></td> \\n   <td style=\"border-right: 1px solid black; padding: 4px; text-align: center;\"><strong>Network bandwidth</strong></td> \\n  </tr> \\n  <tr style=\"border-bottom: 1px solid black;\"> \\n   <td style=\"border-right: 1px solid black; padding: 4px; text-align: center;\"><strong>U7inh-32tb.480xlarge</strong></td> \\n   <td style=\"border-right: 1px solid black; padding: 4px; text-align: center;\">1920</td> \\n   <td style=\"border-right: 1px solid black; padding: 4px; text-align: center;\">32,768 GiB</td> \\n   <td style=\"border-right: 1px solid black; padding: 4px; text-align: center;\">160 Gbps</td> \\n   <td style=\"border-right: 1px solid black; padding: 4px; text-align: center;\">200 Gbps</td> \\n  </tr> \\n </tbody> \\n</table> \\n<p>U7inh instance offers up to two times vCPUs and 1.6 times EBS bandwidth in a single instance, compared with the largest U7i instance. You can run your largest in-memory database workloads like SAP HANA or seamlessly migrate workloads running on HPE hardware to AWS.</p> \\n<p>U7inh instance supports Amazon Linux, Red Hat Enterprise Linux, and SUSE Enterprise Linux Server. Operating system support for SAP HANA workloads on High Memory instances include: SUSE Linux Enterprise Server 15 SP3 for SAP and above and Red Hat Enterprise Linux 8.6/9.0 for SAP and above.</p> \\n<p>U7inh instance is SAP certified to run Business Suite on HANA (SoH), Business Suite S/4HANA, Business Warehouse on HANA (BW), and SAP BW/4HANA in production environments. U7inh instance is also certified for scale-out SAP HANA OLTP workloads such as S/4HANA and customers can deploy up to four U7inh instance (128TB) in a cluster for even larger SAP HANA workloads.</p> \\n<p>To learn more about how to migrate, visit&nbsp;<a href=\"https://docs.aws.amazon.com/sap/latest/sap-hana/migrating-hana-to-hm.html\">Migrating SAP HANA on AWS to an EC2 High Memory Instance</a> in the SAP HANA on AWS Guides and <a href=\"https://docs.aws.amazon.com/launchwizard/latest/userguide/launch-wizard-sap.html\">AWS Launch Wizard for SAP</a> in the AWS Launch Wizard User Guide.</p> \\n<p><u><strong>Now available</strong></u><br /> Amazon EC2 U7inh instance is available in the US East (N. Virginia) and US West (Oregon) <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\">AWS Regions</a>.</p> \\n<p>To learn more, visit the <a href=\"https://aws.amazon.com/ec2/instance-types/u7i/\">U7i instance product page</a> and send feedback to <a href=\"https://repost.aws/tags/TAO-wqN9fYRoyrpdULLa5y7g/amazon-ec-2\">AWS re:Post for EC2</a> or through your usual AWS Support contacts.</p> \\n<p>— <a href=\"https://twitter.com/channyun\">Channy</a></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7b5d66570efffb6adcce029ab8c444bf7cd32f79', embedding=None, metadata={'title': 'And that’s a wrap!', 'link': 'https://aws.amazon.com/blogs/aws/and-thats-a-wrap/', 'date': 'Mon, 16 Dec 2024 17:16:46 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>After 20 years, and 3283 posts adding up to 1,577,106 words I am wrapping up my time as the lead blogger on the AWS News Blog.</p> \\n<p>It has been a privilege to be able to “live in the future” and to get to learn and write about so many of our innovations over the last two decades: <a href=\"https://aws.amazon.com/blogs/aws/amazon_simple_q/\">message queuing</a>, <a href=\"https://aws.amazon.com/blogs/aws/amazon_s3/\">storage</a>, <a href=\"https://aws.amazon.com/blogs/aws/amazon_ec2_beta/\">on-demand computing</a>, <a href=\"https://aws.amazon.com/blogs/aws/run-code-cloud/\">serverless</a>, and <a href=\"https://aws.amazon.com/blogs/aws/amazon-braket-get-started-with-quantum-computing/\">quantum computing</a> to name just a few and to leave many others out. It has also been a privilege to be able to meet and to hear from so many of you that have faithfully read and (hopefully) learned from my content over the years. I treasure those interactions and your kind words, and I keep both in mind when I write.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Next for Jeff<br /> </strong></span>I began my career as a builder. Over the years I have written tens of thousands of lines of assembly code (6502, Z80, and 68000), Visual Basic, and PHP, along with hundreds of thousands of lines of C. However, over the years I’ve progressively spent less time building and more time talking about building. As each new service and feature whizzed past my eyes I would reminiscence about days and decades past, when I could actually use these goodies to create something cool. I went from being a developer who could market, to a marketer who used to be able to develop. There’s absolutely nothing wrong with that, but I like to build. The medium could be code, 3D printing, LEGO bricks, electronics components, or even cardboard –creating and innovating is what motivates and sustains me.</p> \\n<p>With that as my driving force, my goal for the next step of my career at AWS is to invest more time focused on learning and using fewer things, building cool stuff, and creating fresh, developer-focused content as I do so. I’m still working to figure out the form that this will take, so stay tuned. I am also going to continue to make my weekly appearances at <a href=\"https://aws.amazon.com/developer/community/live-video/aws-on-air/\">AWS OnAir</a> (our Friday Twitch show), and I will continue to speak at AWS community events around the globe.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Next for the Blog</strong></span><br /> As for the AWS News Blog, it has long been backed by an awesome team, both visible and invisible. Here we are at the recent <a href=\"https://reinvent.awsevents.com/\">AWS re:Invent</a> celebration of the blog’s 20th anniversary (photo courtesy of <a href=\"https://www.linkedin.com/in/lizfue/\">Liz Fuentes</a> with edits by <a href=\"https://www.linkedin.com/in/channy/?originalSubdomain=kr\">Channy Yun</a> to add those who were otherwise occupied):</p> \\n<p><img alt=\"\" class=\"aligncenter size-full wp-image-93063\" height=\"675\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/12/blog_ri_20_all-1.jpg\" width=\"900\" /></p> \\n<p>During the celebration I told the team that I look forward to celebrating the 30 year anniversary with them at re:Invent 2034.</p> \\n<p>Going forward, the team will continue to grow and the goal remains the same: to provide our customers with carefully chosen, high-quality information about the latest and most meaningful AWS launches. The blog is in great hands and this team will continue to keep you informed even as the AWS pace of innovation continues to accelerate.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Thanks Again</strong></span><br /> Once again I need to thank all of you for the very kind words and gestures over the years. Once in your life, if you work hard and get really lucky, you get a unique opportunity to do something that really and truly matters to people. And I have been lucky.</p> \\n<p></p>\\n<p>— <a href=\"https://twitter.com/jeffbarr\">Jeff</a>;</p>\\n<p></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='7a4166e9acce557cab34dfe6b39d974c819d3d39', embedding=None, metadata={'title': 'AWS Weekly Roundup: Amazon EC2 F2 instances, Amazon Bedrock Guardrails price reduction, Amazon SES update, and more (December 16, 2024)', 'link': 'https://aws.amazon.com/blogs/aws/aws-weekly-roundup-amazon-ec2-f2-instances-amazon-bedrock-guardrails-price-reduction-amazon-ses-update-and-more-december-16-2024/', 'date': 'Mon, 16 Dec 2024 14:47:40 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>The week after <a href=\"https://reinvent.awsevents.com/\">AWS re:Invent</a> builds on the excitement and energy of the event and is a good time to learn more and understand how the recent announcements can help you solve your challenges. As usual, we have you covered with our <a href=\"https://aws.amazon.com/blogs/aws/top-announcements-of-aws-reinvent-2024/\">top announcements of AWS re:Invent 2024 post</a>.</p> \\n<p>You can now watch keynotes and sessions on the <a href=\"https://www.youtube.com/@AWSEventsChannel\">AWS Event YouTube channel</a>. This year <a href=\"https://www.linkedin.com/in/andy-jassy-8b1615/\">Andy Jassy,</a> now President and CEO at Amazon, <a href=\"https://www.youtube.com/playlist?list=PLhr1KZpdzukcqPKhvTp5Dm2wMdmubETnC\">returned to re:Invent and shared some thoughts in these videos</a>.</p> \\n<p></p> \\n<p>Drawing on experiences Amazon has had building distributed systems at massive scale, <a href=\"https://www.allthingsdistributed.com/\">Werner Vogels</a>, VP and CTO at Amazon, shared critical lessons and strategies he has learned for managing complex systems in <a href=\"https://youtu.be/aim5x73crbM?si=CYiQMUXNx92nC1hq\">his keynote</a>.</p> \\n<p></p> \\n<p><span style=\"text-decoration: underline;\"><strong>Last week’s launches</strong></span><br /> Here are the launches that got my attention.</p> \\n<p><a href=\"https://aws.amazon.com/ec2/\"><strong>Amazon Elastic Compute Cloud (Amazon EC2)</strong></a> – A <a href=\"https://aws.amazon.com/blogs/aws/now-available-second-generation-fpga-powered-amazon-ec2-instances-f2/\">new generation of FPGA-powered instances (F2) is now available</a>. In contrast to a purpose-built chip designed with a single function in mind and then hard-wired to implement it, a field programmable gate array (FPGA) can be programmed in the field, after it has been plugged in to a socket on a PC board. We’re also <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-ec2-u7i-instances-6tib-8tib-memory/\">introducing Amazon EC2 High Memory U7i instances</a> with 6TiB and 8TiB of memory. U7i instances are ideal to run large in-memory databases such as SAP HANA, Oracle, and SQL Server. Graviton-based 8th generation instances now <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-ec2-instances-bandwidth-configurations-vpc-ebs/\">support bandwidth configurations for Amazon VPC and Amazon EBS</a>.</p> \\n<p><a href=\"https://aws.amazon.com/bedrock/guardrails/\"><strong>Amazon Bedrock Guardrails</strong></a> – We are <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-bedrock-guardrails-reduces-pricing-85-percent/\">reducing pricing by up to 85%</a> to help you implement safeguards for your <a href=\"https://aws.amazon.com/ai/generative-ai/\">generative AI</a> applications. Also, we’re adding <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-bedrock-guardrails-languages-spanish-french/\">multilingual capabilities</a> with support for Spanish and French languages.</p> \\n<p><a href=\"https://aws.amazon.com/ses/\"><strong>Amazon Simple Email Services (SES)</strong></a>&nbsp;– Now offers <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-ses-global-endpoints-multiregion-sending-resilience/\">Global Endpoints for multi-region sending resilience</a> and announces the <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-simple-email-services-deterministic-easy-dkim/\">availability of Deterministic Easy DKIM (DEED)</a>, a new form of global identity which simplifies the use of DomainKeys Identified Mail (DKIM) management.</p> \\n<p><a href=\"https://aws.amazon.com/cloudformation/\"><strong>AWS CloudFormation</strong></a> – An enhanced version of the <a href=\"https://aws.amazon.com/secrets-manager/\">AWS Secrets Manager</a> <a href=\"https://aws.amazon.com/blogs/security/introducing-an-enhanced-version-of-the-aws-secrets-manager-transform-awssecretsmanager-2024-09-16/\">transform introducing automatic</a> <a href=\"https://aws.amazon.com/lambda/\">AWS Lambda</a> upgrades.</p> \\n<p><a href=\"https://aws.amazon.com/lex/\"><strong>Amazon Lex</strong></a> – Launches new <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-lex-multilingual-speech-recognition-models/\">multilingual streaming speech recognition models</a> that enhance recognition accuracy through two specialized groupings: a European-based model (for Portuguese, Catalan, French, Italian, German, and Spanish) and a Asia Pacific-based model (for Chinese, Korean, and Japanese).</p> \\n<p><a href=\"https://aws.amazon.com/connect/\"><strong>Amazon Connect</strong></a> – Now <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-connect-push-notifications-mobile-chat/\">supports push notifications for mobile chat</a> on iOS and Android devices. In this way, you can be proactively notified as soon as there is a new message from an agent or chatbot, even when not actively chatting. You can now also <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-connect-holiday-overrides-hours-operation/\">configure holidays and other variances to your contact center hours of operation</a>.</p> \\n<p><a href=\"https://aws.amazon.com/security-hub/\"><strong>AWS Security Hub</strong></a> – Now <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/aws-security-hub-pci-dss-v4.0-1-standard/\">supports automated security checks aligned to the Payment Card Industry Data Security Standard (PCI DSS) v4.0.1</a>, a compliance framework that provides a set of rules and guidelines for safely handling credit and debit card information.</p> \\n<p><a href=\"https://aws.amazon.com/resourceexplorer/\"><strong>AWS Resource Explorer</strong></a> – <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/aws-resource-explorer-new-resource-types/\">Supports 59 new resource types</a> including <a href=\"https://aws.amazon.com/eks/\">Amazon Elastic Kubernetes Service (Amazon EKS)</a>, <a href=\"https://aws.amazon.com/kendra/\">Amazon Kendra</a>, <a href=\"https://aws.amazon.com/iam/access-analyzer/\">AWS Identity and Access Management (IAM) Access Analyzer</a>, and <a href=\"https://aws.amazon.com/sagemaker/\">Amazon SageMaker</a>.</p> \\n<p><strong>Amazon SageMaker AI</strong> – Inference optimized Amazon EC2 G6e instances (powered by <a href=\"https://aws.amazon.com/nvidia/\">NVIDIA</a> L40S Tensor Core GPUs) and P5e (powered by NVIDIA H200 Tensor Core GPUs) are <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-sagemaker-ai-p5e-g6e-instances-inference/\">now available on Amazon SageMaker</a>.</p> \\n<p><a href=\"https://aws.amazon.com/redshift/\"><strong>Amazon Redshift</strong></a> – Now <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-redshift-refresh-materialized-views-zero-etl-integrations/\">supports automatically and incrementally refreshable materialized views on tables in a zero-ETL integration</a>. Previously, in this case, you had to run a full refresh.</p> \\n<p><a href=\"https://aws.amazon.com/visualstudiocode/\"><strong>AWS Toolkit for Visual Studio Code</strong></a> – Now <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/aws-toolkit-visual-studio-code-cloudwatch-logs-live-tail/\">includes Amazon CloudWatch Logs Live Tail</a>, an interactive log streaming and analytics capability that provides real-time visibility into your logs and makes it easier to develop and troubleshoot applications.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Other AWS news</strong></span><br /> Here are some additional projects, blog posts, and news items that you might find interesting:</p> \\n<p><strong><a href=\"https://aws.amazon.com/blogs/storage/build-a-managed-transactional-data-lake-with-amazon-s3-tables/\">Build a managed transactional data lake with Amazon S3 Tables</a></strong> – Just introduced at re:Invent 2024, <a href=\"https://aws.amazon.com/s3/features/tables/\">Amazon S3 Tables</a> is the first cloud object store with built-in <a href=\"https://iceberg.apache.org/\">Apache Iceberg</a> support and the easiest way to store tabular data at scale. This <a href=\"https://aws.amazon.com/blogs/storage/build-a-managed-transactional-data-lake-with-amazon-s3-tables/\">post on the AWS Storage Blog</a> provides an overview of S3 Tables and an example of how to build a transactional data lake with S3 Tables using <a href=\"https://spark.apache.org/\">Apache Spark</a> on <a href=\"https://aws.amazon.com/emr/\">Amazon EMR</a>.</p> \\n<p><strong><a href=\"https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-cross-region-connectivity-for-aws-privatelink/\">Introducing Cross-Region Connectivity for AWS PrivateLink</a></strong> – More information on this recent launch that can be used to share and access <a href=\"https://aws.amazon.com/vpc/\">Amazon Virtual Private Cloud (Amazon VPC)</a> endpoint services across different <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Regions</a>.</p> \\n<p><a href=\"https://www.linkedin.com/in/marc-brooker-b431772b/\">Marc Brooker</a>, VP/Distinguished Engineer at AWS, shared on <a href=\"https://brooker.co.za/blog/\">his personal blog</a> a few posts about what <a href=\"https://aws.amazon.com/rds/aurora/dsql/\">Amazon Aurora DSQL</a> is, how it works, and how to make the best use of it:</p> \\n<ul> \\n <li><a href=\"https://brooker.co.za/blog/2024/12/03/aurora-dsql.html\">DSQL Vignette: Aurora DSQL, and A Personal Story</a></li> \\n <li><a href=\"https://brooker.co.za/blog/2024/12/04/inside-dsql.html\">DSQL Vignette: Reads and Compute</a></li> \\n <li><a href=\"https://brooker.co.za/blog/2024/12/05/inside-dsql-writes.html\">DSQL Vignette: Transactions and Durability</a></li> \\n <li><a href=\"https://brooker.co.za/blog/2024/12/06/inside-dsql-cap.html\">DSQL Vignette: Wait! Isn’t That Impossible?</a></li> \\n</ul> \\n<p>That’s all for this week. Check back next Monday for another Weekly Roundup!</p> \\n<p>— <a href=\"https://twitter.com/danilop\">Danilo</a></p> \\n<p><em>This post is part of our&nbsp;<a href=\"https://aws.amazon.com/blogs/aws/tag/week-in-review/\">Weekly Roundup</a>&nbsp;series. Check back each week for a quick roundup of interesting news and announcements from AWS!</em></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='823f176c71ff36d802a6912c8db66ae3c79403cf', embedding=None, metadata={'title': 'Now Available – Second-Generation FPGA-Powered Amazon EC2 instances (F2)', 'link': 'https://aws.amazon.com/blogs/aws/now-available-second-generation-fpga-powered-amazon-ec2-instances-f2/', 'date': 'Wed, 11 Dec 2024 23:09:48 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>Equipped with up to eight AMD Field-Programmable Gate Arrays (FPGAs), AMD EPYC (Milan) processors with up to 192 cores, High Bandwidth Memory (HBM), up to 8 TiB of SSD-based instance storage, and up to 2 TiB of memory, the new F2 instances are available in two sizes, and are ready to accelerate your genomics, multimedia processing, big data, satellite communication, networking, silicon simulation, and live video workloads.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>A Quick FPGA Recap</strong></span><br /> Here’s how I explained the FPGA model when we <a href=\"https://aws.amazon.com/blogs/aws/developer-preview-ec2-instances-f1-with-programmable-hardware/\">previewed</a> the first generation of FPGA-powered <a href=\"https://aws.amazon.com/ec2/\">Amazon Elastic Compute Cloud (Amazon EC2)</a> instances</p> \\n<blockquote>\\n <p>One of the more interesting routes to a custom, hardware-based solution is known as a Field Programmable Gate Array, or FPGA. In contrast to a purpose-built chip which is designed with a single function in mind and then hard-wired to implement it, an FPGA is more flexible. It can be programmed in the field, after it has been plugged in to a socket on a PC board. Each FPGA includes a fixed, finite number of simple logic gates. Programming an FPGA is “simply” a matter of connecting them up to create the desired logical functions (AND, OR, XOR, and so forth) or storage elements (flip-flops and shift registers). Unlike a CPU which is essentially serial (with a few parallel elements) and has fixed-size instructions and data paths (typically 32 or 64 bit), the FPGA can be programmed to perform many operations in parallel, and the operations themselves can be of almost any width, large or small.</p>\\n</blockquote> \\n<p>Since that launch, AWS customers have used F1 instances to host many different types of applications and services. With a newer FPGA, more processing power, and more memory bandwidth, the new F2 instances are an even better host for highly parallelizable, compute-intensive workloads.</p> \\n<p>Each of the&nbsp;<a href=\"https://www.amd.com/en/products/adaptive-socs-and-fpgas/fpga/virtex-ultrascale-plus-hbm.html\">AMD Virtex UltraScale+</a> HBM VU47P FPGAs has 2.85 million system logic cells and 9,024 DSP slices (up to 28 TOPS of DSP compute performance when processing INT8 values). The FPGA Accelerator Card associated with each F2 instance provides 16 GiB of High Bandwidth Memory and 64 GiB of DDR4 memory per FPGA.</p> \\n<p><strong><span style=\"text-decoration: underline;\">Inside the F2</span><br /> </strong>F2 instances are powered by 3rd generation <a href=\"https://www.amd.com/en/products/processors/server/epyc/7003-series.html\">AMD EPYC</a> (Milan) processors. In comparison to F1 instances, they offer up to 3x as many processor cores, up to twice as much system memory and NVMe storage, and up to 4x the network bandwidth. Each FPGA comes with 16 GiB High Bandwidth Memory (HBM) with up to 460 GiB/s bandwidth. Here are the instance sizes and specs:</p> \\n<table cellpadding=\"8\" style=\"margin-left: auto; margin-right: auto; border: 1px solid black; border-collapse: collapse;\"> \\n <tbody> \\n  <tr style=\"background-color: #e0e0e0; vertical-align: bottom;\"> \\n   <td style=\"border-bottom: 1px solid black; text-align: center;\"><strong>Instance Name</strong></td> \\n   <td style=\"border-bottom: 1px solid black; text-align: center;\"><strong>vCPUs<br /> </strong></td> \\n   <td align=\"center\" style=\"border-bottom: 1px solid black; text-align: center;\"><strong>FPGAs<br /> </strong></td> \\n   <td align=\"center\" style=\"border-bottom: 1px solid black; text-align: center;\"><strong>FPGA Memory<br /> HBM / DDR4<br /> </strong></td> \\n   <td align=\"center\" style=\"border-bottom: 1px solid black; text-align: center;\"><strong>Instance Memory<br /> </strong></td> \\n   <td align=\"center\" style=\"border-bottom: 1px solid black; text-align: center;\"><strong>NVMe Storage<br /> </strong></td> \\n   <td align=\"center\" style=\"border-bottom: 1px solid black; text-align: center;\"><strong>EBS Bandwidth<br /> </strong></td> \\n   <td align=\"center\" style=\"border-bottom: 1px solid black; text-align: center;\"><strong>Network Bandwidth<br /> </strong></td> \\n  </tr> \\n  <tr style=\"border-bottom: 1px solid #ddd;\"> \\n   <td align=\"left\"><strong>f2.12xlarge</strong></td> \\n   <td align=\"center\">48</td> \\n   <td align=\"center\">2</td> \\n   <td align=\"center\">32 GiB /<br /> 128 GiB</td> \\n   <td align=\"center\">512 GiB</td> \\n   <td align=\"center\">1900 GiB<br /> (2x 950 GiB)</td> \\n   <td align=\"center\">15 Gbps</td> \\n   <td align=\"center\">25 Gbps</td> \\n  </tr> \\n  <tr> \\n   <td align=\"left\"><strong>f2.48xlarge</strong></td> \\n   <td align=\"center\">192</td> \\n   <td align=\"center\">8</td> \\n   <td align=\"center\">128 GiB /<br /> 512 GiB</td> \\n   <td align=\"center\">2,048 GiB</td> \\n   <td align=\"center\">7600 GiB<br /> (8x 950 GiB)</td> \\n   <td align=\"center\">60 Gbps</td> \\n   <td align=\"center\">100 Gbps</td> \\n  </tr> \\n </tbody> \\n</table> \\n<p>The high-end <strong>f2.48xlarge</strong> instance supports the <a href=\"https://aws.amazon.com/media-services/resources/cdi/\">AWS Cloud Digital Interface</a> (CDI) to reliably transport uncompressed live video between applications, with instance-to-instance latency as low as 8 milliseconds.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Building FPGA Applications</strong></span><br /> The <a href=\"https://github.com/aws/aws-fpga\">AWS EC2 FPGA Development Kit</a> contains the tools that you will use to develop, simulate, debug, compile, and run your hardware-accelerated FPGA applications. You can launch the kit’s <a href=\"https://aws.amazon.com/marketplace/pp/prodview-f5kjsenkfkz5u\">FPGA Developer AMI</a> on a memory-optimized or compute-optimized instance for development and simulation, then use an F2 instance for final debugging and testing.</p> \\n<p>The tools included in the developer kit support a variety of development paradigms, tools, accelerator languages, and debugging options. Regardless of your choice, you will ultimately create an Amazon FPGA Image (AFI) which contains your custom acceleration logic and the <a href=\"https://github.com/aws/aws-fpga/blob/f2/hdk/docs/AWS_Shell_Interface_Specification.md\">AWS Shell</a> which implements access to the FPGA memory, PCIe bus, interrupts, and external peripherals. You can deploy AFIs to as many F2 instances as desired, share with other AWS accounts or publish on AWS Marketplace.</p> \\n<p>If you have already created an application that runs on F1 instances, you will need to update your development environment to use the latest AMD tools, then rebuild and validate before upgrading to F2 instances.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>FPGA Instances in Action</strong></span><br /> Here are some cool examples of how F1 and F2 instances can support unique and highly demanding workloads:</p> \\n<p><strong>Genomics</strong> – Multinational pharmaceutical and biotechnology company AstraZeneca used thousands of F1 instances to build the world’s fastest genomics pipeline, able to process over 400K whole genome samples in under two months. They will adopt <a href=\"https://www.illumina.com/products/by-type/informatics-products/dragen-secondary-analysis.html\">Illumina DRAGEN</a> for F2 to realize better performance at a lower cost, while accelerating disease discovery, diagnosis, and treatment.</p> \\n<p><strong>Satellite Communication</strong> – Satellite operators are moving from inflexible and expensive physical infrastructure (modulators, demodulators, combiners, splitters, and so forth) toward agile, software-defined, FPGA-powered solutions. Using the digital signal processor (DSP) elements on the FPGA, these solutions can be reconfigured in the field to support new waveforms and to meet changing requirements. Key F2 features such as support for up to 8 FPGAs per instance, generous amounts of network bandwidth, and support for the <a href=\"https://www.dpdk.org/\">Data Plan Development Kit</a> (DPDK) using <a href=\"https://github.com/aws/aws-fpga/tree/f2/sdk/apps/virtual-ethernet\">Virtual Ethernet</a> can be used to support processing of multiple, complex waveforms in parallel.</p> \\n<p><strong>Analytics</strong> – <a href=\"https://www.neuroblade.com/\">NeuroBlade</a>‘s SQL Processing Unit (SPU) integrates with Presto, Apache Spark, and other open source query engines, delivering faster query processing and market-leading query throughput efficiency when run on F2 instances.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Things to Know</strong></span><br /> Here are a couple of final things that you should know about the F2 instances:</p> \\n<p><strong>Regions</strong> – F2 instances are available today in the US East (N. Virginia) and Europe (London) AWS Regions, with plans to extend availability to additional regions over time.</p> \\n<p><strong>Operating Systems</strong> – F2 instances are Linux-only.</p> \\n<p><strong>Purchasing Options</strong> – F2 instances are available in <a href=\"https://aws.amazon.com/ec2/pricing/on-demand/\">On-Demand</a>, <a href=\"https://aws.amazon.com/ec2/spot/\">Spot</a>,&nbsp;<a href=\"https://aws.amazon.com/savingsplans/compute-pricing/\">Savings Plan</a>, <a href=\"https://aws.amazon.com/ec2/pricing/dedicated-instances/\">Dedicated Instance</a>, and <a href=\"https://aws.amazon.com/ec2/dedicated-hosts/\">Dedicated Host</a> form.</p> \\n<p></p>\\n<p>— <a href=\"https://twitter.com/jeffbarr\">Jeff</a>;</p>\\n<p></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='a4c71b19d5eb758bbfd45a28141c249bef867d25', embedding=None, metadata={'title': 'Introducing Buy with AWS: an accelerated procurement experience on AWS Partner sites, powered by AWS Marketplace', 'link': 'https://aws.amazon.com/blogs/aws/introducing-buy-with-aws-an-accelerated-procurement-experience-on-aws-partner-sites-powered-by-aws-marketplace/', 'date': 'Wed, 04 Dec 2024 23:30:08 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>Today, we are announcing <a href=\"https://aws.amazon.com/marketplace/features/buy-with-aws\">Buy with AWS</a>, a new way to discover and purchase solutions available in <a href=\"https://aws.amazon.com/mp/marketplace-service/overview/\">AWS Marketplace</a> from <a href=\"https://aws.amazon.com/partners/\">AWS Partner</a> sites. You can use Buy with AWS to accelerate and streamline your product procurement process on websites outside of <a href=\"https://aws.amazon.com/\">Amazon Web Services (AWS)</a>. This feature provides you the ability to find, try, and buy solutions from Partner websites using your AWS account</p> \\n<p>AWS Marketplace is a curated digital store for you to find, buy, deploy, and manage cloud solutions from Partners. Buy with AWS is another step towards AWS Marketplace making it easy for you to find and procure the right Partner solutions, when and where you need them. You can conveniently find and procure solutions in AWS Marketplace, through integrated AWS service consoles, and now on Partner websites.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Accelerate cloud solution discovery and evaluation</strong></span></p> \\n<p>You can now discover solutions from Partners available for purchase through AWS Marketplace as you explore solutions on the web beyond AWS.</p> \\n<p>Look for products that are “Available in AWS Marketplace” when browsing on Partner sites, then accelerate your evaluation process with fast access to free trials, demo requests, and inquiries for custom pricing.</p> \\n<p>For example, I want to evaluate <a href=\"https://www.wiz.io/\">Wiz</a> to see how it can help with my cloud security requirements. While browsing the Wiz website, I come across a <a href=\"https://www.wiz.io/partners/aws\">page where I see “Connect Wiz with Amazon Web Services (AWS)”</a>.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/Wiz-1-2.png\"><img alt=\"Wiz webpage featuring Buy With AWS\" class=\"aligncenter wp-image-91568 size-full\" height=\"785\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/Wiz-1-2.png\" width=\"1307\" /></a></p> \\n<p>I choose <strong>Try with AWS</strong>. It asks me to sign in to my AWS account if I’m not signed in already. I’m then presented with a Wiz and AWS co-branded page for me to sign up for the free trial.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/Wiz-2.png\"><img alt=\"Wiz and AWS co-branded page to sign up for free trial using Buy with AWS through AWS Marketplace\" class=\"aligncenter wp-image-91556 size-full\" height=\"458\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/Wiz-2.png\" width=\"904\" /></a></p> \\n<p>The discovery experience that you see will vary depending on type of the Partner website you’re shopping from. Wiz is an example of how Buy with AWS can be implemented by an independent software vendor (ISV). Now, let’s look at an example of an AWS Marketplace Channel Partner, or reseller, who operates a storefront of their own.</p> \\n<p>I browse to the <a href=\"https://marketplace-aws.bytes.co.uk/products\">Bytes storefront</a> with product listings from AWS Marketplace. I have the option to filter and search from the curated product listings, which are available in AWS Marketplace, on the Bytes site.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/Bytes-1.png\"><img alt=\"Bytes storefront with product listings from AWS Marketplace\" class=\"aligncenter wp-image-91558 size-full\" height=\"634\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/Bytes-1.png\" width=\"904\" /></a></p> \\n<p>I choose <strong>View Details</strong> for Fortinet and see an option to <strong>Request Private Offer</strong> from AWS.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/Bytes-2.png\"><img alt=\"Bytes storefront with option to Request Private Offer for Fortinet from AWS Marketplace\" class=\"aligncenter wp-image-91559 size-full\" height=\"404\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/Bytes-2.png\" width=\"904\" /></a></p> \\n<p>As you can tell, on a Channel Partner site, you can browse curated product listings available in AWS Marketplace, filter products, and request custom pricing directly from their website.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Streamline product procurement on AWS Partner sites</strong></span><br /> I had a seamless experience using Buy with AWS to access a free trial for Wiz and browse through the Bytes storefront to request a private offer.</p> \\n<p>Now I want to try <a href=\"https://www.databricks.com/\">Databricks</a> for one of the applications I’m building. I sign up for a <a href=\"http://signup.databricks.com/\">Databricks trial</a> through their website.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/DB-1.png\"><img alt=\"Database homepage after login with option to Upgrade\" class=\"aligncenter wp-image-91560 size-full\" height=\"500\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/DB-1.png\" width=\"1443\" /></a></p> \\n<p>I chose <strong>Upgrade</strong> and see Databricks is available in AWS Marketplace, which gives me the option to <strong>Buy with AWS</strong>.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/DB-2.png\"><img alt=\"Option to upgrade to Databricks premium using Buy with AWS feature of AWS marketplace\" class=\"aligncenter wp-image-91561 size-full\" height=\"882\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/DB-2.png\" width=\"1426\" /></a></p> \\n<p>I choose <strong>Buy with AWS</strong>, and after I sign in to my AWS account, I land on a Databricks and AWS Marketplace co-branded procurement page.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/DB-3.png\"><img alt=\"Databricks and AWS co-branded page to subscribe using Buy with AWS\" class=\"aligncenter wp-image-91562 size-full\" height=\"888\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/DB-3.png\" width=\"1433\" /></a></p> \\n<p>I complete the purchase on the co-branded procurement page and continue to set up my Databricks account.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/DB-4.png\"><img alt=\"Databricks and AWS co-branded page after subscribing using Buy with AWS\" class=\"aligncenter wp-image-91563 size-full\" height=\"892\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/DB-4.png\" width=\"1431\" /></a></p> \\n<p>As you can tell, I didn’t have to navigate the challenge of managing procurement processes for multiple vendors. I also didn’t have to speak with a sales representative or onboard a new vendor in my billing system, which would have required multiple approvals and delayed the overall process.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Access centralized billing and benefits through AWS Marketplace</strong></span><br /> Because Buy with AWS purchases are transacted through and managed in AWS Marketplace, you also benefit from the post-purchase experience of AWS Marketplace, including consolidated AWS billing, centralized subscription management, and access to cost optimization tools.</p> \\n<p>For example, through the <a href=\"https://aws.amazon.com/aws-cost-management/aws-billing/\">AWS Billing and Cost Management console</a>, I can centrally manage all my AWS purchases, including Buy with AWS purchases, from one dashboard. I can easily access and process invoices for all of my organization’s AWS purchases. I also need to have valid <a href=\"https://docs.aws.amazon.com/marketplace/latest/buyerguide/buyer-iam-users-groups-policies.html\">AWS Identity and Access Management (IAM) permissions</a> to manage subscriptions and make a purchase through AWS Marketplace.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/billing-1-1.png\"><img alt=\"\" class=\"aligncenter size-full wp-image-91571\" height=\"703\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/24/billing-1-1.png\" width=\"1453\" /></a></p> \\n<p>AWS Marketplace not only simplifies my billing but also helps in maintaining governance over spending by helping me manage purchasing authority and subscription access for my organization with centralized visibility and controls. I can manage my budget with pricing flexibility, cost transparency, and AWS cost management tools.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Buy with AWS for Partners</strong></span><br /> Buy with AWS enables Partners who sell or resell products in AWS Marketplace to create new solution discovery and buying experiences for customers on their own websites. By adding call to action (CTA) buttons to their websites such as “Buy with AWS”, “Try free with AWS”, “Request private offer”, and “Request demo”, Partners can help accelerate product evaluation and the path-to-purchase for customers.</p> \\n<p>By integrating with <a href=\"https://docs.aws.amazon.com/marketplace/latest/APIReference/welcome.html\">AWS Marketplace APIs</a>, Partners can display products from the AWS Marketplace catalog, allow customers to sort and filter products, and streamline private offers. Partners implementing Buy with AWS can access AWS Marketplace creative and messaging resources for guidance on building their own web experiences. Partners who implement Buy with AWS can access metrics for insights into engagement and conversion performance.</p> \\n<p>The&nbsp;<a href=\"https://aws.amazon.com/marketplace/management/homepage?pageType=awsmpmp%3Acustomer\">Buy with AWS onboarding guide in the AWS Marketplace Management Portal</a> details how Partners can get started.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Learn more</strong></span><br /> Visit the <a href=\"https://aws.amazon.com/marketplace/features/buy-with-aws\">Buy with AWS page</a> to learn more and explore Partner sites that offer Buy with AWS.</p> \\n<p>To learn more about selling or reselling products using Buy with AWS on your website, visit:</p> \\n<ul> \\n <li><a href=\"https://aws.amazon.com/partners/marketplace/buy-with-aws/\">Buy with AWS seller page</a></li> \\n <li><a href=\"https://aws.amazon.com/marketplace/management/homepage?pageType=awsmpmp%3Acustomer\">Buy with AWS onboarding guide in the AWS Marketplace Management Portal</a></li> \\n</ul> \\n<p>– <a href=\"https://www.linkedin.com/in/kprasadrao/\">Prasad</a></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5f4d931d4077b46731468b69717f5ead7e44dfab', embedding=None, metadata={'title': 'Accelerate foundation model training and fine-tuning with new Amazon SageMaker HyperPod recipes', 'link': 'https://aws.amazon.com/blogs/aws/accelerate-foundation-model-training-and-fine-tuning-with-new-amazon-sagemaker-hyperpod-recipes/', 'date': 'Wed, 04 Dec 2024 18:21:16 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>Today, we’re announcing the general availability of <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes\">Amazon SageMaker HyperPod recipes</a> to help data scientists and developers of all skill sets to get started training and fine-tuning <a href=\"https://aws.amazon.com/what-is/foundation-models/\">foundation models</a> (FMs) in minutes with state-of-the-art performance. They can now access optimized recipes for training and fine-tuning popular publicly available FMs such as <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/fine-tuning/llama/hf_llama3_405b_seq128k_gpu_qlora.yaml\">Llama 3.1 405B</a>, <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/training/llama/hf_llama3_2_90b_seq8k_gpu_p5x32_pretrain.yaml\">Llama 3.2 90B</a>, or <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/training/mixtral/hf_mixtral_8x22b_seq8k_gpu_p5x32_pretrain.yaml\">Mixtral 8x22B</a>.</p> \\n<p>At AWS re:Invent 2023, we <a href=\"https://aws.amazon.com/blogs/aws/introducing-amazon-sagemaker-hyperpod-a-purpose-built-infrastructure-for-distributed-training-at-scale/\">introduced SageMaker HyperPod</a> to reduce time to train FMs by up to 40 percent and scale across more than a thousand compute resources in parallel with preconfigured distributed training libraries. With SageMaker HyperPod, you can find the required accelerated compute resources for training, create the most optimal training plans, and run training workloads across different blocks of capacity based on the availability of compute resources.</p> \\n<p>SageMaker HyperPod recipes include a training stack tested by AWS, removing tedious work experimenting with different model configurations, eliminating weeks of iterative evaluation and testing. The recipes automate several critical steps, such as loading training datasets, applying distributed training techniques, automating checkpoints for faster recovery from faults, and managing the end-to-end training loop.</p> \\n<p>With a simple recipe change, you can seamlessly switch between GPU- or Trainium-based instances to further optimize training performance and reduce costs. You can easily run workloads in production on SageMaker HyperPod or SageMaker training jobs.</p> \\n<p><u><strong>SageMaker HyperPod recipes in action</strong><br /> </u>To get started, visit the <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes\">SageMaker HyperPod recipes GitHub repository</a> to browse training recipes for popular publicly available FMs.</p> \\n<p><img alt=\"\" class=\"aligncenter wp-image-92923 size-full\" height=\"1332\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/03/2024-sagemaker-hyperpod-recipes-github-1.png\" style=\"width: 90%; border: solid 1px #ccc;\" width=\"1206\" /></p> \\n<p>You only need to edit straightforward recipe parameters to specify an instance type and the location of your dataset in cluster configuration, then run the recipe with a single line command to achieve state-of-art performance.</p> \\n<p>You need to edit the recipe config.yaml file to specify the model and cluster type after cloning the repository.</p> \\n<pre><code class=\"lang-bash\">$ git clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git\\n$ cd sagemaker-hyperpod-recipes\\n$ pip3 install -r requirements.txt.\\n$ cd ./recipes_collections\\n$ vim config.yaml</code></pre> \\n<p>The recipes support <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-slurm.html\">SageMaker HyperPod with Slurm</a>, <a href=\"https://aws.amazon.com/blogs/aws/amazon-sagemaker-hyperpod-introduces-amazon-eks-support/\">SageMaker HyperPod with Amazon Elastic Kubernetes Service (Amazon EKS)</a>, and <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html\">SageMaker training jobs</a>. For example, you can set up a cluster type (Slurm orchestrator), a model name (Meta Llama 3.1 405B language model), an instance type (<code>ml.p5.48xlarge</code>), and your data locations, such as storing the training data, results, logs, and so on.</p> \\n<pre><code class=\"lang-yaml\">defaults:\\n- <strong>cluster: slurm</strong> # support: slurm / k8s / sm_jobs\\n- <strong>recipes: fine-tuning/llama/hf_llama3_405b_seq8k_gpu_qlora</strong> # name of model to be trained\\ndebug: False # set to True to debug the launcher configuration\\n<strong>instance_type: ml.p5.48xlarge</strong> # or other supported cluster instances\\nbase_results_dir: # Location(s) to store the results, checkpoints, logs etc.</code></pre> \\n<p>You can optionally adjust model-specific training parameters in this YAML file, which outlines the optimal configuration, including the number of accelerator devices, instance type, training precision, parallelization and sharding techniques, the optimizer, and logging to monitor experiments through <a href=\"https://www.tensorflow.org/tensorboard\">TensorBoard</a>.</p> \\n<pre><code class=\"lang-yaml\">run:\\n  name: llama-405b\\n  results_dir: ${base_results_dir}/${.name}\\n  time_limit: \"6-00:00:00\"\\nrestore_from_path: null\\ntrainer:\\n  devices: 8\\n  num_nodes: 2\\n  accelerator: gpu\\n  precision: bf16\\n  max_steps: 50\\n  log_every_n_steps: 10\\n  ...\\nexp_manager:\\n  exp_dir: # location for TensorBoard logging\\n  name: helloworld \\n  create_tensorboard_logger: True\\n  create_checkpoint_callback: True\\n  checkpoint_callback_params:\\n    ...\\n  auto_checkpoint: True # for automated checkpointing\\nuse_smp: True \\ndistributed_backend: smddp # optimized collectives\\n# Start training from pretrained model\\nmodel:\\n  model_type: llama_v3\\n  train_batch_size: 4\\n  tensor_model_parallel_degree: 1\\n  expert_model_parallel_degree: 1\\n  # other model-specific params</code></pre> \\n<p>To run this recipe in SageMaker HyperPod with Slurm, you must prepare the SageMaker HyperPod cluster following the <a href=\"https://catalog.workshops.aws/sagemaker-hyperpod/en-US/01-cluster\">cluster setup instruction</a>.</p> \\n<p>Then, connect to the SageMaker HyperPod head node, access the Slurm controller, and copy the edited recipe. Next, you run a helper file to generate a Slurm submission script for the job that you can use for a dry run to inspect the content before starting the training job.</p> \\n<pre><code class=\"lang-bash\">$ python3 main.py --config-path recipes_collection --config-name=config</code></pre> \\n<p>After training completion, the trained model is automatically saved to your assigned data location.</p> \\n<p>To run this recipe on SageMaker HyperPod with Amazon EKS, clone the recipe from the GitHub repository, install the requirements, and edit the recipe (<code>cluster: k8s</code>) on your laptop. Then, create a link between your laptop and running the EKS cluster and subsequently use the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/smcluster-getting-started-cli.html\">HyperPod Command Line Interface (CLI)</a> to run the recipe.</p> \\n<pre><code class=\"lang-bash\">$ hyperpod start-job –recipe fine-tuning/llama/hf_llama3_405b_seq8k_gpu_qlora \\\\\\n--persistent-volume-claims fsx-claim:data \\\\\\n--override-parameters \\\\\\n\\'{\\n  \"recipes.run.name\": \"hf-llama3-405b-seq8k-gpu-qlora\",\\n  \"recipes.exp_manager.exp_dir\": \"/data/&lt;your_exp_dir&gt;\",\\n  \"cluster\": \"k8s\",\\n  \"cluster_type\": \"k8s\",\\n  \"container\": \"658645717510.dkr.ecr.&lt;region&gt;.amazonaws.com/smdistributed-modelparallel:2.4.1-gpu-py311-cu121\",\\n  \"recipes.model.data.train_dir\": \"&lt;your_train_data_dir&gt;\",\\n  \"recipes.model.data.val_dir\": \"&lt;your_val_data_dir&gt;\",\\n}\\'</code></pre> \\n<p>You can also run recipe on SageMaker training jobs using <a href=\"https://sagemaker.readthedocs.io/en/stable/\">SageMaker Python SDK</a>. The following example is running PyTorch training scripts on SageMaker training jobs with overriding training recipes.</p> \\n<pre><code class=\"lang-python\">...\\nrecipe_overrides = {\\n    \"run\": {\\n        \"results_dir\": \"/opt/ml/model\",\\n    },\\n    \"exp_manager\": {\\n        \"exp_dir\": \"\",\\n        \"explicit_log_dir\": \"/opt/ml/output/tensorboard\",\\n        \"checkpoint_dir\": \"/opt/ml/checkpoints\",\\n    },   \\n    \"model\": {\\n        \"data\": {\\n            \"train_dir\": \"/opt/ml/input/data/train\",\\n            \"val_dir\": \"/opt/ml/input/data/val\",\\n        },\\n    },\\n}\\npytorch_estimator = PyTorch(\\n           output_path=&lt;output_path&gt;,\\n           base_job_name=f\"llama-recipe\",\\n           role=&lt;role&gt;,\\n           instance_type=\"p5.48xlarge\",\\n           training_recipe=\"fine-tuning/llama/hf_llama3_405b_seq8k_gpu_qlora\",\\n           recipe_overrides=recipe_overrides,\\n           sagemaker_session=sagemaker_session,\\n           tensorboard_output_config=tensorboard_output_config,\\n)\\n...</code></pre> \\n<p>As training progresses, the model checkpoints are stored on <a href=\"https://aws.amazon.com/s3\">Amazon Simple Storage Service (Amazon S3)</a> with the fully automated checkpointing capability, enabling faster recovery from training faults and instance restarts.</p> \\n<p><strong><u>Now available</u></strong><br /> Amazon SageMaker HyperPod recipes are now available in the <a href=\"https://github.com/aws/sagemaker-hyperpod-recipes\">SageMaker HyperPod recipes GitHub repository</a>. To learn more, visit the <a href=\"https://aws.amazon.com/sagemaker-ai/hyperpod/\">SageMaker HyperPod product page</a> and the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html\">Amazon SageMaker AI Developer Guide</a>.</p> \\n<p>Give SageMaker HyperPod recipes a try and send feedback to <a href=\"https://repost.aws/tags/TAT80swPyVRPKPcA0rsJYPuA/amazon-sagemaker\">AWS re:Post for SageMaker</a> or through your usual AWS Support contacts.</p> \\n<p>— <a href=\"https://twitter.com/channyun\">Channy</a></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='895b231f474517397ead998b88bfac9286617a23', embedding=None, metadata={'title': 'AWS Education Equity Initiative: Applying generative AI to educate the next wave of innovators', 'link': 'https://aws.amazon.com/blogs/aws/aws-education-equity-initiative-applying-generative-ai-to-educate-the-next-wave-of-innovators/', 'date': 'Wed, 04 Dec 2024 18:13:13 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>Building on the work that we and our partners have been doing for many years, Amazon is committing up to $100 million in cloud technology and technical resources to help existing, dedicated learning organizations reach more learners by creating new and innovative digital learning solutions, all as part of the <a href=\"https://aws.amazon.com/about-aws/our-impact/education-equity-initiative/\">AWS Education Equity Initiative</a>.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>The Work So Far</strong></span><br /> AWS and Amazon have a long-standing commitment to learning and education. Here’s a sampling of what we have already done:</p> \\n<p><a href=\"https://aws.amazon.com/machine-learning/scholarship/\"><strong>AWS AI &amp; ML Scholarship Program</strong></a> – This program has awarded $28 million in scholarships to approximately 6000 students.</p> \\n<p><a href=\"https://aws.amazon.com/ai/machine-learning/educators/\"><strong>Machine Learning University</strong></a> – MLU offers a free program helping community colleges and Historically Black Colleges and Universities (HBCUs) teach data management, artificial intelligence, and machine learning concepts. The program is designed to address opportunity gaps by supporting students who are historically underserved and underrepresented in technology disciplines.</p> \\n<p><a href=\"https://www.amazonfutureengineer.com/\"><strong>Amazon Future Engineer</strong></a> – Since 2021, up to $46 million in scholarships has been awarded to 1150 students through this program. In the past year, more than 2.1 million students received over 17 million hours of STEM education, literacy, and career exploration courses through this and other Amazon philanthropic education programs in the United States. I was able to speak to one such session last year and it was an amazing experience:</p> \\n<p><img alt=\"\" class=\"aligncenter size-full wp-image-91813\" height=\"670\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/afe_jeff_2024_1.png\" width=\"892\" /></p> \\n<p><a href=\"https://www.aboutamazon.com/news/workplace/amazon-to-help-29-million-people-around-the-world-grow-their-tech-skills-with-free-cloud-computing-skills-training-by-2025\"><strong>Free Cloud Training</strong></a> – In late 2020 we set a goal of helping 29 million people grow their tech skills with free cloud computing training by 2025. We worked hard and met that target a year ahead of time!</p> \\n<p><span style=\"text-decoration: underline;\"><strong>There’s More To Do</strong></span><br /> Despite all of this work and progress, there’s still more to be done. The future is definitely not evenly distributed: over half a billion students cannot be reached by digital learning today.</p> \\n<p>We believe that Generative AI can amplify the good work that socially-minded edtech organizations, non-profits, and governments are already doing. Our goal is to empower them to build new and innovative digital learning systems that can amplify their work and allow them to reach a bigger audience.</p> \\n<p>With the launch of the AWS Education Equity Initiative, we want to help pave the way for the next generation of technology pioneers as they build powerful tools, train foundation models at scale, and create AI-powered teaching assistants.</p> \\n<p>We are committing up to $100 million in cloud technology and comprehensive technical advising over the next five years. The awardees will have access to the portfolio of AWS services and technical expertise so that they can build and scale learning management systems, mobile apps, chatbots, and other digital learning tools. As part of the application process, applicants will be asked to demonstrate how their proposed solution will benefit students from underserved and underrepresented communities.</p> \\n<p>As I mentioned earlier, our partners are already doing a lot of great work in this area. For example:</p> \\n<p><a href=\"https://code.org/\"><strong> Code.org</strong></a> has already used AWS to scale their free computer science curriculum to millions of students in more than 100 countries. With this initiative, they will expand their use of <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a> to provide an automated assessment of student projects, freeing up educator time that can be use for individual instruction and tailored learning.</p> \\n<p><a href=\"https://rocketlearning.org/\"><strong>Rocket Learning</strong></a> focuses on early childhood education in India. They will use Amazon Q in QuickSight to enhance learning outcomes for more than three million children.</p> \\n<p>I’m super excited about this initiative and look forward to seeing how it will help to create and educate the next generation of technology pioneers!</p> \\n<p></p>\\n<p>— <a href=\"https://twitter.com/jeffbarr\">Jeff</a>;</p>\\n<p></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6703e9b3f4cad53a2b6710109c7fdcd36d1688d7', embedding=None, metadata={'title': 'Solve complex problems with new scenario analysis capability in Amazon Q in QuickSight', 'link': 'https://aws.amazon.com/blogs/aws/solve-complex-problems-with-new-scenario-analysis-capability-in-amazon-q-in-quicksight/', 'date': 'Wed, 04 Dec 2024 17:57:51 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>Today, we announced a new capability of <a href=\"https://aws.amazon.com/quicksight/q/\">Amazon Q in QuickSight</a> that helps users perform scenario analyses to find answers to complex problems quickly. This AI-assisted data analysis experience helps business users find answers to complex problems by guiding them step-by-step through in-depth data analysis—suggesting analytical approaches, automatically analyzing data, and summarizing findings with suggested actions—using natural language prompts. This new capability eliminates hours of tedious and error-prone manual work traditionally required to perform analyses using spreadsheets or other alternatives. In fact, Amazon Q in QuickSight enables business users to perform complex scenario analysis up to 10x faster than spreadsheets. This capability expands upon existing data Q&amp;A capabilities of Amazon QuickSight so business professionals can start their analysis by simply asking a question.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>How it works</strong></span><br /> Business users are often faced with complex questions that have traditionally required specialized training and days or weeks of time analyzing data in spreadsheets or other tools to address. For example, let’s say you’re a franchisee with multiple locations to manage. You might use this new capability in Amazon Q in QuickSight to ask, “<em>How can I help our new Chicago store perform as well as the ﬂagship store in New York?</em>” Using an agentic approach, Amazon Q would then suggest analytical approaches needed to address the underlying business goal, automatically analyze data, and present results complete with visualizations and suggested actions. You can conduct this multistep analysis in an expansive analysis canvas, giving you the ﬂexibility to make changes, explore multiple analysis paths simultaneously, and adapt to situations over time.</p> \\n<p>This new analysis experience is part of Amazon QuickSight meaning it can read from QuickSight dashboards which connect to sources such as <a href=\"https://aws.amazon.com/athena/\">Amazon Athena</a>, <a href=\"https://aws.amazon.com/rds/aurora/\">Amazon Aurora</a>, <a href=\"https://aws.amazon.com/pm/redshift/\">Amazon Redshift</a>, <a href=\"https://aws.amazon.com/pm/serv-s3/\">Amazon Simple Storage Service (Amazon S3)</a>, and <a href=\"https://aws.amazon.com/opensearch-service/\">Amazon OpenSearch Service</a>. Specifically, this new experience is part of Amazon Q in QuickSight, which allows it to seamlessly integrate with other generative business intelligence (BI) capabilities such as data Q&amp;A. You can also upload either a .csv or a single-table, single-sheet .xlsx file to incorporate into your analysis.</p> \\n<p>Here’s a visual walkthrough of this new analysis experience in Amazon Q in QuickSight.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/image-23-2.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-91314\" height=\"350\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/image-23-2-1024x350.png\" width=\"1024\" /></a></p> \\n<p>I’m planning a customer event, and I’ve received an Excel spreadsheet of all who’ve registered to attend the event. I want to learn more about the attendees, so I analyze the spreadsheet and ask a few questions. I start by describing what I want to explore.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/image-24.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-91315\" height=\"481\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/image-24-1024x481.png\" width=\"1024\" /></a></p> \\n<p>I upload the spreadsheet to start my analysis. Firstly, I want to understand how many people have registered for the event.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/image-25.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-91316\" height=\"450\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/image-25-1024x450.png\" width=\"1024\" /></a></p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/screen5-2.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-91317\" height=\"376\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/screen5-2-1024x376.png\" width=\"1024\" /></a></p> \\n<p>To design an agenda that’s suitable for the audience, I want to understand the various roles that will be attending. I select on the <strong>+ icon</strong> to add a new block for asking a question following along the thread from the previous block.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/screen6-3.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-91318\" height=\"448\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/screen6-3-1024x448.png\" width=\"1024\" /></a></p> \\n<p>I can continue to ask more questions. However, there are suggested questions for analyzing my data even further, and I now select one of these suggested questions. I want to increase marketing efforts at companies that don’t currently have a lot of attendees in this case, companies with fewer than two attendees.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/screen8-2.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-91319\" height=\"382\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/screen8-2-1024x382.png\" width=\"1024\" /></a></p> \\n<p>Amazon Q executes the required analysis and keeps me updated of the progress. <strong>Step 1</strong> of the process identifies companies that have fewer than two attendees and lists them.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/screen9-1.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-91320\" height=\"435\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/screen9-1-1024x435.png\" width=\"1024\" /></a></p> \\n<p><strong>Step 2</strong> gives an estimate of how many more attendees I might get from each company if marketing efforts are increased.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/08/screen10.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-89396\" height=\"301\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/08/screen10-1024x301.png\" width=\"1024\" /></a></p> \\n<p>In <strong>Step 3</strong> I can see the potential increase in total attendees (including the percentage increase) in line with the increase in marketing efforts.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/08/screen11.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-89397\" height=\"321\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/08/screen11-1024x321.png\" width=\"1024\" /></a></p> \\n<p>Lastly, <strong>Step 4</strong> goes even further to highlight companies I should prioritize for these increased marketing efforts.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/08/screen12.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-89398\" height=\"244\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/08/screen12-1024x244.png\" width=\"1024\" /></a></p> \\n<p>To increase the potential number of attendees even more, I wanted to change the analysis to identify companies with fewer than three attendees instead of two attendees. I choose the <strong>AI sparkle icon</strong> in the upper right to launch a modal that I then use to provide more context and make specific changes to the previous result.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/08/screen14.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-89400\" height=\"227\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/08/screen14-1024x227.png\" width=\"1024\" /></a><br /> This change resulted in new projections, and I can choose to consider them for my marketing efforts or keep to the previous projections.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/08/screen15.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-89401\" height=\"123\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/08/screen15-1024x123.png\" width=\"1024\" /></a><br /> <span style=\"text-decoration: underline;\"><strong>Now available</strong></span><br /> Amazon Q in QuickSight Pro users can use this new capability in preview in the following <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Regions</a> at launch: US East (N. Virginia) and US West (Oregon). Get started with a <a href=\"https://aws.amazon.com/quicksight/pricing/\">free 30-day trial</a> of QuickSight today. To learn more, visit the <a href=\"https://docs.aws.amazon.com/quicksight/latest/user/working-with-scenarios.html\">Amazon QuickSight User Guide</a>. You can submit your questions to <a href=\"https://repost.aws/questions/QUBt6GS7a3TA6sTrsM-iE-hw/amazon-quicksight\">AWS re:Post for Amazon QuickSight</a>, or through your usual AWS Support contacts.</p> \\n<p>– <a href=\"https://www.linkedin.com/in/veliswa-boya/\">Veliswa</a>.</p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='45cc4696a14ba4b33f0777d03cb184d6d7c05cf0', embedding=None, metadata={'title': 'Use Amazon Q Developer to build ML models in Amazon SageMaker Canvas', 'link': 'https://aws.amazon.com/blogs/aws/use-amazon-q-developer-to-build-ml-models-in-amazon-sagemaker-canvas/', 'date': 'Wed, 04 Dec 2024 17:56:37 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>As a data scientist, I’ve experienced firsthand the challenges of making machine learning (ML) accessible to business analysts, marketing analysts, data analysts, and data engineers who are experts in their domains without ML experience. That’s why I’m particularly excited about today’s <a href=\"https://aws.amazon.com/\">Amazon Web Services (AWS)</a> announcement that <a href=\"https://aws.amazon.com/q/developer/\">Amazon Q Developer</a> is now available in <a href=\"https://aws.amazon.com/sagemaker/canvas/\">Amazon SageMaker Canvas</a>. What catches my attention is how Amazon Q Developer helps connect ML expertise with business needs, making ML more accessible across organizations.</p> \\n<p><a href=\"https://aws.amazon.com/q/developer/\">Amazon Q Developer</a> helps domain experts build accurate, production-quality ML models through natural language interactions, even if they don’t have ML expertise. Amazon Q Developer guides these users by breaking down their business problems and analyzing their data to recommend step-by-step guidance for building custom ML models. It transforms users’ data to remove anomalies, and builds and evaluates custom ML models to recommend the best one, while providing users control and visibility into every step of the guided ML workflow. This empowers organizations to innovate faster with reduced time to market. It also reduces their reliance on ML experts so their specialists can focus on more complex technical challenges.</p> \\n<p>For example, a marketing analyst can state, “I want to predict home sales prices using home characteristics and past sales data”, and Amazon Q Developer will translate this into a set of ML steps, analyzing relevant customer data, building multiple models, and recommending the best approach.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Let’s see it in action</strong></span><br /> To start using Amazon Q Developer, I follow the <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-getting-started.html\">Getting started with using Amazon SageMaker Canvas</a> guide to launch the Canvas application. In this demo, I use natural language instructions to create a model to predict house prices for marketing and finance teams. From the SageMaker Canvas page, I select <strong>Amazon Q</strong>&nbsp;and then choose <strong>Start a new conversation.</strong></p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/21/Screenshot-2024-11-21-at-2.02.00\\u202fPM.png\"><img alt=\"\" class=\"aligncenter wp-image-91203 size-full\" height=\"332\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/21/Screenshot-2024-11-21-at-2.02.00\\u202fPM.png\" style=\"border: 1px black solid;\" width=\"688\" /></a></p> \\n<p>In the new conversation I write:</p> \\n<p><strong><em>I am an analyst and need to predict house prices for my marketing and finance teams.</em></strong></p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/27/AmazonQ-CanvasPDP-1127.jpg\"><img alt=\"\" class=\"aligncenter size-full wp-image-92369\" height=\"813\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/27/AmazonQ-CanvasPDP-1127.jpg\" style=\"border: 1px black solid;\" width=\"824\" /></a></p> \\n<p>Next, Amazon Q Developer explains the problem and recommends the appropriate ML model type. It also outlines the solution requirements, including the necessary dataset characteristics. Amazon Q Developer then asks if <strong>I want to upload my dataset</strong> or<strong> I want to choose a target column</strong>. I select it to upload my dataset.</p> \\n<p>In the next step, Amazon Q Developer lists the dataset requirements, which include relevant information about houses, current house prices, and the target variable for the regression model. It then recommended next steps, including: <strong>I want to upload my dataset</strong>, <strong>Select an existing dataset</strong>, <strong>Create a new dataset</strong> or <strong>I want to choose a target column</strong>. For this demo, I’ll use the <strong>canvas-sample-housing.csv</strong> <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-sample-datasets.html\">sample dataset</a> as my existing dataset.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/select_an_existing_dataset.png\"><img alt=\"select_an_existing_dataset\" class=\"aligncenter size-large wp-image-90045\" height=\"1024\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/select_an_existing_dataset-922x1024.png\" style=\"border: 1px black solid;\" width=\"922\" /></a></p> \\n<p>After selecting and loading the dataset, Amazon Q Developer analyzes it and suggests <strong>median_house_value</strong> as the target column for the regression model. I accept by selecting <strong>I would like to predict the “median_house_value” column.</strong> Moving on to the next step, Amazon Q Developer details which dataset features (such as “location”, “housing_median_age”, and “total_rooms”) it will use to predict the median_house_value.</p> \\n<p><img class=\"size-large aligncenter\" height=\"864\" src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/AWSNews/2024/AWSNEWS-1199-upload-dataset.gif\" style=\"border: 1px black solid;\" width=\"864\" /></p> \\n<p>Before moving forward with model training, I ask about the data quality, because without good data we can’t build a reliable model. Amazon Q Developer responds with quality insights for my entire dataset.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/quality.png\"><img alt=\"\" class=\"aligncenter size-large wp-image-90051\" height=\"967\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/quality-1024x967.png\" style=\"border: 1px black solid;\" width=\"1024\" /></a></p> \\n<p>I can ask specific questions about individual features and their distributions to better understand the data quality.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/Screenshot-2024-11-13-at-7.01.49\\u202fPM.png\"><img alt=\"columns in dataset\" class=\"aligncenter size-large wp-image-90066\" height=\"1024\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/Screenshot-2024-11-13-at-7.01.49\\u202fPM-866x1024.png\" style=\"border: 1px black solid;\" width=\"866\" /></a></p> \\n<p>To my surprise, through the previous question, I discovered that the “households” column has a wide variation between extreme values, which could affect the model’s prediction accuracy. Therefore, I ask Amazon Q Developer to fix this outlier problem.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/Screenshot-2024-11-13-at-6.39.42\\u202fPM.png\"><img alt=\"\" class=\"aligncenter wp-image-90055 size-large\" height=\"1024\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/Screenshot-2024-11-13-at-6.39.42\\u202fPM-885x1024.png\" style=\"border: 1px black solid;\" width=\"885\" /></a></p> \\n<p>After the transformation is done, I can ask what steps Amazon Q Developer followed to make this change. Behind the scenes, Amazon Q Developer applies advanced data preparation steps using <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/canvas-data-export.html\">SageMaker Canvas data preparation capabilities</a>, which I can review and see the steps so that I can visualize and replicate the process to get the final, prepared dataset for training the model.</p> \\n<p><img class=\"alignnone size-large\" height=\"652\" src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/AWSNews/2024/AWSNEWS-1199-data.gif\" style=\"border: 1px black solid;\" width=\"1090\" /></p> \\n<p>After reviewing the data preparation steps, I select <strong>Launch my training job</strong>.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/Screenshot-2024-11-13-at-7.05.21\\u202fPM.png\"><img alt=\"launch training job\" class=\"aligncenter size-large wp-image-90068\" height=\"765\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/13/Screenshot-2024-11-13-at-7.05.21\\u202fPM-1024x765.png\" style=\"border: 1px black solid;\" width=\"1024\" /></a></p> \\n<p>After the training job is launched, I can see its progress in the conversation, and the datasets created.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/21/Screenshot-2024-11-21-at-11.30.20\\u202fAM.png\"><img alt=\"\" class=\"aligncenter wp-image-91170 size-full\" height=\"822\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/21/Screenshot-2024-11-21-at-11.30.20\\u202fAM.png\" style=\"border: 1px black solid;\" width=\"706\" /></a></p> \\n<p>As a data scientist, I particularly appreciate that, with Amazon Q Developer, Ican see detailed metrics such as the confusion matrix and precision-recall scores for classification models and root mean square error (RMSE) for regression models. These are crucial elements I always look for when evaluating model performance and making data-driven decisions, and it’s refreshing to see them presented in a way that’s accessible to nontechnical users to build trust and enable proper governance while maintaining the depth that technical teams need.</p> \\n<p>You can access these metrics by selecting the new model from <strong>My Models</strong> or from the <strong>Amazon Q </strong>conversation menu:</p> \\n<ul> \\n <li><strong>Overview – </strong>This tab shows the <strong>Column impact</strong> analysis. In this case, <em><strong>median_income</strong></em>&nbsp;emerges as the primary factor influencing my model.</li> \\n <li><strong>Scoring – </strong>This tab&nbsp;provides model accuracy insights, including RMSE metrics.</li> \\n <li><strong>Advanced metrics – </strong>This tab displays the detailed <strong>Metrics table</strong>,<strong> Residuals</strong> and <strong>Error density</strong> for in-depth model evaluation.</li> \\n</ul> \\n<p><img alt=\"Analyze My Model\" class=\"aligncenter size-medium\" height=\"864\" src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/AWSNews/2024/AWSNEWS-1199-analyze.gif\" style=\"border: 1px black solid;\" width=\"864\" /></p> \\n<p>After reviewing these metrics and validating the model’s performance, I can move to the final stages of the ML workflow:</p> \\n<ul> \\n <li><strong>Predictions –</strong> I can test my model using the <strong>Predictions</strong> tab to validate its real-world performance.</li> \\n <li><strong>Deployment </strong>– I can create an endpoint deployment to make my model available for production use.</li> \\n</ul> \\n<p>This simplifies the deployment process, a step that traditionally requires significant DevOps knowledge, into a straightforward operation that business analysts can handle confidently.</p> \\n<p><img alt=\"predictions and deploy\" class=\"aligncenter size-medium\" height=\"864\" src=\"https://d2908q01vomqb2.cloudfront.net/artifacts/AWSNews/2024/AWSNEWS-1199-end.gif\" style=\"border: 1px black solid;\" width=\"864\" /></p> \\n<p><span style=\"text-decoration: underline;\"><strong>Things to know</strong></span><br /> Amazon Q Developer democratizes ML across organizations:</p> \\n<p><strong>Empowering all skill levels with ML</strong> – Amazon Q Developer is now available in SageMaker Canvas, helping business analysts, marketing analysts, and data professionals who don’t have ML experience create solutions for business problems through a guided ML workflow. From data analysis and model selection to deployment, users can solve business problems using natural language, reducing dependence on ML experts such as data scientists and enabling organizations to innovate faster with reduced time to market.</p> \\n<p><strong>Streamlining the ML workflow </strong>– With Amazon Q Developer available in SageMaker Canvas, users can prepare data, and build, analyze, and deploy ML models through a guided, transparent workflow. Amazon Q Developer provides advanced data preparation and AutoML capabilities that democratize ML, and allows non-ML experts to produce highly-accurate ML models.</p> \\n<p><strong>Providing full visibility into the ML workflow</strong> – Amazon Q Developer provides full transparency by generating the underlying code and technical artifacts such as data transformation steps, model explainability, and accuracy measures. This allows cross-functional teams, including ML experts, to review, validate, and update the models as needed, facilitating collaboration in a secure environment.</p> \\n<p><strong>Availability</strong> – Amazon Q Developer is now in preview release in Amazon SageMaker Canvas.</p> \\n<p><strong>Pricing</strong> – Amazon Q Developer is now available in SageMaker Canvas at no additional cost to both <a href=\"https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/q-pro-tier.html\">Amazon Q Developer Pro Tier</a> and <a href=\"https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/q-free-tier.html\">Amazon Q Developer Free tier</a> users. However, standard charges apply for resources such as <a href=\"https://aws.amazon.com/sagemaker/canvas/pricing/\">SageMaker Canvas workspace</a> instances and any resources used for building or deploying models. For detailed pricing information, visit the <a href=\"https://aws.amazon.com/sagemaker/canvas/pricing/\">Amazon SageMaker Canvas Pricing.</a></p> \\n<p>To learn more about getting started visit the <a href=\"https://aws.amazon.com/q/developer/\">Amazon Q Developer product web page</a>.</p> \\n<p>— <a href=\"https://www.linkedin.com/in/lizfue/\">Eli</a></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c70235c3e3185931cb16a48df9d852ef0b2af318', embedding=None, metadata={'title': 'Amazon Bedrock Guardrails now supports multimodal toxicity detection with image support (preview)', 'link': 'https://aws.amazon.com/blogs/aws/amazon-bedrock-guardrails-now-supports-multimodal-toxicity-detection-with-image-support/', 'date': 'Wed, 04 Dec 2024 17:38:16 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>Today, we’re announcing the preview of multimodal toxicity detection with image support in <a href=\"https://aws.amazon.com/bedrock/guardrails/\">Amazon Bedrock Guardrails</a>. This new capability detects and filters out undesirable image content in addition to text, helping you improve user experiences and manage model outputs in your <a href=\"https://aws.amazon.com/ai/generative-ai/\">generative AI</a> applications.</p> \\n<p>Amazon Bedrock Guardrails helps you implement safeguards for generative AI applications by filtering undesirable content, redacting personally identifiable information (PII), and enhancing content safety and privacy. You can configure policies for denied topics, content filters, word filters, PII redaction, contextual grounding checks, and Automated Reasoning checks (preview), to tailor safeguards to your specific use cases and responsible AI policies.</p> \\n<p>With this launch, you can now use the existing content filter policy in Amazon Bedrock Guardrails to detect and block harmful image content across categories such as hate, insults, sexual, and violence. You can configure thresholds from low to high to match your application’s needs.</p> \\n<p>This new image support works with all <a href=\"https://aws.amazon.com/what-is/foundation-models/\">foundation models (FMs)</a> in Amazon Bedrock that support image data, as well as any custom fine-tuned models you bring. It provides a consistent layer of protection across text and image modalities, making it easier to build responsible AI applications.</p> \\n<p><a href=\"https://www.linkedin.com/in/terohottinen/\">Tero Hottinen</a>, VP, Head of Strategic Partnerships at <a href=\"http://www.kone.com\">KONE</a>, envisions the following use case:</p> \\n<blockquote>\\n <p>In its ongoing evaluation, KONE recognizes the potential of Amazon Bedrock Guardrails as a key component in protecting gen AI applications, particularly for relevance and contextual grounding checks, as well as the multimodal safeguards. The company envisions integrating product design diagrams and manuals into its applications, with Amazon Bedrock Guardrails playing a crucial role in enabling more accurate diagnosis and analysis of multimodal content.</p>\\n</blockquote> \\n<p>Here’s how it works.</p> \\n<p><strong><u>Multimodal toxicity detection in action<br /> </u></strong>To get started, create a guardrail in the <a href=\"https://aws.amazon.com/console/\">AWS Management Console</a> and configure the content filters for either text or image data or both. You can also use <a href=\"https://aws.amazon.com/developer/tools/\">AWS SDKs</a> to integrate this capability into your applications.</p> \\n<p><strong>Create guardrail<br /> </strong>On the <a href=\"https://console.aws.amazon.com/console/home\">console</a>, navigate to<strong> Amazon Bedrock</strong> and select <strong>Guardrails</strong>. From there, you can create a new guardrail and use the existing content filters to detect and block image data in addition to text data. The categories for <strong>Hate</strong>, <strong>Insults</strong>, <strong>Sexual</strong>, and <strong>Violence</strong> under <strong>Configure content filters</strong> can be configured for either text or image content or both. The <strong>Misconduct</strong>&nbsp;and <strong>Prompt attacks</strong> categories can be configured for text content only.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/23/2024-guardrails-toxicity-7.png\"><img alt=\"Amazon Bedrock Guardrails Multimodal Support\" class=\"aligncenter wp-image-91497 size-full\" height=\"1138\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/23/2024-guardrails-toxicity-7.png\" width=\"1442\" /></a></p> \\n<p>After you’ve selected and configured the content filters you want to use, you can save the guardrail and start using it to build safe and responsible generative AI applications.</p> \\n<p>To test the new guardrail in the console, select the guardrail and choose <strong>Test</strong>. You have two options: test the guardrail by choosing and invoking a model or to test the guardrail without invoking a model by using the Amazon Bedrock Guardrails independent <code>ApplyGuardail</code> API.</p> \\n<p>With the <code>ApplyGuardrail</code> API, you can validate content at any point in your application flow before processing or serving results to the user. You can also use the API to evaluate inputs and outputs for any self-managed (custom), or third-party FMs, regardless of the underlying infrastructure. For example, you could use the API to evaluate a <a href=\"https://www.llama.com/\">Meta Llama 3.2</a> model hosted on <a href=\"https://aws.amazon.com/sagemaker/\">Amazon SageMaker</a> or a <a href=\"https://mistral.ai/news/mistral-nemo/\">Mistral NeMo</a> model running on your laptop.</p> \\n<p><strong>Test guardrail by choosing and invoking a model<br /> </strong>Select a model that supports image inputs or outputs, for example, Anthropic’s Claude 3.5 Sonnet. Verify that the prompt and response filters are enabled for image content. Next, provide a prompt, upload an image file, and choose <strong>Run</strong>.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-2-1.png\"><img alt=\"Amazon Bedrock Guardrails Multimodal Support\" class=\"aligncenter wp-image-90493 size-full\" height=\"803\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-2-1.png\" width=\"1165\" /></a></p> \\n<p>In my example, Amazon Bedrock Guardrails intervened. Choose <strong>View trace</strong> for more details.</p> \\n<p>The guardrail trace provides a record of how safety measures were applied during an interaction. It shows whether Amazon Bedrock Guardrails intervened or not and what assessments were made on both input (prompt) and output (model response). In my example, the content filters blocked the input prompt because they detected insults in the image with a high confidence.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-3.png\"><img alt=\"Amazon Bedrock Guardrails Multimodal Support\" class=\"aligncenter wp-image-90489 size-full\" height=\"890\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-3.png\" width=\"1172\" /></a></p> \\n<p><strong>Test guardrail without invoking a model<br /> </strong>In the console,&nbsp;choose <strong>Use Guardrails independent API</strong> to test the guardrail without invoking a model. Choose whether you want to validate an input prompt or an example of a model generated output. Then, repeat the steps from before. Verify that the prompt and response filters are enabled for image content, provide the content to validate, and choose <strong>Run</strong>.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-5.png\"><img alt=\"Amazon Bedrock Guardrails Multimodal Support\" class=\"aligncenter size-full wp-image-90486\" height=\"799\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-5.png\" width=\"1163\" /></a></p> \\n<p>I reused the same image and input prompt for my demo, and Amazon Bedrock Guardrails intervened again. Choose <strong>View trace</strong> again for more details.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-6.png\"><img alt=\"Amazon Bedrock Guardrails Multimodal Support\" class=\"aligncenter wp-image-90487 size-full\" height=\"840\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/16/2024-guardrails-toxicity-6.png\" width=\"1176\" /></a></p> \\n<p><strong><u>Join the preview<br /> </u></strong>Multimodal toxicity detection with image support is available today in preview in Amazon Bedrock Guardrails in the US East (N. Virginia, Ohio), US West (Oregon), Asia Pacific (Mumbai, Seoul, Singapore, Tokyo), Europe (Frankfurt, Ireland, London), and AWS GovCloud (US-West)&nbsp;<a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region\">AWS Regions</a>. To learn more, visit <a href=\"https://aws.amazon.com/bedrock/guardrails/\">Amazon Bedrock Guardrails</a>.</p> \\n<p>Give the multimodal toxicity detection content filter a try today in the <a href=\"https://console.aws.amazon.com/bedrock/home#/guardrails\">Amazon Bedrock console</a> and let us know what you think! Send feedback to <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag\">AWS re:Post for Amazon Bedrock</a> or through your usual AWS Support contacts.</p> \\n<p>—&nbsp;<a href=\"https://www.linkedin.com/in/antje-barth/\" rel=\"noopener noreferrer\" target=\"_blank\">Antje</a></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='6c66c1933fafb068e6c428f028a30fdcb6ca46cf', embedding=None, metadata={'title': 'New Amazon Bedrock capabilities enhance data processing and retrieval', 'link': 'https://aws.amazon.com/blogs/aws/new-amazon-bedrock-capabilities-enhance-data-processing-and-retrieval/', 'date': 'Wed, 04 Dec 2024 17:35:20 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>Today, <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a> introduces four enhancements that streamline how you can analyze data with <a href=\"https://aws.amazon.com/ai/generative-ai/\">generative AI</a>:</p> \\n<p><strong>Amazon Bedrock Data Automation (preview) </strong>– A fully managed capability of Amazon Bedrock that streamlines the generation of valuable insights from unstructured, multimodal content such as documents, images, audio, and videos. With Amazon Bedrock Data Automation, you can build automated <a href=\"https://aws.amazon.com/ai/generative-ai/use-cases/document-processing/\">intelligent document processing (IDP)</a>, media analysis, and <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/\">Retrieval-Augmented Generation (RAG)</a> workflows quickly and cost-effectively. Insights include video summaries of key moments, detection of inappropriate image content, automated analysis of complex documents, and much more. You can customize outputs to tailor insights into your specific business needs. Amazon Bedrock Data Automation can be used as a standalone feature or as a parser when setting up a knowledge base for RAG workflows.</p> \\n<p><strong>Amazon Bedrock Knowledge Bases now processes multimodal data</strong> –To help build applications that process both text and visual elements in documents and images, you can configure a knowledge base to parse documents using either Amazon Bedrock Data Automation or use a <a href=\"https://aws.amazon.com/what-is/foundation-models/\">foundation model (FM)</a> as the parser. Multimodal data processing can improve the accuracy and relevancy of the responses you get from a knowledge base which includes information embedded in both images and text.</p> \\n<p><strong>Amazon Bedrock Knowledge Bases now supports GraphRAG (preview)</strong> – We now offer one of the first fully-managed GraphRAG capabilities. GraphRAG enhances generative AI applications by providing more accurate and comprehensive responses to end users by using RAG techniques combined with graphs.</p> \\n<p><strong>Amazon Bedrock Knowledge Bases now supports structured data retrieval</strong> – This capability extends a knowledge base to support natural language querying of data warehouses and data lakes so that applications can access business intelligence (BI) through conversational interfaces and improve the accuracy of the responses by including critical enterprise data. Amazon Bedrock Knowledge Bases provides one of the first fully-managed out-of-the-box RAG solutions that can natively query structured data from where it resides. This capability helps break data silos across data sources and accelerates building generative AI applications from over a month to just a few days.</p> \\n<p>These new capabilities make it easier to build comprehensive AI applications that can process, understand, and retrieve information from structured and unstructured data sources. For example, a car insurance company can use Amazon Bedrock Data Automation to automate their claims adjudication workflow to reduce the time taken to process automobile claims, improving the productivity of their claims department.</p> \\n<p>Similarly, a media company can analyze TV shows and extract insights needed for smart advertisement placement such as scene summaries, industry standard advertising taxonomies (IAB), and company logos. A media production company can generate scene-by-scene summaries and capture key moments in their video assets. A financial services company can process complex financial documents containing charts and tables and use GraphRAG to understand relationships between different financial entities. All these companies can use structured data retrieval to query their data warehouse while retrieving information from their knowledge base.</p> \\n<p>Let’s take a closer look at these features.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Introducing Amazon Bedrock Data Automation<br /> </strong></span>Amazon Bedrock Data Automation is a capability of Amazon Bedrock that simplifies the process of extracting valuable insights from multimodal, unstructured content, such as documents, images, videos, and audio files.</p> \\n<p>Amazon Bedrock Data Automation provides a unified, API-driven experience that developers can use to process multimodal content through a single interface, eliminating the need to manage and orchestrate multiple AI models and services. With built-in safeguards, such as visual grounding and confidence scores, Amazon Bedrock Data Automation helps promote the accuracy and trustworthiness of the extracted insights, making it easier to integrate into enterprise workflows.</p> \\n<p>Amazon Bedrock Data Automation supports 4 modalities (documents, images, video, and audio). When used in an application, all modalities use the same asynchronous inference API, and results are written to an <a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service (Amazon S3)</a> bucket.</p> \\n<p>For each modality, you can configure the output based on your processing needs and&nbsp;generate two types of outputs:</p> \\n<p><strong>Standard output</strong> – With standard output, you get predefined default insights that are relevant to the input data type. Examples include semantic representation of documents, summaries of videos by scene, audio transcripts and more. You can configure which insights you want to extract with just a few steps.</p> \\n<p><strong>Custom output</strong> –&nbsp;With custom output, you have the flexibility to define and specify your extraction needs using artifacts called “blueprints” to generate insights tailored to your business needs. You can also transform the generated output into a specific format or schema that is compatible with your downstream systems such as databases or other applications.</p> \\n<p>Standard output can be used with all formats (audio, documents, images, and videos). During the preview, custom output can only be used with documents and images.</p> \\n<p>Both standard and custom output configurations can be saved in a project to reference in the Amazon Bedrock Data Automation inference API. A project can be configured to generate both standard output and custom output for each processed file.</p> \\n<p>Let’s look at an example of processing a document for both standard and custom outputs.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Using Amazon Bedrock Data Automation</strong></span><br /> On the <a href=\"https://console.aws.amazon.com/bedrock\">Amazon Bedrock console</a>, I choose <strong>Data Automation</strong> in the navigation pane. Here, I can review how this capability works with a few sample use cases.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-bda-1.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91923\" height=\"935\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-bda-1.png\" width=\"1563\" /></a></p> \\n<p>Then, I choose <strong>Demo</strong> in the <strong>Data Automation</strong> section of the navigation pane. I can try this capability using one of the provided sample documents or by uploading my own. For example, let’s say I am working on an application that needs to process birth certificates.</p> \\n<p>I start by uploading a birth certificate to see the standard output results. The first time I upload a document, I’m asked to confirm to create an S3 bucket to store the assets. When I look at the standard output, I can tailor the result with a few quick settings.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/02/bedrock-bda-demo-standard.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92797\" height=\"1530\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/02/bedrock-bda-demo-standard.png\" width=\"1603\" /></a></p> \\n<p>I choose the <strong>Custom output</strong> tab. The document is recognized by one of the sample blueprints and information is extracted across multiple fields.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-bda-demo-custom-1.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92499\" height=\"1421\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-bda-demo-custom-1.png\" width=\"1401\" /></a></p> \\n<p>Most of the data for my application is there but I need a few customizations. For example, the date the birth certificate was issued (<code>JUNE 10, 2022</code>) is in a different format than the other dates in the document. I also need the state that issued the certificate and a couple of flags that tell me if the child last name matches the one from the mother or the father.</p> \\n<p>Most of the fields in the previous blueprint use the <strong>Explicit</strong> extraction type. That means they’re extracted as they are from the document.</p> \\n<p>If I want a date in a specific format, I can create a new field using the <strong>Inferred</strong> extraction type and add instructions on how to format the result starting from the content of the document. Inferred extractions can be used to perform transformations, such as date or Social Security number&nbsp;(SSN) format, or validations, for example, to check if a person is over 21 based on today’s date.</p> \\n<p>Sample blueprints cannot be edited. I choose <strong>Duplicate blueprint</strong> to create a new blueprint that I can edit and then <strong>Add field</strong> from the <strong>Fields</strong> drop down.</p> \\n<p>I add four fields with extraction type <strong>Inferred</strong> and these instructions:</p> \\n<ol> \\n <li><code>The date the birth certificate was issued in MM/DD/YYYY format</code></li> \\n <li><code>The state that issued the birth certificate&nbsp;</code></li> \\n <li><code>Is ChildLastName equal to FatherLastName</code></li> \\n <li><code>Is ChildLastName equal to MotherLastName</code></li> \\n</ol> \\n<p>The first two fields are strings and the last two booleans.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-bda-demo-add-fields.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92500\" height=\"1375\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-bda-demo-add-fields.png\" width=\"1484\" /></a></p> \\n<p>After I create the new fields, I can apply the new blueprint to the document I previously uploaded.</p> \\n<p>I choose <strong>Get result</strong> and look for the new fields in the results. I see the date formatted as I need, the two flags, and the state.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-bda-demo-results.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92501\" height=\"668\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-bda-demo-results.png\" width=\"1617\" /></a></p> \\n<p>Now that I have created this custom blueprint tailored to the needs of my application, I can add it to a project. I can associate multiple blueprints with a project for the different document types I want to process, such as a blueprint for passports, a blueprint for birth certificates, a blueprint for invoices, and so on. When processing documents, Amazon Bedrock Data Automation matches each document to a blueprints within the project to extract relevant information.</p> \\n<p>I can also create a new blueprint form scratch. In that case, I can start with a prompt where I declare any fields I expect to find in the uploaded document and perform normalizations or validations.</p> \\n<p>Amazon Bedrock Data Automation can also process audio and video files. For example, here’s the standard output when uploading a video from a keynote presentation by <a href=\"https://www.linkedin.com/in/swaminathansivasubramanian/\">Swami Sivasubramanian VP, AI and Data at AWS</a>.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/03/bedrock-bda-demo-video-output.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92889\" height=\"1658\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/03/bedrock-bda-demo-video-output.png\" width=\"1565\" /></a></p> \\n<p>It takes a few minutes to get the output. The results include a summarization of the overall video, a summary scene by scene, and the text that appears during the video. From here, I can toggle the options to have a full audio transcript, content moderation, or <a href=\"https://www.iab.com/\">Interactive Advertising Bureau (IAB)</a> taxonomy.</p> \\n<p>I can also use Amazon Bedrock Data Automation as a parser when creating a knowledge base to extract insights from visually rich documents and images, for retrieval and response generation. Let’s see that in the next section.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Using multimodal data processing in Amazon Bedrock Knowledge Bases</strong></span><br /> Multimodal data processing support enables applications to understand both text and visual elements in documents.</p> \\n<p>With multimodal data processing, applications can use a knowledge base to:</p> \\n<ul> \\n <li>Retrieve answers from visual elements in addition to existing support of text.</li> \\n <li>Generate responses based on the context that includes both text and visual data.</li> \\n <li>Provide source attribution that references visual elements from the original documents.</li> \\n</ul> \\n<p>When creating a knowledge base in the Amazon Bedrock console, I now have the option to select <strong>Amazon Bedrock Data Automation</strong> as <strong>Parsing strategy</strong>.</p> \\n<p>When I select <strong>Amazon Bedrock Data Automation as parser</strong>, Amazon Bedrock Data Automation handles the extraction, transformation, and generation of insights from visually rich content, while Amazon Bedrock Knowledge Bases manages ingestion, retrieval, model response generation, and source attribution.</p> \\n<p>Alternatively, I can use the existing <strong>Foundation models as a parser</strong> option. With this option, there’s now support for Anthropic’s Claude 3.5 Sonnet as parser, and I can use the default prompt or modify it to suit a specific use case.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-kb-bda-3.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91892\" height=\"1094\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-kb-bda-3.png\" width=\"1241\" /></a></p> \\n<p>In the next step, I specify the <strong>Multimodal storage destination</strong> on Amazon S3 that will be used by Amazon Bedrock Knowledge Bases to store images extracted from my documents in the knowledge base data source. These images can be retrieved based on a user query, used to generate the response, and cited in the response.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-kb-bda-storage-1.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91886\" height=\"1389\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-kb-bda-storage-1.png\" width=\"1246\" /></a></p> \\n<p>When using the knowledge base, the information extracted by Amazon Bedrock Data Automation or FMs as parser is used to retrieve information about visual elements, understand charts and diagrams, and provide responses that reference both textual and visual content.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Using GraphRAG in Amazon Bedrock Knowledge Bases<br /> </strong></span>Extracting insights from scattered data sources presents significant challenges for RAG applications, requiring multi-step reasoning across these data sources to generate relevant responses. For example, a customer might ask a generative AI-powered travel application to identify family-friendly beach destinations with direct flights from their home location that also offer good seafood restaurants. This requires a connected workflow to identify suitable beaches that other families have enjoyed, match these to flight routes, and select highly-rated local restaurants. A traditional RAG system may struggle to synthesize all these pieces into a cohesive recommendation because the information lives in disparate sources and is not interlinked.</p> \\n<p>Knowledge graphs can address this challenge by modeling complex relationships between entities in a structured way. However, building and integrating graphs into an application requires significant expertise and effort.</p> \\n<p>Amazon Bedrock Knowledge Bases now offers one of the first fully managed GraphRAG capabilities that enhances generative AI applications by providing more accurate and comprehensive responses to end users by using RAG techniques combined with graphs.</p> \\n<p>When creating a knowledge base, I can now enable GraphRAG in just a few steps by choosing <a href=\"https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html\">Amazon Neptune Analytics</a> as database, automatically generating vector and graph representations of the underlying data, entities and their relationships, and reducing development effort from several weeks to just a few hours.</p> \\n<p>I start the creation of new knowledge base. In the <strong>Vector database section</strong>, when creating a new vector store, I select <strong>Amazon Neptune Analytics (GraphRAG)</strong>. If I don’t want to create a new graph, I can provide an existing vector store and select a Neptune Analytics graph from the list. GraphRAG uses <a href=\"https://aws.amazon.com/bedrock/claude/\">Anthropic’s Claude 3 Haiku</a> to automatically build graphs for a knowledge base.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-kb-graph-rag-1.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91887\" height=\"1389\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-kb-graph-rag-1.png\" width=\"1246\" /></a></p> \\n<p>After I complete the creation of the knowledge base, Amazon Bedrock automatically builds a graph, linking related concepts and documents. When retrieving information from the knowledge base, GraphRAG traverses these relationships to provide more comprehensive and accurate responses.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Using structured data retrieval in Amazon Bedrock Knowledge Bases<br /> </strong></span>Structured data retrieval allows natural language querying of databases and data warehouses. For example, a business analyst might ask, “What were our top-selling products last quarter?” and the system automatically generates and runs the appropriate SQL query for a data warehouse stored in an <a href=\"https://aws.amazon.com/redshift/\">Amazon Redshift</a> database.</p> \\n<p>When creating a knowledge base, I now have the option to use a <strong>structured data store</strong>.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/kb-structured-create.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91907\" height=\"177\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/kb-structured-create.png\" width=\"1561\" /></a></p> \\n<p>I enter a name and description for the knowledge base. In <strong>Data source details</strong>, I use <a href=\"https://aws.amazon.com/redshift/\">Amazon Redshift</a> as <strong>Query engine</strong>. I create a new <a href=\"https://aws.amazon.com/iam/\">AWS Identity and Access Management (IAM)</a> service role to manage the knowledge base resources and choose <strong>Next</strong>.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/kb-structured-create-details.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91908\" height=\"1252\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/kb-structured-create-details.png\" width=\"1241\" /></a></p> \\n<p>I choose <strong>Redshift serverless</strong> in <strong>Connection options</strong> and the <strong>Workgroup</strong> to use. Amazon Redshift provisioned clusters are also supported. I use the previously created IAM role for <strong>Authentication</strong>. Storage metadata can be managed with <strong>AWS Glue Data Catalog</strong> or directly within an Amazon Redshift database. I select a database from the list.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/kb-structured-create-query-engine.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91909\" height=\"1569\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/kb-structured-create-query-engine.png\" width=\"1241\" /></a></p> \\n<p>In the configuration of the knowledge base, I can define the maximum duration for a query and include or exclude access to tables or columns. To improve the accuracy of query generation from natural language, I can optionally add a description for tables and columns and a list of curated queries that provides practical examples of how to translate a question into a SQL query for my database. I choose <strong>Next</strong>, review the settings, and complete the creation of the knowledge base</p> \\n<p>After a few minutes, the knowledge base is ready. Once synced, Amazon Bedrock Knowledge Bases handles generating, running, and formatting the result of the query, making it easy to build natural language interfaces to structured data. When invoking a knowledge base using structured data, I can ask to only generate SQL, retrieve data, or summarize the data in natural language.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Things to know<br /> </strong></span>These new capabilities are available today in the following <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Regions</a>:</p> \\n<ul> \\n <li>Amazon Bedrock Data Automation is available in preview in US West (Oregon).</li> \\n <li>Multimodal data processing support in Amazon Bedrock Knowledge Bases using Amazon Bedrock Data Automation as parser is available in preview in US West (Oregon). FM as a parser is available in all Regions where Amazon Bedrock Knowledge Bases is offered.</li> \\n <li>GraphRAG in Amazon Bedrock Knowledge Bases is available in preview in all commercial Regions where Amazon Bedrock Knowledge Bases and Amazon Neptune Analytics are offered.</li> \\n <li>Structured data retrieval is available in Amazon Bedrock Knowledge Bases in all commercial Regions where Amazon Bedrock Knowledge Bases is offered.</li> \\n</ul> \\n<p>As usual with Amazon Bedrock, pricing is based on usage:</p> \\n<ul> \\n <li>Amazon Bedrock Data Automation charges per images, per page for documents, and per minute for audio or video.</li> \\n <li>Multimodal data processing in Amazon Bedrock Knowledge Bases is charged based on the use of either Amazon Bedrock Data Automation or the FM as parser.</li> \\n <li>There is no additional cost for using GraphRAG in Amazon Bedrock Knowledge Bases but you pay for using <a href=\"https://docs.aws.amazon.com/neptune-analytics/latest/userguide/what-is-neptune-analytics.html\">Amazon Neptune Analytics</a> as the vector store. For more information, visit <a href=\"https://aws.amazon.com/neptune/pricing/\">Amazon Neptune pricing</a>.</li> \\n <li>There is an additional cost when using structured data retrieval in Amazon Bedrock Knowledge Bases.</li> \\n</ul> \\n<p>For detailed pricing information, see <a href=\"https://aws.amazon.com/bedrock/pricing/\">Amazon Bedrock pricing</a>.</p> \\n<p>Each capability can be used independently or in combination. Together, they make it easier and faster to build applications that use AI to process data. To get started, visit the <a href=\"https://console.aws.amazon.com/bedrock\">Amazon Bedrock console</a>. To learn more, you can access the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html\">Amazon Bedrock documentation</a> and send feedback to <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag/amazon-bedrock\">AWS re:Post for Amazon Bedrock</a>. You can find deep-dive technical content and discover how our Builder communities are using Amazon Bedrock at <a href=\"https://community.aws/\">community.aws</a>. Let us know what you build with these new capabilities!</p> \\n<p>— <a href=\"https://twitter.com/danilop\">Danilo</a></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4778d78f6e2e6f9f5978d0b34961edebc2bac614', embedding=None, metadata={'title': 'Reduce costs and latency with Amazon Bedrock Intelligent Prompt Routing and prompt caching (preview)', 'link': 'https://aws.amazon.com/blogs/aws/reduce-costs-and-latency-with-amazon-bedrock-intelligent-prompt-routing-and-prompt-caching-preview/', 'date': 'Wed, 04 Dec 2024 17:22:27 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p><em><strong>December 5, 2024</strong>: Added instructions to request access to the Amazon Bedrock prompt caching preview.&nbsp;</em></p> \\n<p>Today, <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a> has introduced in preview two capabilities that help reduce costs and latency for <a href=\"https://aws.amazon.com/ai/generative-ai\">generative AI</a> applications:</p> \\n<p><strong>Amazon Bedrock Intelligent Prompt Routing</strong> – When invoking a model, you can now use a combination of <a href=\"https://aws.amazon.com/what-is/foundation-models/\">foundation models (FMs)</a> from the same model family to help optimize for quality and cost. For example, with the <a href=\"https://aws.amazon.com/bedrock/claude/\">Anthropic’s Claude</a> model family, Amazon Bedrock can intelligently route requests between Claude 3.5 Sonnet and Claude 3 Haiku depending on the complexity of the prompt. Similarly, Amazon Bedrock can route requests between <a href=\"https://aws.amazon.com/bedrock/llama/\">Meta Llama</a> 3.1 70B and 8B. The prompt router predicts which model will provide the best performance for each request while optimizing the quality of response and cost. This is particularly useful for applications such as customer service assistants, where uncomplicated queries can be handled by smaller, faster, and more cost-effective models, and complex queries are routed to more capable models. Intelligent Prompt Routing can reduce costs by up to 30 percent without compromising on accuracy.</p> \\n<p><strong>Amazon Bedrock now supports prompt caching</strong> – You can now cache frequently used context in prompts across multiple model invocations. This is especially valuable for applications that repeatedly use the same context, such as document Q&amp;A systems where users ask multiple questions about the same document or coding assistants that need to maintain context about code files. The cached context remains available for up to 5 minutes after each access. Prompt caching in Amazon Bedrock can reduce costs by up to 90% and latency by up to 85% for supported models.</p> \\n<p>These features make it easier to reduce latency and balance performance with cost efficiency. Let’s look at how you can use them in your applications.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Using Amazon Bedrock Intelligent Prompt Routing in the console<br /> </strong></span>Amazon Bedrock Intelligent Prompt Routing uses advanced prompt matching and model understanding techniques to predict the performance of each model for every request, optimizing for quality of responses and cost. During the preview, you can use the default prompt routers for <a href=\"https://aws.amazon.com/bedrock/claude/\">Anthropic’s Claude</a> and <a href=\"https://aws.amazon.com/bedrock/llama/\">Meta Llama</a> model families.</p> \\n<p>Intelligent prompt routing can be accessed through the <a href=\"https://console.aws.amazon.com\">AWS Management Console</a>, the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a>, and the <a href=\"https://aws.amazon.com/tools/\">AWS SDKs</a>. In the <a href=\"https://console.aws.amazon.com/bedrock\">Amazon Bedrock console</a>, I choose <strong>Prompt routers</strong> in the <strong>Foundation models</strong> section of the navigation pane.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/02/bedrock-prompt-routers.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92808\" height=\"676\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/12/02/bedrock-prompt-routers.png\" width=\"1465\" /></a></p> \\n<p>I choose the <strong>Anthropic Prompt Router</strong> default router to get more information.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-prompt-routers-anthropic-1.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91863\" height=\"496\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-prompt-routers-anthropic-1.png\" width=\"1481\" /></a></p> \\n<p>From the configuration of the prompt router, I see that it’s routing requests between Claude 3.5 Sonnet and Claude 3 Haiku using <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html\">cross-Region inference profiles</a>. The routing criteria defines the quality difference between the response of the largest model and the smallest model for each prompt as predicted by the router internal model at runtime. The fallback model, used when none of the chosen models meet the desired performance criteria, is Anthropic’s Claude 3.5 Sonnet.</p> \\n<p>I choose <strong>Open in Playground</strong> to chat using the prompt router and enter this prompt:</p> \\n<p><code>Alice has N brothers and she also has M sisters. How many sisters does Alice’s brothers have?</code></p> \\n<p>The result is quickly provided. I choose the new <strong>Router metrics</strong> icon on the right to see which model was selected by the prompt router. In this case, because the question is rather complex, Anthropic’s Claude 3.5 Sonnet was used.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/15/bedrock-prompt-routers-anthropic-chat.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-90288\" height=\"447\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/15/bedrock-prompt-routers-anthropic-chat.png\" width=\"1476\" /></a></p> \\n<p>Now I ask a straightforward question to the same prompt router:</p> \\n<p><code>Describe the purpose of a \\'hello world\\' program in one line.</code></p> \\n<p>This time, Anthropic’s Claude 3 Haiku has been selected by the prompt router.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-prompt-routers-anthropic-chat-simple.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91980\" height=\"365\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-prompt-routers-anthropic-chat-simple.png\" width=\"1615\" /></a></p> \\n<p>I select the <strong>Meta Prompt Router</strong> to check its configuration. It’s using the cross-Region inference profiles for Llama 3.1 70B and 8B with the 70B model as fallback.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-prompt-routers-meta-1.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91864\" height=\"491\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-prompt-routers-meta-1.png\" width=\"1481\" /></a></p> \\n<p>Prompt routers are integrated with other Amazon Bedrock capabilities, such as <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\">Amazon Bedrock Knowledge Bases</a> and <a href=\"https://aws.amazon.com/bedrock/agents/\">Amazon Bedrock Agents</a>, or when <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation.html\">performing evaluations</a>. For example, here I create a model evaluation to help me compare, for my use case, a prompt router to another model or prompt router.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-prompt-routers-evaluation.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91856\" height=\"777\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/26/bedrock-prompt-routers-evaluation.png\" width=\"1304\" /></a></p> \\n<p>To use a prompt router in an application, I need to set the prompt router <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html\">Amazon Resource Name (ARN)</a> as model ID in the Amazon Bedrock API. Let’s see how this works with the AWS CLI and an AWS SDK.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Using Amazon Bedrock Intelligent Prompt Routing with the AWS CLI<br /> </strong></span>The Amazon Bedrock API has been extended to handle prompt routers. For example, I can list the existing prompt routes in an AWS Region using <strong>ListPromptRouters</strong>:</p> \\n<div class=\"hide-language\"> \\n <pre class=\"unlimited-height-code\"><code class=\"lang-bash\">aws bedrock list-prompt-routers</code></pre> \\n</div> \\n<p>In output, I receive a summary of the existing prompt routers, similar to what I saw in the console.</p> \\n<p>Here’s the full output of the previous command:</p> \\n<pre><code class=\"lang-json\">{\\n    \"promptRouterSummaries\": [\\n        {\\n            \"promptRouterName\": \"Anthropic Prompt Router\",\\n            \"routingCriteria\": {\\n                \"responseQualityDifference\": 0.26\\n            },\\n            \"description\": \"Routes requests among models in the Claude family\",\\n            \"createdAt\": \"2024-11-20T00:00:00+00:00\",\\n            \"updatedAt\": \"2024-11-20T00:00:00+00:00\",\\n            \"promptRouterArn\": \"arn:aws:bedrock:us-east-1:123412341234:default-prompt-router/anthropic.claude:1\",\\n            \"models\": [\\n                {\\n                    \"modelArn\": \"arn:aws:bedrock:us-east-1:123412341234:inference-profile/us.anthropic.claude-3-haiku-20240307-v1:0\"\\n                },\\n                {\\n                    \"modelArn\": \"arn:aws:bedrock:us-east-1:123412341234:inference-profile/us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\\n                }\\n            ],\\n            \"fallbackModel\": {\\n                \"modelArn\": \"arn:aws:bedrock:us-east-1:123412341234:inference-profile/us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\\n            },\\n            \"status\": \"AVAILABLE\",\\n            \"type\": \"default\"\\n        },\\n        {\\n            \"promptRouterName\": \"Meta Prompt Router\",\\n            \"routingCriteria\": {\\n                \"responseQualityDifference\": 0.0\\n            },\\n            \"description\": \"Routes requests among models in the LLaMA family\",\\n            \"createdAt\": \"2024-11-20T00:00:00+00:00\",\\n            \"updatedAt\": \"2024-11-20T00:00:00+00:00\",\\n            \"promptRouterArn\": \"arn:aws:bedrock:us-east-1:123412341234:default-prompt-router/meta.llama:1\",\\n            \"models\": [\\n                {\\n                    \"modelArn\": \"arn:aws:bedrock:us-east-1:123412341234:inference-profile/us.meta.llama3-1-8b-instruct-v1:0\"\\n                },\\n                {\\n                    \"modelArn\": \"arn:aws:bedrock:us-east-1:123412341234:inference-profile/us.meta.llama3-1-70b-instruct-v1:0\"\\n                }\\n            ],\\n            \"fallbackModel\": {\\n                \"modelArn\": \"arn:aws:bedrock:us-east-1:123412341234:inference-profile/us.meta.llama3-1-70b-instruct-v1:0\"\\n            },\\n            \"status\": \"AVAILABLE\",\\n            \"type\": \"default\"\\n        }\\n    ]\\n}</code></pre> \\n<p>I can get information about a specific prompt router using <strong>GetPromptRouter</strong> with a prompt router ARN. For example, for the Meta Llama model family:</p> \\n<div class=\"hide-language\"> \\n <pre class=\"unlimited-height-code\"><code class=\"lang-bash\">aws bedrock get-prompt-router --prompt-router-arn arn:aws:bedrock:us-east-1:123412341234:default-prompt-router/meta.llama:1</code></pre> \\n</div> \\n<pre><code class=\"lang-json\">{\\n    \"promptRouterName\": \"Meta Prompt Router\",\\n    \"routingCriteria\": {\\n        \"responseQualityDifference\": 0.0\\n    },\\n    \"description\": \"Routes requests among models in the LLaMA family\",\\n    \"createdAt\": \"2024-11-20T00:00:00+00:00\",\\n    \"updatedAt\": \"2024-11-20T00:00:00+00:00\",\\n    \"promptRouterArn\": \"arn:aws:bedrock:us-east-1:123412341234:default-prompt-router/meta.llama:1\",\\n    \"models\": [\\n        {\\n            \"modelArn\": \"arn:aws:bedrock:us-east-1:123412341234:inference-profile/us.meta.llama3-1-8b-instruct-v1:0\"\\n        },\\n        {\\n            \"modelArn\": \"arn:aws:bedrock:us-east-1:123412341234:inference-profile/us.meta.llama3-1-70b-instruct-v1:0\"\\n        }\\n    ],\\n    \"fallbackModel\": {\\n        \"modelArn\": \"arn:aws:bedrock:us-east-1:123412341234:inference-profile/us.meta.llama3-1-70b-instruct-v1:0\"\\n    },\\n    \"status\": \"AVAILABLE\",\\n    \"type\": \"default\"\\n}\\n</code></pre> \\n<p>To use a prompt router with Amazon Bedrock, I set the prompt router ARN as model ID when making API calls. For example, here I use the Anthropic Prompt Router with the AWS CLI and the Amazon Bedrock Converse API:</p> \\n<div class=\"hide-language\"> \\n <pre class=\"unlimited-height-code\"><code class=\"lang-bash\">aws bedrock-runtime converse \\\\\\n    --model-id arn:aws:bedrock:us-east-1:123412341234:default-prompt-router/anthropic.claude:1 \\\\\\n    --messages \\'[{ \"role\": \"user\", \"content\": [ { \"text\": \"Alice has N brothers and she also has M sisters. How many sisters does Alice’s brothers have?\" } ] }]\\' \\\\</code></pre> \\n</div> \\n<p>In output, invocations using a prompt router include a new <code>trace</code> section that tells which model was actually used. In this case, it’s Anthropic’s Claude 3.5 Sonnet:</p> \\n<pre><code class=\"lang-json\">{\\n    \"output\": {\\n        \"message\": {\\n            \"role\": \"assistant\",\\n            \"content\": [\\n                {\\n                    \"text\": \"To solve this problem, let\\'s think it through step-by-step:\\\\n\\\\n1) First, we need to understand the relationships:\\\\n   - Alice has N brothers\\\\n   - Alice has M sisters\\\\n\\\\n2) Now, we need to consider who Alice\\'s brothers\\' sisters are:\\\\n   - Alice herself is a sister to all her brothers\\\\n   - All of Alice\\'s sisters are also sisters to Alice\\'s brothers\\\\n\\\\n3) So, the total number of sisters that Alice\\'s brothers have is:\\\\n   - The number of Alice\\'s sisters (M)\\\\n   - Plus Alice herself (+1)\\\\n\\\\n4) Therefore, the answer can be expressed as: M + 1\\\\n\\\\nThus, Alice\\'s brothers have M + 1 sisters.\"\\n                }\\n            ]\\n        }\\n    },\\n    . . .\\n    \"trace\": {\\n        \"promptRouter\": {\\n            \"invokedModelId\": \"arn:aws:bedrock:us-east-1:123412341234:inference-profile/us.anthropic.claude-3-5-sonnet-20240620-v1:0\"\\n        }\\n    }\\n}</code></pre> \\n<p><span style=\"text-decoration: underline;\"><strong>Using Amazon Bedrock Intelligent Prompt Routing with an AWS SDK<br /> </strong></span>Using an AWS SDK with a prompt router is similar to the previous command line experience. When invoking a model, I set the model ID to the prompt model ARN. For example, in this Python code I’m using the Meta Llama router with the <strong>ConverseStream</strong> API:</p> \\n<pre><code class=\"lang-python\">import json\\nimport boto3\\n\\nbedrock_runtime = boto3.client(\\n    \"bedrock-runtime\",\\n    region_name=\"us-east-1\",\\n)\\n\\nMODEL_ID = \"arn:aws:bedrock:us-east-1:123412341234:default-prompt-router/meta.llama:1\"\\n\\nuser_message = \"Describe the purpose of a \\'hello world\\' program in one line.\"\\nmessages = [\\n    {\\n        \"role\": \"user\",\\n        \"content\": [{\"text\": user_message}],\\n    }\\n]\\n\\nstreaming_response = bedrock_runtime.converse_stream(\\n    modelId=MODEL_ID,\\n    messages=messages,\\n)\\n\\nfor chunk in streaming_response[\"stream\"]:\\n    if \"contentBlockDelta\" in chunk:\\n        text = chunk[\"contentBlockDelta\"][\"delta\"][\"text\"]\\n        print(text, end=\"\")\\n    if \"messageStop\" in chunk:\\n        print()\\n    if \"metadata\" in chunk:\\n        if \"trace\" in chunk[\"metadata\"]:\\n            print(json.dumps(chunk[\\'metadata\\'][\\'trace\\'], indent=2))\\n</code></pre> \\n<p>This script prints the response text and the content of the trace in response metadata. For this uncomplicated request, the faster and more affordable model has been selected by the prompt router:</p> \\n<div class=\"hide-language\"> \\n <pre class=\"unlimited-height-code\"><code class=\"lang-json\">A \"Hello World\" program is a simple, introductory program that serves as a basic example to demonstrate the fundamental syntax and functionality of a programming language, typically used to verify that a development environment is set up correctly.\\n{\\n  \"promptRouter\": {\\n    \"invokedModelId\": \"arn:aws:bedrock:us-east-1:123412341234:inference-profile/us.meta.llama3-1-8b-instruct-v1:0\"\\n  }\\n}</code></pre> \\n</div> \\n<p><span style=\"text-decoration: underline;\"><strong>Using prompt caching with an AWS SDK<br /> </strong></span>You can use prompt caching with the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html\">Amazon Bedrock Converse API</a>. When you tag content for caching and send it to the model for the first time, the model processes the input and saves the intermediate results in a cache. For subsequent requests containing the same content, the model loads the preprocessed results from the cache, significantly reducing both costs and latency.</p> \\n<p>You can implement prompt caching in your applications with a few steps:</p> \\n<ol> \\n <li>Identify the portions of your prompts that are frequently reused.</li> \\n <li>Tag these sections for caching in the list of messages using the new <code>cachePoint</code> block.</li> \\n <li>Monitor cache usage and latency improvements in the response metadata <code>usage</code> section.</li> \\n</ol> \\n<p>Here’s an example of implementing prompt caching when working with documents.</p> \\n<p>First, I download <a href=\"https://aws.amazon.com/getting-started/decision-guides/\">three decision guides in PDF format from the AWS website</a>. These guides help choose the AWS services that fit your use case.</p> \\n<p>Then, I use a Python script&nbsp;to ask three questions about the documents. In the code, I create a <code>converse()</code> function to handle the conversation with the model. The first time I call the function, I include a list of documents and a flag to add a <code>cachePoint</code> block.</p> \\n<pre><code class=\"lang-python\">import json\\n\\nimport boto3\\n\\nMODEL_ID = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\\nAWS_REGION = \"us-west-2\"\\n\\nbedrock_runtime = boto3.client(\\n    \"bedrock-runtime\",\\n    region_name=AWS_REGION,\\n)\\n\\nDOCS = [\\n    \"bedrock-or-sagemaker.pdf\",\\n    \"generative-ai-on-aws-how-to-choose.pdf\",\\n    \"machine-learning-on-aws-how-to-choose.pdf\",\\n]\\n\\nmessages = []\\n\\n\\ndef converse(new_message, docs=[], cache=False):\\n\\n    if len(messages) == 0 or messages[-1][\"role\"] != \"user\":\\n        messages.append({\"role\": \"user\", \"content\": []})\\n\\n    for doc in docs:\\n        print(f\"Adding document: {doc}\")\\n        name, format = doc.rsplit(\\'.\\', maxsplit=1)\\n        with open(doc, \"rb\") as f:\\n            bytes = f.read()\\n        messages[-1][\"content\"].append({\\n            \"document\": {\\n                \"name\": name,\\n                \"format\": format,\\n                \"source\": {\"bytes\": bytes},\\n            }\\n        })\\n\\n    messages[-1][\"content\"].append({\"text\": new_message})\\n\\n    if cache:\\n        messages[-1][\"content\"].append({\"cachePoint\": {\"type\": \"default\"}})\\n\\n    response = bedrock_runtime.converse(\\n        modelId=MODEL_ID,\\n        messages=messages,\\n    )\\n\\n    output_message = response[\"output\"][\"message\"]\\n    response_text = output_message[\"content\"][0][\"text\"]\\n\\n    print(\"Response text:\")\\n    print(response_text)\\n\\n    print(\"Usage:\")\\n    print(json.dumps(response[\"usage\"], indent=2))\\n\\n    messages.append(output_message)\\n\\n\\nconverse(\"Compare AWS Trainium and AWS Inferentia in 20 words or less.\", docs=DOCS, cache=True)\\nconverse(\"Compare Amazon Textract and Amazon Transcribe in 20 words or less.\")\\nconverse(\"Compare Amazon Q Business and Amazon Q Developer in 20 words or less.\")</code></pre> \\n<p>For each invocation, the script prints the response and the <code>usage</code> counters.</p> \\n<div class=\"hide-language\"> \\n <pre><code class=\"lang-bash\">Adding document: bedrock-or-sagemaker.pdf\\nAdding document: generative-ai-on-aws-how-to-choose.pdf\\nAdding document: machine-learning-on-aws-how-to-choose.pdf\\nResponse text:\\nAWS Trainium is optimized for machine learning training, while AWS Inferentia is designed for low-cost, high-performance machine learning inference.\\nUsage:\\n{\\n  \"inputTokens\": 4,\\n  \"outputTokens\": 34,\\n  \"totalTokens\": 29879,\\n  \"cacheReadInputTokenCount\": 0,\\n  \"cacheWriteInputTokenCount\": 29841\\n}\\nResponse text:\\nAmazon Textract extracts text and data from documents, while Amazon Transcribe converts speech to text from audio or video files.\\nUsage:\\n{\\n  \"inputTokens\": 59,\\n  \"outputTokens\": 30,\\n  \"totalTokens\": 29930,\\n  \"cacheReadInputTokenCount\": 29841,\\n  \"cacheWriteInputTokenCount\": 0\\n}\\nResponse text:\\nAmazon Q Business answers questions using enterprise data, while Amazon Q Developer assists with building and operating AWS applications and services.\\nUsage:\\n{\\n  \"inputTokens\": 108,\\n  \"outputTokens\": 26,\\n  \"totalTokens\": 29975,\\n  \"cacheReadInputTokenCount\": 29841,\\n  \"cacheWriteInputTokenCount\": 0\\n}</code></pre> \\n</div> \\n<p>The <code>usage</code> section of the response contains two new counters: <code>cacheReadInputTokenCount</code> and <code>cacheWriteInputTokenCount</code>. The total number of tokens for an invocation is the sum of the input and output tokens plus the tokens read and written into the cache.</p> \\n<p>Each invocation processes a list of messages. The messages in the first invocation contain the documents, the first question, and the cache point. Because the messages preceding the cache point aren’t currently in the cache, they’re written to cache. According to the <code>usage</code> counters, 29,841 tokens have been written into the cache.</p> \\n<div class=\"hide-language\"> \\n <pre class=\"unlimited-height-code\"><code class=\"lang-bash\">\"cacheWriteInputTokenCount\": 29841</code></pre> \\n</div> \\n<p>For the next invocations, the previous response and the new question are appended to the list of messages. The messages before the <code>cachePoint</code> are not changed and found in the cache.</p> \\n<p>As expected, we can tell from the <code>usage</code> counters that the same number of tokens previously written is now read from the cache.</p> \\n<div class=\"hide-language\"> \\n <pre class=\"unlimited-height-code\"><code class=\"lang-bash\">\"cacheReadInputTokenCount\": 29841</code></pre> \\n</div> \\n<p>In my tests, the next invocations take 55 percent less time to complete compared to the first one. Depending on your use case (for example, with more cached content), prompt caching can improve latency up to 85 percent.</p> \\n<p>Depending on the model, you can set more than one cache point in a list of messages. To find the right cache points for your use case, try different configurations and look at the effect on the reported usage.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Things to know<br /> </strong></span>Amazon Bedrock Intelligent Prompt Routing is available in preview today in US East (N. Virginia) and US West (Oregon) <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Regions</a>. During the preview, you can use the default prompt routers, and there is no additional cost for using a prompt router. You pay the cost of the selected model. You can use prompt routers with other Amazon Bedrock capabilities such as <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation.html\">performing evaluations</a>, <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\">using knowledge bases</a>, and <a href=\"https://aws.amazon.com/bedrock/agents/\">configuring agents</a>.</p> \\n<p>Because the internal model used by the prompt routers needs to understand the complexity of a prompt, intelligent prompt routing currently only supports English language prompts.</p> \\n<p>Amazon Bedrock support for prompt caching is available in preview in US West (Oregon) for Anthropic’s Claude 3.5 Sonnet V2 and Claude 3.5 Haiku. Prompt caching is also available in US East (N. Virginia) for Amazon Nova Micro, Amazon Nova Lite, and Amazon Nova Pro. You can <a href=\"https://pages.awscloud.com/promptcaching-Preview.html\">request access to the Amazon Bedrock prompt caching preview here</a>.</p> \\n<p>With prompt caching, cache reads receive a 90 percent discount compared to noncached input tokens. There are no additional infrastructure charges for cache storage. When using Anthropic models, you pay an additional cost for tokens written in the cache. There are no additional costs for cache writes with Amazon Nova models. For more information, see <a href=\"https://aws.amazon.com/bedrock/pricing/\">Amazon Bedrock pricing</a>.</p> \\n<p>When using prompt caching, content is cached for up to 5 minutes, with each cache hit resetting this countdown. Prompt caching has been implemented to transparently support <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html\">cross-Region inference</a>. In this way, your applications can get the cost optimization and latency benefit of prompt caching with the flexibility of cross-Region inference.</p> \\n<p>These new capabilities make it easier to build cost-effective and high-performing generative AI applications. By intelligently routing requests and caching frequently used content, you can significantly reduce your costs while maintaining and even improving application performance.</p> \\n<p>To learn more and start using these new capabilities today, visit the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html\">Amazon Bedrock documentation</a>&nbsp;and send feedback to <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag/amazon-bedrock\">AWS re:Post for Amazon Bedrock</a>. You can find deep-dive technical content and discover how our Builder communities are using Amazon Bedrock at <a href=\"https://community.aws/\">community.aws</a>.</p> \\n<p>— <a href=\"https://twitter.com/danilop\">Danilo</a></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9c7ab21de127224740b9d90929760c36e9143cf5', embedding=None, metadata={'title': 'Amazon Bedrock Marketplace: Access over 100 foundation models in one place', 'link': 'https://aws.amazon.com/blogs/aws/amazon-bedrock-marketplace-access-over-100-foundation-models-in-one-place/', 'date': 'Wed, 04 Dec 2024 17:16:36 +0000'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='<p>Today, we’re introducing <a href=\"https://aws.amazon.com/bedrock/marketplace/\">Amazon Bedrock Marketplace</a>, a new capability that gives you access to over 100 popular, emerging, and specialized <a href=\"https://aws.amazon.com/what-is/foundation-models/\">foundation models (FMs)</a> through <a href=\"https://aws.amazon.com/bedrock/\">Amazon Bedrock</a>. With this launch, you can now discover, test,&nbsp;and deploy new models from enterprise providers such as IBM and Nvidia, specialized models such as Upstages’ Solar Pro for Korean language processing, and Evolutionary Scale’s ESM3 for protein research, alongside Amazon Bedrock general-purpose FMs from providers such as <a href=\"https://aws.amazon.com/bedrock/claude/\">Anthropic</a> and <a href=\"https://aws.amazon.com/bedrock/llama/\">Meta</a>.</p> \\n<p>Models deployed with Amazon Bedrock Marketplace can be accessed through the same standard APIs as the serverless models and, for models which are compatible with Converse API, be used with tools such as <a href=\"https://aws.amazon.com/bedrock/agents/\">Amazon Bedrock Agents</a> and <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\">Amazon Bedrock Knowledge Bases</a>.</p> \\n<p>As <a href=\"https://aws.amazon.com/ai/generative-ai/\">generative AI</a> continues to reshape how organizations work, the need for specialized models optimized for specific domains, languages, or tasks is growing. However, finding and evaluating these models can be challenging and costly. You need to discover them across different services, build abstractions to use them in your applications, and create complex security and governance layers. Amazon Bedrock Marketplace addresses these challenges by providing a single interface to access both specialized and general-purpose FMs.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Using Amazon Bedrock Marketplace<br /> </strong></span>To get started, in the Amazon Bedrock console, I choose <strong>Model catalog</strong> in the <strong>Foundation models</strong> section of the navigation pane. Here, I can search for models that help me with a specific use case or language. The results of the search include both serverless models and models available in Amazon Bedrock Marketplace. I can filter results by provider, modality (such as text, image, or audio), or task (such as classification or text summarization).</p> \\n<p>In the catalog, there are models from organizations like <a href=\"https://www.arcee.ai/\">Arcee AI</a>, which builds context-adapted small language models (SLMs), and <a href=\"https://www.widn.ai/en/\">Widn.AI</a>, which provides multilingual models.</p> \\n<p>For example, I am interested in the <a href=\"https://www.ibm.com/granite\">IBM Granite</a> models and search for models from <strong>IBM Data and AI</strong>.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-search-1.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92473\" height=\"1464\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-search-1.png\" width=\"3212\" /></a></p> \\n<p>I select <strong>Granite 3.0 2B Instruct</strong>, a language model designed for enterprise applications. Choosing the model opens the model detail page where I can see more information from the model provider such as highlights about the model, pricing, and usage including sample API calls.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-model-details-1.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92474\" height=\"1609\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-model-details-1.png\" width=\"2253\" /></a></p> \\n<p>This specific model requires a subscription, and I choose <strong>View subscription options</strong>.</p> \\n<p>From the subscription dialog, I review pricing and legal notes. In <strong>Pricing details</strong>, I see the software price set by the provider. For this model, there are no additional costs on top of the deployed infrastructure. The <a href=\"https://aws.amazon.com/sagemaker/\">Amazon SageMaker</a> infrastructure cost is charged separately and can be seen in <a href=\"https://aws.amazon.com/sagemaker/pricing/\">Amazon SageMaker pricing</a>.</p> \\n<p>To proceed with this model, I choose <strong>Subscribe</strong>.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-subscribe-1.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92475\" height=\"1686\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-subscribe-1.png\" width=\"2901\" /></a></p> \\n<p>After the subscription has been completed, which usually takes a few minutes, I can deploy the model. For <strong>Deployment details</strong>, I use the default settings and the recommended instance type.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-deploy.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92481\" height=\"648\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-deploy.png\" width=\"1594\" /></a></p> \\n<p>I expand the optional <strong>Advanced settings</strong>. Here, I can choose to deploy in a <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html\">virtual private cloud (VPC)</a> or specify the <a href=\"https://aws.amazon.com/iam/\">AWS Identity and Access Management (IAM)</a> service role used by the deployment. Amazon Bedrock Marketplace automatically creates a service role to access <a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service (Amazon S3)</a> buckets where the model weights are stored, but I can choose to use an existing role.</p> \\n<p>I keep the default values and complete the deployment.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-deploy-advanced.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92476\" height=\"1717\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-deploy-advanced.png\" width=\"2983\" /></a></p> \\n<p>After a few minutes, the deployment is <strong>In Service</strong> and can be reviewed in the <strong>Marketplace deployments</strong> page from the navigation pane.</p> \\n<p>There, I can choose an endpoint to view details and edit the configuration such as the number of instances. To test the deployment, I choose <strong>Open in playground</strong> and ask for some poetry.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-playground-1.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92477\" height=\"1586\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-playground-1.png\" width=\"2492\" /></a></p> \\n<p>I can also select the model from the <strong>Chat/text</strong> page of the <strong>Playground</strong> using the new <strong>Marketplace</strong> category where the deployed endpoints are listed.</p> \\n<p>In a similar way, I can use the model with other tools such as <a href=\"https://aws.amazon.com/bedrock/agents/\">Amazon Bedrock Agents</a>, <a href=\"https://aws.amazon.com/bedrock/knowledge-bases/\">Amazon Bedrock Knowledge Bases</a>, <a href=\"https://aws.amazon.com/bedrock/prompt-management/\">Amazon Bedrock Prompt Management</a>, <a href=\"https://aws.amazon.com/bedrock/guardrails/\">Amazon Bedrock Guardrails</a>, and <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/evaluation.html\">model evaluations</a>, by choosing <strong>Select Model</strong> and selecting the <strong>Marketplace</strong> model endpoint.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-select-model-1.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-92478\" height=\"1448\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/28/bedrock-marketplace-select-model-1.png\" width=\"2065\" /></a></p> \\n<p>The model I used here is text-to-text, but I can use Amazon Bedrock Marketplace to deploy models with different modalities. For example, after I deploy <a href=\"https://stability.ai/\">Stability AI</a>&nbsp;<a href=\"https://stability.ai/news/introducing-stable-diffusion-3-5\">Stable Diffusion 3.5 Large</a>, I can run a quick test in the Amazon Bedrock <strong>Image playground</strong>.</p> \\n<p><a href=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/bedrock-marketplace-playground-image.png\"><img alt=\"Console screenshot.\" class=\"aligncenter size-full wp-image-91339\" height=\"1424\" src=\"https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2024/11/22/bedrock-marketplace-playground-image.png\" width=\"2842\" /></a></p> \\n<p>The models I deployed are now available through the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/inference-invoke.html\">Amazon Bedrock InvokeModel API</a>. When a model is deployed, I can use it with the <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a> and any <a href=\"https://aws.amazon.com/tools/\">AWS SDKs</a> using the endpoint <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference-arns.html\">Amazon Resource Name (ARN)</a> as model ID.</p> \\n<p>For chat-tuned text-to-text models, I can also use the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html\">Amazon Bedrock Converse API</a>, which abstracts model differences and enables model switching with a single parameter change.</p> \\n<p><span style=\"text-decoration: underline;\"><strong>Things to know<br /> </strong></span><a href=\"https://aws.amazon.com/bedrock/marketplace/\">Amazon Bedrock Marketplace</a> is available in the following <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Regions</a>: US East (N. Virginia), US East (Ohio), US West (Oregon), Asia Pacific (Mumbai), Asia Pacific (Seoul), Asia Pacific (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Canada (Central), Europe (Frankfurt), Europe (Ireland), Europe (London), Europe (Paris), and South America (São Paulo).</p> \\n<p>With Amazon Bedrock Marketplace, you pay a software fee to the third-party model provider (which can be zero, as in the previous example) and a hosting fee based on the type and number of instances you choose for your model endpoints.</p> \\n<p>Start browsing the new models using the <a href=\"https://console.aws.amazon.com/bedrock/home#/model-catalog\">Model catalog in the Amazon Bedrock console</a>, visit the <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/amazon-bedrock-marketplace.html\">Amazon Bedrock Marketplace documentation</a>, and send feedback to <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag/amazon-bedrock\">AWS re:Post for Amazon Bedrock</a>. You can find deep-dive technical content and discover how our Builder communities are using Amazon Bedrock at <a href=\"https://community.aws/\">community.aws</a>.</p> \\n<p>— <a href=\"https://twitter.com/danilop\">Danilo</a></p>', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.readers.web import RssReader\n",
    "\n",
    "# Initialize the RSS reader\n",
    "rss_reader = RssReader()\n",
    "\n",
    "# Load data from RSS feeds\n",
    "urls = [\"https://aws.amazon.com/blogs/aws/feed/\"]\n",
    "documents = rss_reader.load_data(urls)\n",
    "\n",
    "# Now you can index these documents using a suitable index, such as VectorStoreIndex\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "print(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The U7inh instance is a new addition to the EC2 High Memory family, built in collaboration with Hewlett Packard Enterprise (HPE). It runs on the 16-socket HPE Compute Scale-up Server 3200 and is powered by the fourth generation Intel Xeon Scalable processors (Sapphire Rapids). The U7inh instance supports 32 TB of memory, 1920 vCPUs, and offers the highest compute performance and largest compute and memory size in the AWS Cloud for running large, mission-critical database workloads like SAP HANA.\n"
     ]
    }
   ],
   "source": [
    "vector_query_engine = index.as_query_engine()\n",
    "query_str = \"whats new with u7inh\"\n",
    "print(vector_query_engine.query(query_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
